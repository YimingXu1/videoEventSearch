[('./static/5.mp4', 4, u'Okay. Hello everyone. '), ('./static/5.mp4', 7, u'Um, welcome back to the second class of, um, CS224N. '), ('./static/5.mp4', 12, u'Okay, so right at the end of last time I was just showing you a little, '), ('./static/5.mp4', 17, u'um, from this, um, '), ('./static/5.mp4', 18, u'IPython Notebook of things that you could do with word vectors '), ('./static/5.mp4', 22, u'but I kind of ran out of time a little for a bit. '), ('./static/5.mp4', 25, u"So, I'll just spend a couple of more minutes first, "), ('./static/5.mp4', 28, u'um, showing the end of  this. '), ('./static/5.mp4', 29, u'I stuck this IPython Notebook up on the course page. '), ('./static/5.mp4', 33, u'So, under lecture one you can find a copy of it and you can download it. '), ('./static/5.mp4', 37, u'So, I both stuck up just an HTML version of it and a zip file. '), ('./static/5.mp4', 41, u'Like HTML file is only good to look at. '), ('./static/5.mp4', 44, u"You can't do anything with it. "), ('./static/5.mp4', 45, u'So, you wanna, if you wanna play with it by yourself, um, '), ('./static/5.mp4', 48, u'download the zip file and get the IPython Notebook out of that. '), ('./static/5.mp4', 52, u'Okay. So we were looking at '), ('./static/5.mp4', 53, u"these Glove word vectors which I'll talk about a bit more today and so there were "), ('./static/5.mp4', 57, u'these sort of basic results of similarity in this vector space work very nicely for, '), ('./static/5.mp4', 64, u'um, discovering similar words and then going on from that, '), ('./static/5.mp4', 70, u"there was this idea that we'll spend some more time on today which was, um, "), ('./static/5.mp4', 75, u'maybe this vector space is not only a similarity space where '), ('./static/5.mp4', 80, u'close together things have similar meaning but it actually captures meaning '), ('./static/5.mp4', 85, u'in a considerably deeper and more profound way which is to say that there are '), ('./static/5.mp4', 90, u'actually directions in the space that you can point which have a certain meaning. '), ('./static/5.mp4', 97, u'So, that if you are pointing in one direction it means this is more so the case, '), ('./static/5.mp4', 103, u'if you are pointing in a different direction and the meaning space it might be this is '), ('./static/5.mp4', 108, u'the capital of this country or '), ('./static/5.mp4', 110, u'all sorts of different meanings could be encoded in the space. '), ('./static/5.mp4', 114, u'And a way of testing that, '), ('./static/5.mp4', 116, u'is to use these analogy, um, problems. '), ('./static/5.mp4', 119, u"And I quickly showed this at the end but just to make sure if you're "), ('./static/5.mp4', 123, u"unguarded since it's sort of- it's sort of a clever thing right? "), ('./static/5.mp4', 126, u"So, the idea is that we're going to start with a pair of words like king and man. "), ('./static/5.mp4', 134, u"And so what we're gonna do is we're gonna say well, "), ('./static/5.mp4', 137, u"there's a vector for king in the space and there's a vector for man in "), ('./static/5.mp4', 143, u"the space and but what we're gonna do is we're going to "), ('./static/5.mp4', 146, u'subtract as in just good old vector subtraction that you hopefully learned in your, '), ('./static/5.mp4', 152, u'um, linear algebra class. '), ('./static/5.mp4', 153, u"We're gonna subtract the man vector from "), ('./static/5.mp4', 156, u'the king vector and the idea we have in our head then is if we do '), ('./static/5.mp4', 160, u"that what will happen is we'll be left with the meaning of kingship without the manness. "), ('./static/5.mp4', 167, u"Um, and so then there's also a direct vector for a woman. "), ('./static/5.mp4', 174, u'So, we can add the woman vector to that resulting vector and then we could say well, '), ('./static/5.mp4', 179, u"in the vector, we end up at some point in the vector space and then we're gonna say well, "), ('./static/5.mp4', 184, u"what's the closest word that you're gonna find the here "), ('./static/5.mp4', 187, u"and it's gonna print out the closest word and as we saw, "), ('./static/5.mp4', 191, u'um, last time, um, '), ('./static/5.mp4', 194, u'lo and behold if you do that, '), ('./static/5.mp4', 197, u'um, you get the answer. '), ('./static/5.mp4', 200, u"I'm saying you get, "), ('./static/5.mp4', 202, u'um, king, man, woman. '), ('./static/5.mp4', 213, u'No? All right. [LAUGHTER]. '), ('./static/5.mp4', 216, u'You gotta reverse king and man. '), ('./static/5.mp4', 219, u'I have to reverse king and, '), ('./static/5.mp4', 220, u"ah, sure, sure, sure. I'm sorry. "), ('./static/5.mp4', 222, u'Oops. Yeah, okay, I kinda do it well like man, king. '), ('./static/5.mp4', 229, u"Ah, [LAUGHTER] Okay. Yeah, that's right. "), ('./static/5.mp4', 235, u'Sorry. Okay. Yeah, because it should be '), ('./static/5.mp4', 237, u'man is to king as woman is to something sorry yeah. '), ('./static/5.mp4', 240, u'I was getting [LAUGHTER] my order of components wrong. '), ('./static/5.mp4', 243, u'Okay. Um, and, you know, '), ('./static/5.mp4', 246, u'as I was sort of I guess I was showing some examples last time with '), ('./static/5.mp4', 250, u'nationality words but I mean this in a way that is sort of surprising to shocking, '), ('./static/5.mp4', 258, u'this actually works for all kinds of things that you can get meaning in this space. '), ('./static/5.mp4', 263, u'So, I can ask various kinds of analogies of sorts. '), ('./static/5.mp4', 269, u'So I can say Australia is to beer as France is to-. '), ('./static/5.mp4', 273, u'Wine. '), ('./static/5.mp4', 274, u'Wine. You might think wine. '), ('./static/5.mp4', 275, u'What it gives back as champagne which seems a pretty good answer. '), ('./static/5.mp4', 278, u"[LAUGHTER] Um, I'll go with that. "), ('./static/5.mp4', 280, u'Um, um, you can do more syntactic facts. '), ('./static/5.mp4', 284, u'So, I can say tall ta- tall is to tallest as long is to longest and it gets set. '), ('./static/5.mp4', 291, u'Um, if I say good is to fantastic as bad is to terrible. '), ('./static/5.mp4', 296, u"That it seems to get out that there's some kind of notion of "), ('./static/5.mp4', 300, u'make more extreme direction and get this direction out. '), ('./static/5.mp4', 304, u'I skipped over one. '), ('./static/5.mp4', 306, u'A bomber is to Clinton as Reagan is to. '), ('./static/5.mp4', 310, u'You may or may not like the answer it gives for this one '), ('./static/5.mp4', 313, u'as Obama is to- as Reagan is to Nixon. '), ('./static/5.mp4', 317, u'Um, now one thing you might notice at '), ('./static/5.mp4', 320, u'this point and this is something I actually want to come back to at the end. '), ('./static/5.mp4', 324, u"Um, well, there's this problem because Clinton's ambiguous, right? "), ('./static/5.mp4', 328, u"There's Bill or there's Hillary. "), ('./static/5.mp4', 329, u'Um, and, um, I forget, '), ('./static/5.mp4', 332, u'you know, so this data as I said is a few years old. '), ('./static/5.mp4', 336, u'So, this data was done in 2014. '), ('./static/5.mp4', 339, u"So, in sort of in- it definitely doesn't "), ('./static/5.mp4', 342, u'have Trump really in it as a politician, um, but, you know, '), ('./static/5.mp4', 345, u'it would have variously both Clintons but as sort of makes sense if probably um, '), ('./static/5.mp4', 350, u'for a sort of proof for 2014 data, '), ('./static/5.mp4', 353, u'um, that Bill Clinton dominated. '), ('./static/5.mp4', 355, u"So, I think what we're getting, um, "), ('./static/5.mp4', 357, u'out of this is that Clinton and Nixon are sort of similar of people in dangers, '), ('./static/5.mp4', 363, u'um, of being impeached. '), ('./static/5.mp4', 365, u'Um, and, uh, on both sides of the aisle had us thinking primarily of Bill Clinton. '), ('./static/5.mp4', 371, u'But, um, if this sort of brings up something that '), ('./static/5.mp4', 374, u"I'll come back to right at the end of, um, "), ('./static/5.mp4', 377, u"it sort of looks like we've got a sort of a problem here because we "), ('./static/5.mp4', 381, u'just have this string literally Clinton and that, um, '), ('./static/5.mp4', 385, u'string is any possible sense and meaning of the string Clinton and so minimally um, '), ('./static/5.mp4', 395, u'that we have Bill Clinton and Hillary Clinton that near. '), ('./static/5.mp4', 398, u'Maybe you have some friends that are called Clinton as well, right, '), ('./static/5.mp4', 401, u"and they're all mixed together in this Clinton. "), ('./static/5.mp4', 403, u"And so that seems kinda problematic and that's sort of been an issue "), ('./static/5.mp4', 407, u"that's been discussed some for these word vectors and I'll come back to that. "), ('./static/5.mp4', 410, u'Um, another thing you can do is you can give '), ('./static/5.mp4', 413, u'a set of words and say which is the odd one out. '), ('./static/5.mp4', 416, u'Maybe you used to do puzzles like that in middle school or something. '), ('./static/5.mp4', 420, u'Um, and so you can do that and it decides '), ('./static/5.mp4', 422, u'that cereal is the odd one out of that set. It seems okay. '), ('./static/5.mp4', 426, u"Um, and then one other thing I'll just show you is, so, um, "), ('./static/5.mp4', 429, u"it'll sort of be nice to look at these words that I've drawn "), ('./static/5.mp4', 433, u'them in some of the slide pictures. '), ('./static/5.mp4', 436, u'So, this is saying to put together a PCA or '), ('./static/5.mp4', 438, u'Principal Components Analysis, um, scatter plot. '), ('./static/5.mp4', 442, u'Um, so, I can do that and then I can say, "Um, '), ('./static/5.mp4', 446, u'give it a set of words and draw me these as a scatter plot" and um, '), ('./static/5.mp4', 453, u'hopefully if I can just about fit it in, '), ('./static/5.mp4', 456, u"um, here's my scatter plot. "), ('./static/5.mp4', 458, u'And it works pretty well, right? '), ('./static/5.mp4', 460, u"I've got the wine, champagne, "), ('./static/5.mp4', 461, u'beer up here then the coffee and tea. '), ('./static/5.mp4', 463, u'Um, here are the countries. '), ('./static/5.mp4', 465, u'Here is the schools, college institute, universities. '), ('./static/5.mp4', 469, u'Um, the animals are down here. '), ('./static/5.mp4', 471, u'Um, foodstuffs there. '), ('./static/5.mp4', 475, u'So, yeah, this sort of really does work with this two direction- dimensional display. '), ('./static/5.mp4', 480, u'It basically shows you similarity. '), ('./static/5.mp4', 482, u'Now, um, there are, you know, '), ('./static/5.mp4', 486, u'to some extent though you want to hold on to your wallet with these PCA displays. '), ('./static/5.mp4', 490, u"So, it's as I've discussed before since you're "), ('./static/5.mp4', 493, u"taking something that was 100-dimensional and we're just doing "), ('./static/5.mp4', 496, u'this 2D projection that is capturing some of the major geometry of '), ('./static/5.mp4', 500, u'the space but it just has to be losing a huge amount of the information. '), ('./static/5.mp4', 505, u'So, when things end up close together, '), ('./static/5.mp4', 507, u'they might be really close together in the original space or '), ('./static/5.mp4', 510, u'they might just have been words that lost in '), ('./static/5.mp4', 513, u'the 2D projection because they- there are other patterns that '), ('./static/5.mp4', 517, u'were more dominant and were chosen as the first two principal components. '), ('./static/5.mp4', 521, u"So, you sort of don't wanna over trust "), ('./static/5.mp4', 524, u'these things and something if you like Infoviz you might think '), ('./static/5.mp4', 527, u'about is how there are other ways that I might be able to '), ('./static/5.mp4', 530, u'represent the distances in a way that was more accurate. '), ('./static/5.mp4', 533, u"Um, but anyway this is very simple to do and I'm just getting "), ('./static/5.mp4', 536, u'a PCA to reduce the dimensionality of the matrix and then, '), ('./static/5.mp4', 540, u'um, transforming with it these word vectors and printing them. '), ('./static/5.mp4', 544, u"Um, it's mainly easy to do. "), ('./static/5.mp4', 547, u"The bit that wasn't easy for me to do, um, "), ('./static/5.mp4', 549, u"but if someone's got some clever Python um plotting tips I'd like one, "), ('./static/5.mp4', 554, u'if someone wants to send me a message after class. '), ('./static/5.mp4', 557, u"I would have thought there'd be some default way in which you could just "), ('./static/5.mp4', 560, u"label points in a scatter plot but I wasn't able to find one. "), ('./static/5.mp4', 565, u'So, what I did, um, '), ('./static/5.mp4', 567, u"was I'm just sort of plotting the texts and "), ('./static/5.mp4', 569, u"I'm offsetting it a little bit from the points. "), ('./static/5.mp4', 571, u'Um, now that works kinda crappily '), ('./static/5.mp4', 573, u'because they just collide with each other as you can see. '), ('./static/5.mp4', 576, u"Um, so, it'd be better if there was a better way to do point labeling in Python plots. "), ('./static/5.mp4', 580, u'So, if anyone knows the answer to that one you can send it to me. '), ('./static/5.mp4', 584, u"Um, okay. So, that's that. Ah. "), ('./static/5.mp4', 589, u"And if you haven't used IPython Notebooks "), ('./static/5.mp4', 592, u"before and don't want your computer to run really slowly, "), ('./static/5.mp4', 595, u"it's a good idea to halt "), ('./static/5.mp4', 596, u"your IPython Notebooks when you're not gonna be using them anymore, "), ('./static/5.mp4', 600, u"um, especially if they're computing something. "), ('./static/5.mp4', 602, u'Um, okay. [NOISE] Um. '), ('./static/5.mp4', 604, u'[NOISE] '), ('./static/5.mp4', 626, u'Okay. [NOISE] So now, '), ('./static/5.mp4', 628, u'[NOISE] um, lecture two and so for today, '), ('./static/5.mp4', 632, u"we're gonna keep on talking about things you can do with "), ('./static/5.mp4', 634, u'Word Vectors and say a little bit at the end about Word sensors. '), ('./static/5.mp4', 638, u'So, in more detail, [NOISE] um, '), ('./static/5.mp4', 641, u"I'm gonna say a bit more about, um, Word2Vec. "), ('./static/5.mp4', 645, u"I'm gonna have a sort of a very brief excursion on optimization, um, "), ('./static/5.mp4', 650, u'but then I sort of want to explain a bit more of the space of what '), ('./static/5.mp4', 655, u'people have done and can do with dense word representations. '), ('./static/5.mp4', 660, u'So I am gonna say something about '), ('./static/5.mp4', 662, u'count-based approaches to capturing meaning and how do they work. '), ('./static/5.mp4', 667, u"I'm gonna talk for a bit about a, "), ('./static/5.mp4', 668, u'a different model of Word Vectors which was the GloVe model that, '), ('./static/5.mp4', 672, u'um, as a post-doc of mine, um, '), ('./static/5.mp4', 676, u'Jeffrey Pennington and, uh, '), ('./static/5.mp4', 678, u'me worked on a couple of years ago, '), ('./static/5.mp4', 680, u'um, talk some about evaluation, '), ('./static/5.mp4', 682, u'really quite dominant theme on a lot of what '), ('./static/5.mp4', 685, u'we do on natural language processing is how do we, '), ('./static/5.mp4', 687, u'how do we evaluate things and how much do we trust our evaluations, '), ('./static/5.mp4', 692, u'um, and then say a little bit about, um, word sensors. '), ('./static/5.mp4', 695, u'I have a sort of a goal here which is that by the end of the class, '), ('./static/5.mp4', 698, u'um, you should actually sort of understand, um, '), ('./static/5.mp4', 701, u'enough of the lay of the land that you could '), ('./static/5.mp4', 703, u'read papers about word vectors such as the ones that '), ('./static/5.mp4', 707, u'are in the syllabus and actually understand '), ('./static/5.mp4', 709, u"them and where they're coming from and roughly how they work. "), ('./static/5.mp4', 712, u'And so, you know, if you really wanna minimize '), ('./static/5.mp4', 714, u'work for your c- this class, you could think, "I, '), ('./static/5.mp4', 716, u"I know everything I need to know after the first week and I'm gonna "), ('./static/5.mp4', 719, u'do a final project on word vectors and I\'ll be okay." '), ('./static/5.mp4', 723, u'Um, and you know, you could actually do that, '), ('./static/5.mp4', 725, u'I mentioned during the wo- um, '), ('./static/5.mp4', 726, u'class, um, a couple of recent pieces of work on word vectors. '), ('./static/5.mp4', 731, u'On the other hand, um, '), ('./static/5.mp4', 733, u'doing things with word vectors as a fairly mined out areas, '), ('./static/5.mp4', 736, u"so you're probably better off, um, "), ('./static/5.mp4', 738, u'also listening to some of the later parts of the class. '), ('./static/5.mp4', 741, u'Okay. So, remember we had this idea of Word2Vec, '), ('./static/5.mp4', 744, u'so it was an iterative updating algorithm that learned, um, '), ('./static/5.mp4', 749, u'these vector representations of words, '), ('./static/5.mp4', 752, u'then in some sense capture their meaning and the way it worked was we kinda '), ('./static/5.mp4', 756, u'moved position by position through a corpus and each point in time, '), ('./static/5.mp4', 760, u"we had a center word here into and it's trying to predict "), ('./static/5.mp4', 765, u'the words around that by having '), ('./static/5.mp4', 767, u'a probability distribution over words will occur around that, '), ('./static/5.mp4', 770, u'and that probability distribution is defined simply in terms '), ('./static/5.mp4', 775, u'of the.product of the word vectors via the Softmax function. '), ('./static/5.mp4', 780, u'And so, what we wanna do is change those vectors in '), ('./static/5.mp4', 783, u'a way that this gives good probability predictions, '), ('./static/5.mp4', 786, u'that gives as high probability as possible to words that you tend to see in the context. '), ('./static/5.mp4', 792, u'And so, just to drill that in a little bit more, you know, '), ('./static/5.mp4', 795, u'what we actually have is we have two matrices, right? '), ('./static/5.mp4', 801, u'We have for center words, '), ('./static/5.mp4', 803, u'we have a matrix where for each word in our vocabulary, '), ('./static/5.mp4', 806, u'we have a vector, um, and at this, '), ('./static/5.mp4', 809, u'this is probably as good a point as any to say that it '), ('./static/5.mp4', 812, u'turns out that all the major deep learning packages, '), ('./static/5.mp4', 816, u'TensorFlow, PyTorch, etc., for their word vectors, '), ('./static/5.mp4', 820, u'the word vectors are represented as rows. '), ('./static/5.mp4', 823, u"If you've done a bunch of math classes, "), ('./static/5.mp4', 824, u'that might not be what you would expect. '), ('./static/5.mp4', 826, u'You might have expected the other way around, '), ('./static/5.mp4', 828, u'but they all put them in rows. '), ('./static/5.mp4', 830, u'So we can have rows for our, '), ('./static/5.mp4', 833, u'um, so we have six words and a five dimensional vector each. '), ('./static/5.mp4', 836, u'Okay. And then, we have this outside, um, '), ('./static/5.mp4', 840, u'matrix where we also have a second, um, '), ('./static/5.mp4', 842, u'vector for each word which is this representation in context. '), ('./static/5.mp4', 847, u'Um, so when we have a particular center word here, '), ('./static/5.mp4', 851, u'word four, you know, '), ('./static/5.mp4', 852, u"when we're doing our computations, "), ('./static/5.mp4', 854, u"we're taking a.product between v_4 and each row of "), ('./static/5.mp4', 859, u"U and that's then giving us a vector of dot product scores. "), ('./static/5.mp4', 865, u'And so, then after that, '), ('./static/5.mp4', 867, u"we're running Softmaxes on each of those numbers doing it "), ('./static/5.mp4', 871, u"element-wise and that's been giving us "), ('./static/5.mp4', 873, u'a probability distribution over words in the context. '), ('./static/5.mp4', 877, u'Um, and the sort of things to notice there, um, '), ('./static/5.mp4', 881, u'which hopefully you noticed last time, '), ('./static/5.mp4', 883, u'but to make sure you noticed that, '), ('./static/5.mp4', 885, u"um, you know, we've just got one probability distribution, right? "), ('./static/5.mp4', 889, u'So in terms of what words we predict, '), ('./static/5.mp4', 891, u"we're predicting exactly the same probability distribution, every position. "), ('./static/5.mp4', 895, u"We've sort of saying the most likely word one to the left "), ('./static/5.mp4', 898, u'is whatever it is house or most likely word to the left is house, '), ('./static/5.mp4', 902, u'three to the left is house, '), ('./static/5.mp4', 904, u'the one to the right should be house too, right? '), ('./static/5.mp4', 906, u"So, it's sort of no sort of find us a prediction, "), ('./static/5.mp4', 909, u"it's just an overall kind of "), ('./static/5.mp4', 911, u'probability distribution of words that are likely to occur in my context. '), ('./static/5.mp4', 916, u"So, all we're asking for is a model that gives "), ('./static/5.mp4', 919, u'reasonably high probability estimates to all words that '), ('./static/5.mp4', 923, u'occur in the context of this word relatively often, '), ('./static/5.mp4', 927, u'is nothing more to it than that. '), ('./static/5.mp4', 929, u"And that's part of why it's sort of surprising when you've got "), ('./static/5.mp4', 932, u'such a simplistic thing that it seems like at the end of the day, '), ('./static/5.mp4', 936, u'it can end up capturing so much about '), ('./static/5.mp4', 938, u'the meanings of words and aspects of the meanings of words, '), ('./static/5.mp4', 942, u"like in the examples I've just showing you in the IPython Notebook. "), ('./static/5.mp4', 947, u"Um, and [NOISE] there's one other thing I was gonna say, oh yeah, "), ('./static/5.mp4', 953, u'one other thing I was gonna say was the other thing that might occur to you from this is, '), ('./static/5.mp4', 959, u'um, well, wait a minute, '), ('./static/5.mp4', 960, u'there was like that and-and, '), ('./static/5.mp4', 963, u'and-of that occur all the time. '), ('./static/5.mp4', 966, u'Um, so that means every word must have a high dot product with words like that and of and, '), ('./static/5.mp4', 976, u'um, they get their probabilities right. '), ('./static/5.mp4', 978, u'And the first answer to that is, "Yup, that\'s true." '), ('./static/5.mp4', 983, u'And it turns out that all word vectors, [NOISE] um, '), ('./static/5.mp4', 985, u'have a very strong prob- word probability component that reflects that. '), ('./static/5.mp4', 991, u'And I mean, one of the things that some workers discuss, '), ('./static/5.mp4', 994, u'so on the readings, '), ('./static/5.mp4', 996, u"there are two papers from Sanjeev Arora's group in Princeton and one of "), ('./static/5.mp4', 1001, u'those papers sort of discusses, um, this probability, '), ('./static/5.mp4', 1005, u'high frequency effect and your crude way of [NOISE] actually '), ('./static/5.mp4', 1009, u'fixing this high frequency effect is that normally, um, '), ('./static/5.mp4', 1013, u'the first, um, '), ('./static/5.mp4', 1015, u'the first biggest component in '), ('./static/5.mp4', 1018, u'your word vectors is actually a frequency effect and if you just lop it off, '), ('./static/5.mp4', 1022, u'you can make your semantic similarities better. '), ('./static/5.mp4', 1025, u'Um, but there are other things that we do to sort of deal with high frequencies. '), ('./static/5.mp4', 1029, u"Okay, so we get these lovely spaces that I've shown some of. "), ('./static/5.mp4', 1034, u"But I'll make one more remark. Um. "), ('./static/5.mp4', 1037, u'Yeah, so did I say this last time? Oh, oh. '), ('./static/5.mp4', 1041, u'Um, my remark anyway is that, '), ('./static/5.mp4', 1044, u'um, we show all these two-dimensional pictures. '), ('./static/5.mp4', 1048, u"They're exceedingly, exceedingly misleading because in these pic, "), ('./static/5.mp4', 1053, u'two-dimensional pictures, you know, '), ('./static/5.mp4', 1056, u'you have these effects that if, you know, '), ('./static/5.mp4', 1059, u'Samsung is close to Nokia, '), ('./static/5.mp4', 1062, u'it has to be over here and then it has to be far away from words that are over here. '), ('./static/5.mp4', 1067, u'Um, whereas you might sort of also want to have the effect that '), ('./static/5.mp4', 1070, u'Nokia is close to Finland for a different reason, '), ('./static/5.mp4', 1074, u"um, and you can't do that in two-dimensional, um, "), ('./static/5.mp4', 1077, u'vector spaces but, you know, one of the, um, '), ('./static/5.mp4', 1080, u'most of the properties of high dimensional vector spaces are very unintuitive, '), ('./static/5.mp4', 1085, u"and one of the ways that they're unintuitive is in a high dimensional vector space, "), ('./static/5.mp4', 1089, u'a word can be close to lots of other words in different directions. '), ('./static/5.mp4', 1095, u'Um, okay. So um, '), ('./static/5.mp4', 1098, u'we sort of started to talk about how we went about learning these word vectors. '), ('./static/5.mp4', 1104, u"I'm sort of going to take about a five minute detour into optimization. "), ('./static/5.mp4', 1111, u"Now, this isn't really an optimization class, "), ('./static/5.mp4', 1113, u'if you want to learn a lot about optimization. '), ('./static/5.mp4', 1116, u'Well you can learn more about optimization if you do '), ('./static/5.mp4', 1118, u"229 and if you do something like Stephen Boyd's optimization class, "), ('./static/5.mp4', 1122, u'you can learn a lot of optimization but this is '), ('./static/5.mp4', 1125, u"sort of really baby optimization but just to make sure everyone's on the same page, "), ('./static/5.mp4', 1129, u'here are three slides. '), ('./static/5.mp4', 1132, u'Right, so what we did at the end, '), ('./static/5.mp4', 1133, u'what we did over there, '), ('./static/5.mp4', 1135, u'where I apologized that my writing was too small, '), ('./static/5.mp4', 1139, u'but that will give you the chance to when doing homework too and you have to '), ('./static/5.mp4', 1143, u'write that out to work it out for yourselves and learn more in the process. '), ('./static/5.mp4', 1148, u'Right, so what we had was a cost function that we wanted to '), ('./static/5.mp4', 1151, u'minimize and so what we did was we did our bit of '), ('./static/5.mp4', 1155, u'calculus to calculate the gradient of the cost function with respect '), ('./static/5.mp4', 1160, u'to our word vectors which were our variables theta and then what we want to do is say, '), ('./static/5.mp4', 1166, u'well if we take a small step in '), ('./static/5.mp4', 1170, u'the direction of the negative of the gradient that will be taking us down, '), ('./static/5.mp4', 1175, u'down hill in this space and we want to keep on '), ('./static/5.mp4', 1178, u'doing that and sort of head to the minimum of our space. '), ('./static/5.mp4', 1182, u'I mean, of course in our high multi-dimensional space, '), ('./static/5.mp4', 1185, u'you know, it might not be a nice smooth curve like this. '), ('./static/5.mp4', 1188, u"It might be a horrible and non-convex curve but that's just the idea. "), ('./static/5.mp4', 1192, u"So, essentially we're saying we've got the old parameters, "), ('./static/5.mp4', 1196, u'we work out the gradient of the objective function using those old parameters. '), ('./static/5.mp4', 1201, u'We multiply that by a small alpha which is '), ('./static/5.mp4', 1205, u'our step size or learning rate because we only want to move a '), ('./static/5.mp4', 1208, u'little bit each time because if back here, '), ('./static/5.mp4', 1211, u'if we sort of said downhill is this way and said, '), ('./static/5.mp4', 1215, u'"Great let\'s go a long way that way." '), ('./static/5.mp4', 1216, u'You could kind of completely overshoot, '), ('./static/5.mp4', 1218, u'so we only want to go a little bit each time. '), ('./static/5.mp4', 1221, u'So we normally have a small learning rate alpha and so we '), ('./static/5.mp4', 1224, u'subtract a small multiple of the gradient and we, '), ('./static/5.mp4', 1228, u'from the old parameters and we get '), ('./static/5.mp4', 1230, u'our new parameters and that sort of effectively being worked out, '), ('./static/5.mp4', 1234, u'component wise as is shown below, '), ('./static/5.mp4', 1237, u"that we're just doing that to each of the partial derivatives and then, "), ('./static/5.mp4', 1240, u'that our hope is that that will let us gradually walk down this surface. '), ('./static/5.mp4', 1245, u'Now, if you actually did this, '), ('./static/5.mp4', 1248, u'it would be unbelievably bad for the kind of '), ('./static/5.mp4', 1250, u"systems that we build and there's a lot of work on "), ('./static/5.mp4', 1254, u'clever optimization but the most basic thing '), ('./static/5.mp4', 1257, u'which you definitely need to know is that well, '), ('./static/5.mp4', 1262, u'our objective function here, '), ('./static/5.mp4', 1264, u'J of theta was a function of our entire corpus, right? '), ('./static/5.mp4', 1269, u'And to get this to work well, '), ('./static/5.mp4', 1271, u'the first thing you want to do is, '), ('./static/5.mp4', 1272, u'you know collect a few billion words of your favorite language and then say, '), ('./static/5.mp4', 1277, u'"Go and build a Word2Vec model for me, " and so, '), ('./static/5.mp4', 1281, u'if you have to evaluate '), ('./static/5.mp4', 1283, u'a billion center words and maybe then to- for each of 10 billion context words, '), ('./static/5.mp4', 1290, u'if you have a window size of five and you- so you have to do these sort of 10 billion um, '), ('./static/5.mp4', 1295, u'Softmax calculations before you work out what your gradient is, '), ('./static/5.mp4', 1300, u"that you're going to be having your computer compute for a quite a long time before "), ('./static/5.mp4', 1305, u'you make one little step in the gradient and so things are going to go so, so slowly. '), ('./static/5.mp4', 1310, u'So, no one does that in deep learning systems. '), ('./static/5.mp4', 1313, u'Um, so what people- everyone does is use '), ('./static/5.mp4', 1316, u'stochastic gradient descent and in stochastic gradient descent, '), ('./static/5.mp4', 1320, u'we sample our window in the simplest case. '), ('./static/5.mp4', 1325, u'We, just for this one window, '), ('./static/5.mp4', 1328, u'work out an estimate of the gradient and we use it as a parameter update. '), ('./static/5.mp4', 1333, u'So, this is sort of an amazingly, '), ('./static/5.mp4', 1336, u'amazingly noisy estimate of '), ('./static/5.mp4', 1338, u"the gradient but it sort of doesn't matter too much because as soon as we've done it, "), ('./static/5.mp4', 1343, u"we're going to choose a different center word and do it again and again, "), ('./static/5.mp4', 1346, u"so that gradually we sort of approach what we would have gotten if we'd sort "), ('./static/5.mp4', 1350, u'of looked at all of the center words before we took any steps, '), ('./static/5.mp4', 1354, u'but because we take steps as we go, '), ('./static/5.mp4', 1357, u'we get to the minimum of the function orders and magnitude more quickly. '), ('./static/5.mp4', 1363, u"So thi- this shows the simplest case where we're just sampling one window. "), ('./static/5.mp4', 1368, u"In practice, that's not what we normally do. "), ('./static/5.mp4', 1371, u'We normally sample as- a small bunch, '), ('./static/5.mp4', 1375, u'you know, order of approximately 32 or 64. '), ('./static/5.mp4', 1379, u"Um, so if we have a sample that's bigger, "), ('./static/5.mp4', 1383, u"that's generally referred to as a mini-batch and we "), ('./static/5.mp4', 1385, u'calculate a gradient estimate from the mini-batch. '), ('./static/5.mp4', 1389, u'Um, so that has two advantages. '), ('./static/5.mp4', 1392, u'One advantage is that you kind of get less noisy estimates of '), ('./static/5.mp4', 1396, u"the gradient because you've kind of averaged "), ('./static/5.mp4', 1398, u'over a bunch of examples rather than just using one, '), ('./static/5.mp4', 1401, u'but the second advantage, '), ('./static/5.mp4', 1403, u'which is the one why we really care, '), ('./static/5.mp4', 1405, u"is if we want our computations to go fast when we're using a GPU, "), ('./static/5.mp4', 1410, u'that you need to get parallelization of doing the same operation a whole bunch of '), ('./static/5.mp4', 1416, u'times and then you gain a lot by using '), ('./static/5.mp4', 1418, u'a mini-batch of 64 examples or something like that. '), ('./static/5.mp4', 1422, u"Um, and you don't have to but you know, "), ('./static/5.mp4', 1424, u'it turns out the details of the guts of the hardware that you know, '), ('./static/5.mp4', 1427, u"it isn't- [inaudible] GPUs, you know, they have these, "), ('./static/5.mp4', 1430, u'whatever they have inside them, '), ('./static/5.mp4', 1432, u'there in powers of two. '), ('./static/5.mp4', 1433, u'So, you get better speedups if you use batches like 32 or 64, '), ('./static/5.mp4', 1437, u'rather than just deciding that 42 is still your favorite number from '), ('./static/5.mp4', 1441, u"high school [LAUGHTER] and you're going to use that as the size of your mini-batch. "), ('./static/5.mp4', 1447, u"Okay. um, yeah here's "), ('./static/5.mp4', 1450, u'one other interesting thing which '), ('./static/5.mp4', 1453, u'actually has some optimization details in it, it turns out. '), ('./static/5.mp4', 1457, u'Um, if you think of these um, '), ('./static/5.mp4', 1459, u'doing stochastic gradients with word vectors, '), ('./static/5.mp4', 1462, u"that's actually very different to "), ('./static/5.mp4', 1464, u'some other deep learning problems like vision deep learning problems. '), ('./static/5.mp4', 1468, u'Because for either a single window or even a sort of a reasonably sized mini-batch, '), ('./static/5.mp4', 1473, u'it will turn out that those mini-batch, '), ('./static/5.mp4', 1476, u'mini-batch only has, you know, '), ('./static/5.mp4', 1479, u'relatively speaking a handful of words in it, right? '), ('./static/5.mp4', 1481, u'So, if you have mini-batch of size 32 and a window size of ten, '), ('./static/5.mp4', 1485, u'you know, probably there are only about a 100,150 different words in it. '), ('./static/5.mp4', 1490, u"Um, but yet we're building this model over "), ('./static/5.mp4', 1493, u'a vocabulary of quarter of a million words or something like that. '), ('./static/5.mp4', 1496, u'So, just about all of the elements in this vector are zero. '), ('./static/5.mp4', 1500, u'Um, and so, um, '), ('./static/5.mp4', 1503, u'we sort of really have this very sparse um, '), ('./static/5.mp4', 1506, u'perimeter update and so, um, '), ('./static/5.mp4', 1510, u'that sort of suggests that we actually probably um, '), ('./static/5.mp4', 1514, u'want to sort of only update the word vectors that '), ('./static/5.mp4', 1517, u'appear and then the question is whether you can achieve that, right? '), ('./static/5.mp4', 1521, u"The dumb way to do it, is you just have this matrix that's normally, "), ('./static/5.mp4', 1525, u'nearly all zeros and you say add '), ('./static/5.mp4', 1527, u'those two matrices together and there you go and then the question is, '), ('./static/5.mp4', 1533, u'can you actually have a sparse matrix update which only updates '), ('./static/5.mp4', 1538, u'the certain rows of the matrix that contain '), ('./static/5.mp4', 1541, u"the words that you've entered and do things much faster? "), ('./static/5.mp4', 1544, u"And if you're doing something even cleverer like doing "), ('./static/5.mp4', 1547, u'distributed computation over multiple computers and sharing your parameters, '), ('./static/5.mp4', 1552, u'well then definitely you just sort of only want to update '), ('./static/5.mp4', 1554, u"the word vectors that you've actually been getting a parameter estimate for. "), ('./static/5.mp4', 1558, u"So, there's sort of some details there but I'm "), ('./static/5.mp4', 1560, u'going to skip past them for more details, um. '), ('./static/5.mp4', 1563, u'Right. So, a couple of people asked afterwards, yeah, '), ('./static/5.mp4', 1568, u'why are there these two word vectors that sort of center and the outside one? '), ('./static/5.mp4', 1574, u'And, I mean the answer to that is, '), ('./static/5.mp4', 1576, u'it makes that math I showed you easy, right? '), ('./static/5.mp4', 1579, u'So that if, um, '), ('./static/5.mp4', 1581, u'if you do it as I showed you, well, '), ('./static/5.mp4', 1585, u'you know, for working out, um, '), ('./static/5.mp4', 1588, u'the partial derivatives for the center word. '), ('./static/5.mp4', 1591, u"It's just as I showed you, it's easy. "), ('./static/5.mp4', 1595, u'Um, but if you use only one set of word vectors, '), ('./static/5.mp4', 1599, u'well then the same word, '), ('./static/5.mp4', 1601, u"that's the center word, "), ('./static/5.mp4', 1603, u'will be one of the choices for '), ('./static/5.mp4', 1605, u"the context word when you're working out that Softmax for the context word. "), ('./static/5.mp4', 1609, u"And then you'll get these terms that are then "), ('./static/5.mp4', 1612, u'squared terms in terms of the two references, '), ('./static/5.mp4', 1615, u'so that same word, '), ('./static/5.mp4', 1617, u'and that makes your math more difficult. '), ('./static/5.mp4', 1619, u"Um, so it's sort of just a practical thing, "), ('./static/5.mp4', 1623, u'um, in the end. '), ('./static/5.mp4', 1625, u"I mean it sort of doesn't make very much difference, "), ('./static/5.mp4', 1628, u"because if you sort of think about it since you're going along through all the, "), ('./static/5.mp4', 1632, u'um, positions, you know. '), ('./static/5.mp4', 1633, u'What was a center word at one point is immediately afterwards '), ('./static/5.mp4', 1637, u'a context word of what used to be a context word, '), ('./static/5.mp4', 1640, u'which is now the center word. '), ('./static/5.mp4', 1641, u'So, sort of doing the same computations because, you know, '), ('./static/5.mp4', 1646, u'the dot product is symmetric actually, '), ('./static/5.mp4', 1648, u'um, all over again. '), ('./static/5.mp4', 1650, u'So, they get pretty similar vector representations. '), ('./static/5.mp4', 1654, u'So, it seems like in general you can get the best results '), ('./static/5.mp4', 1656, u'by averaging what comes out for your two vectors, '), ('./static/5.mp4', 1659, u'and you end up with just one vector per word. '), ('./static/5.mp4', 1661, u'Okay, more substantively, um, '), ('./static/5.mp4', 1665, u'if you go to the word2vec paper, '), ('./static/5.mp4', 1668, u"you'll discover that there's sort of more to word2vec that they "), ('./static/5.mp4', 1671, u'define as sort of a family of word2vec models. '), ('./static/5.mp4', 1675, u'And there are so two main parts of that family. '), ('./static/5.mp4', 1679, u"Um, firstly, there's a choice between the Continuous Bag of Words model, "), ('./static/5.mp4', 1683, u'and the skip-grams model. '), ('./static/5.mp4', 1685, u'And what I presented with the skip-grams models. '), ('./static/5.mp4', 1687, u'So, in the skip-grams model, '), ('./static/5.mp4', 1689, u"you've got one center word and you're trying to "), ('./static/5.mp4', 1692, u'predict all the words in context one at a time. '), ('./static/5.mp4', 1695, u"For the Continuous Bag of Words model it's the opposite. "), ('./static/5.mp4', 1699, u"You've got all of the outside words and you're trying to use all of them, "), ('./static/5.mp4', 1703, u'though considered independently like a Naive Bayes model to predict the center word. '), ('./static/5.mp4', 1709, u'Um, and then the second one is, um, '), ('./static/5.mp4', 1714, u'the way I presented learning this was '), ('./static/5.mp4', 1717, u"the method that's using the so called Naive Softmax. "), ('./static/5.mp4', 1721, u'So, therefore when we are wanting to work things out, '), ('./static/5.mp4', 1724, u'we were sort of saying okay we want probability estimates for the context words, '), ('./static/5.mp4', 1729, u"and so we're just going to sum over "), ('./static/5.mp4', 1730, u"the whole vocabulary and we'll come up with these probability estimates. "), ('./static/5.mp4', 1735, u'Um, in practice, that turns out to be a sort of '), ('./static/5.mp4', 1739, u'a bad idea because that would also make things mega slow. '), ('./static/5.mp4', 1743, u'So, in homework two, '), ('./static/5.mp4', 1745, u'coming up next week, um, '), ('./static/5.mp4', 1748, u'you will get to implement a much more practical, um, '), ('./static/5.mp4', 1751, u'way of doing this which they present in the word2vec papers, right? '), ('./static/5.mp4', 1755, u'So, the problem is, '), ('./static/5.mp4', 1756, u"if we're using this equation that we use to do the calculus, "), ('./static/5.mp4', 1761, u'that down in this denominator here, '), ('./static/5.mp4', 1763, u"we're doing the sum over the entire vocabulary. "), ('./static/5.mp4', 1766, u'So, if you have a vocabulary of a quarter million words, '), ('./static/5.mp4', 1769, u"we're sort of doing a quarter of a million dot products and "), ('./static/5.mp4', 1771, u'exponentials and adding them all to work out that denominator. '), ('./static/5.mp4', 1776, u'And that sort of seems uh, '), ('./static/5.mp4', 1778, u'sort of a really bad idea if you want things to be fast. '), ('./static/5.mp4', 1781, u'Um, so, um, Tomas Mikolov and '), ('./static/5.mp4', 1784, u'colleagues came up with this idea of negative sampling would be near enough. '), ('./static/5.mp4', 1788, u'And so the idea of negative sampling, '), ('./static/5.mp4', 1791, u"is we're going to train binary logistic regressions instead. "), ('./static/5.mp4', 1795, u"And so, we're going to train one binary logistic regression "), ('./static/5.mp4', 1799, u"for the actual word observed what's in the numerator, "), ('./static/5.mp4', 1804, u'and you want to give high probability to the word that was actually observed. '), ('./static/5.mp4', 1809, u"And then, what we're going to do, "), ('./static/5.mp4', 1811, u"is we're going to sort of randomly sample a bunch of other words, "), ('./static/5.mp4', 1815, u"they're the negative samples and say they weren't the ones that were actually seen. "), ('./static/5.mp4', 1821, u'So, you should be trying to give them as low a probability as possible. '), ('./static/5.mp4', 1826, u'Okay, so, um, the sort of notation that '), ('./static/5.mp4', 1830, u"they use in the paper is sort of slightly different to the one I've used. "), ('./static/5.mp4', 1835, u'They actually do maximization not minimization, '), ('./static/5.mp4', 1838, u"and that's the equation which I'll come back to. "), ('./static/5.mp4', 1842, u"Um, though before we do that here's the sigmoid function. "), ('./static/5.mp4', 1846, u'So, the sigmoid function is normally written like this, '), ('./static/5.mp4', 1849, u'one over one plus E to the minus X. '), ('./static/5.mp4', 1851, u'But, um, essentially, '), ('./static/5.mp4', 1854, u'the sigmoid function is like a binary case of the Softmax function, right? '), ('./static/5.mp4', 1859, u'That we have two possible outcomes, yes or no, '), ('./static/5.mp4', 1863, u"and that you're sort of again got an input that is any real number, "), ('./static/5.mp4', 1867, u"and it's mapping it onto a probability distribution between "), ('./static/5.mp4', 1871, u'zero and one which represents these two binary outcomes. '), ('./static/5.mp4', 1874, u'And to the extent that the number is positive, '), ('./static/5.mp4', 1876, u'it kind of ceilings to one and negative goes down to zero. '), ('./static/5.mp4', 1881, u'Okay, so with this time, '), ('./static/5.mp4', 1883, u"we're going to take the dot for- for the good word, "), ('./static/5.mp4', 1886, u"we're going to take the dot product of the two vectors, "), ('./static/5.mp4', 1889, u'shove it through our sigmoid function '), ('./static/5.mp4', 1891, u"and then we're going to want that probability estimate, "), ('./static/5.mp4', 1894, u'um, to be as high as possible. '), ('./static/5.mp4', 1896, u'So, if I show you this version, '), ('./static/5.mp4', 1898, u'which is just written slightly differently, um, '), ('./static/5.mp4', 1901, u'to look as much as possible like the notation that we used last time, '), ('./static/5.mp4', 1906, u'here is our new objective function for using negative sampling. '), ('./static/5.mp4', 1910, u"And we've got two terms, "), ('./static/5.mp4', 1912, u'the first one, um, '), ('./static/5.mp4', 1914, u'is the log of the sigmoid of the observed context word, '), ('./static/5.mp4', 1919, u'the outside words, dot producted with the center word, '), ('./static/5.mp4', 1922, u"and we're going to want that to be big. "), ('./static/5.mp4', 1925, u'Um, and then on the other hand, '), ('./static/5.mp4', 1928, u"um, we've got, um, the, "), ('./static/5.mp4', 1932, u'um, randomly chosen K words, '), ('./static/5.mp4', 1936, u'which are just other words, '), ('./static/5.mp4', 1938, u"and we're going to work out dot products between them and the center word. "), ('./static/5.mp4', 1941, u"And we're going to want those to be as small as possible. "), ('./static/5.mp4', 1945, u'Um, note that extra minus sign in there which is causing '), ('./static/5.mp4', 1948, u'the sign of the two things to be different, right? '), ('./static/5.mp4', 1951, u'So, those are our negative samples. '), ('./static/5.mp4', 1954, u'And for big K, it can be a reasonably modest number, '), ('./static/5.mp4', 1957, u'you can just take kind of 10, '), ('./static/5.mp4', 1958, u'15 negative samples and that works pretty fine. '), ('./static/5.mp4', 1962, u'Um, I said we sort of sampled some words, '), ('./static/5.mp4', 1966, u'um, to be the negative samples. '), ('./static/5.mp4', 1968, u'They in particular propose a sampling distribution that helps them along '), ('./static/5.mp4', 1974, u'a little in partly dealing with this pro- problem of very frequent words. '), ('./static/5.mp4', 1980, u'Um, so the starting point of how you sample words is you '), ('./static/5.mp4', 1983, u'use what we call the- the unigram distribution. '), ('./static/5.mp4', 1987, u'So, that just means you take words on a large corpus and count up '), ('./static/5.mp4', 1991, u'how often each one occurs just as a count of independent words, '), ('./static/5.mp4', 1996, u"so there's the called unigram counts. "), ('./static/5.mp4', 1998, u'And so you start off with unigram counts, '), ('./static/5.mp4', 2000, u'but then you raise them to the three quarters power. '), ('./static/5.mp4', 2003, u'And raising to the three quarters power, '), ('./static/5.mp4', 2005, u'has the effect of, um, '), ('./static/5.mp4', 2007, u'decreasing how often you sample very common words, '), ('./static/5.mp4', 2010, u'and increasing how often you sample rarer words. '), ('./static/5.mp4', 2015, u"Okay, um, and that's that. "), ('./static/5.mp4', 2019, u"Okay, so that's everything about word2vec I am going to say. "), ('./static/5.mp4', 2023, u'Anyone have any last thing. '), ('./static/5.mp4', 2028, u'Yes. [NOISE] '), ('./static/5.mp4', 2032, u'Oh, oh [NOISE]. This is a- sorry Z, '), ('./static/5.mp4', 2035, u'that capital Z is often used as a normalization term and so this is saying, '), ('./static/5.mp4', 2040, u'well if you want the probability distribution of words, '), ('./static/5.mp4', 2043, u'is you work out this three quarters power of the count '), ('./static/5.mp4', 2046, u'of the word for every word in the vocabulary and then these '), ('./static/5.mp4', 2050, u"numbers you just sum them up over the vocabulary and it'll be sum "), ('./static/5.mp4', 2053, u"total and we're dividing by that so we get a probability distribution. "), ('./static/5.mp4', 2057, u"Good question because i hadn't explained that. "), ('./static/5.mp4', 2059, u'Um, in this class, when you see the letter Z with no explanation, '), ('./static/5.mp4', 2063, u'it normally means I am a normalization term to turn things into '), ('./static/5.mp4', 2068, u'probabilities and you sort of iterate over '), ('./static/5.mp4', 2071, u'the numerator term and summing them and divide through. '), ('./static/5.mp4', 2074, u"Any other questions of things I haven't explained or otherwise? Yes. "), ('./static/5.mp4', 2080, u"So the window [inaudible] that's a [inaudible] "), ('./static/5.mp4', 2085, u'Yeah, yes. '), ('./static/5.mp4', 2086, u'So, [NOISE] what size window do you use? '), ('./static/5.mp4', 2088, u"I'll actually come back to that in a bit and show a little bit of data on that, "), ('./static/5.mp4', 2092, u"but yeah, we haven't done anything about that. "), ('./static/5.mp4', 2094, u"At the moment we're guessing a window size like five, "), ('./static/5.mp4', 2097, u"which isn't a bad one um, "), ('./static/5.mp4', 2098, u"but you know there isn't- there hasn't really been any science behind that, um, "), ('./static/5.mp4', 2104, u"that people treat that as what's then called a hyperparameter which means that um, "), ('./static/5.mp4', 2109, u'you try a few different numbers and see which one seems '), ('./static/5.mp4', 2112, u"best and that's the one that you use in your future work. Yeah. "), ('./static/5.mp4', 2117, u'Um, [inaudible] three quarters power '), ('./static/5.mp4', 2119, u'chosen for any theoretical reason or just because it seems to work in practice? '), ('./static/5.mp4', 2124, u'Um, no. Um, that, '), ('./static/5.mp4', 2127, u'that was um, also chosen as a hyperparameter and improved performance. '), ('./static/5.mp4', 2133, u'I mean, actually um, you know, '), ('./static/5.mp4', 2135, u'for this Word2Vec paper, I mean, '), ('./static/5.mp4', 2138, u'you know, it turns out that um, '), ('./static/5.mp4', 2140, u'in the actual paper um, '), ('./static/5.mp4', 2143, u'the model looks very- fairly clean but what '), ('./static/5.mp4', 2147, u"people's discovered when they started digging through the code, "), ('./static/5.mp4', 2151, u'which to- to their credit they did make available, reproducible research, '), ('./static/5.mp4', 2156, u'that there are actually a whole bunch of tricks '), ('./static/5.mp4', 2159, u'of different things like these hyperparameters of um, '), ('./static/5.mp4', 2163, u'how you sample, and how you wait windows and various things to make the numbers better. '), ('./static/5.mp4', 2168, u'So, you know, people play quite a few tricks to make '), ('./static/5.mp4', 2171, u"the numbers go up which aren't particularly theoretical. "), ('./static/5.mp4', 2175, u'Are we good? '), ('./static/5.mp4', 2177, u'Yeah. '), ('./static/5.mp4', 2193, u'[inaudible] [NOISE]. '), ('./static/5.mp4', 2199, u'Ah, sometimes. '), ('./static/5.mp4', 2202, u'I so- I- you- so in general for a lot of these sampling things, '), ('./static/5.mp4', 2208, u"it's a bad idea if you're going to be doing multiple passes if you just go bloom, "), ('./static/5.mp4', 2213, u'bloom, bloom and then bloom, bloom, bloom again, '), ('./static/5.mp4', 2215, u"that's a bad idea, "), ('./static/5.mp4', 2216, u'but a common technique a lot of the packages use '), ('./static/5.mp4', 2219, u'is that they do use this shuffling operation at the beginning. '), ('./static/5.mp4', 2222, u'So for each epoch, '), ('./static/5.mp4', 2224, u"they'll shuffle the data randomly and then they'll go through it in sequence and that has "), ('./static/5.mp4', 2228, u'the benefits of faster computation from locality et cetera um, '), ('./static/5.mp4', 2233, u'while ha- meaning that when you do it differently epoch, '), ('./static/5.mp4', 2236, u'it will work out differently. '), ('./static/5.mp4', 2238, u'Uh, yeah, yeah. '), ('./static/5.mp4', 2248, u'[inaudible] [NOISE] [inaudible]. '), ('./static/5.mp4', 2248, u'That last question I think was talking about taking the mini-batches from the corpus and '), ('./static/5.mp4', 2254, u'contrasting whether you actually say sample 20 '), ('./static/5.mp4', 2257, u'randomly from the whole corpus versus just sort of working from left to right. '), ('./static/5.mp4', 2261, u'Yes, do you have a question? '), ('./static/5.mp4', 2263, u'Um, yeah [inaudible] [NOISE]. '), ('./static/5.mp4', 2275, u'Yeah. So- so you could argue- you could argue '), ('./static/5.mp4', 2279, u'whether or not this was written in the clearest way, but, right. '), ('./static/5.mp4', 2282, u"So, we're making this dot product and then we're negating it "), ('./static/5.mp4', 2287, u"which is then flipping which side of the space we're on, right? "), ('./static/5.mp4', 2291, u'Because the sigmoid is symmetric around zero. '), ('./static/5.mp4', 2295, u"So, if we've got some dot product um, "), ('./static/5.mp4', 2299, u'and then we negate it, '), ('./static/5.mp4', 2300, u"we're sort of working out a one minus probability and so "), ('./static/5.mp4', 2305, u"that's the way in which we're actually for the first um, "), ('./static/5.mp4', 2310, u"for the first time we're wanting the probability to be "), ('./static/5.mp4', 2312, u'high and then for the negative samples, '), ('./static/5.mp4', 2315, u"we're wanting their probability to be low. "), ('./static/5.mp4', 2318, u"Okay, I'll maybe run ahead now. "), ('./static/5.mp4', 2322, u'Um, so this was an algorithm which um, '), ('./static/5.mp4', 2327, u"sort of you're going through this corpus position by position and you're sort of doing "), ('./static/5.mp4', 2333, u"this prediction of words and then you're "), ('./static/5.mp4', 2336, u"updating some parameters and you're learning something and you know, "), ('./static/5.mp4', 2340, u'by job it seemed to work based on what we saw in the examples, '), ('./static/5.mp4', 2344, u'but you know, you might have thought um, '), ('./static/5.mp4', 2347, u'that that was kind of weird right? '), ('./static/5.mp4', 2348, u'Look we have this whole big pile of data you know, '), ('./static/5.mp4', 2352, u"sort of traditional, I'm thinking of statistics, right? "), ('./static/5.mp4', 2356, u'So you have a big pile of data, '), ('./static/5.mp4', 2358, u'you aggregate it and it sort of seems like there are obvious things you could do here. '), ('./static/5.mp4', 2362, u"You could say, well there's a word like, "), ('./static/5.mp4', 2364, u"whatever word we're using, banana. "), ('./static/5.mp4', 2367, u"Let's just see what words occur in the context of the gut banana and count "), ('./static/5.mp4', 2371, u"them all up and then we'll be able to use those to predict somehow and you know, "), ('./static/5.mp4', 2375, u'those kinds of methods were traditionally '), ('./static/5.mp4', 2378, u'used including even with distributed representation techniques. '), ('./static/5.mp4', 2383, u'Um, so I want to say a bit about that, '), ('./static/5.mp4', 2385, u"so you're fully educated and don't sound like one of those people who were "), ('./static/5.mp4', 2389, u"aware of no work that happened before 2013 when your network's took off. "), ('./static/5.mp4', 2395, u'Um, okay. So, what we could do is we can essentially '), ('./static/5.mp4', 2399, u'do the same thing as sort of Word2Vec. '), ('./static/5.mp4', 2403, u"We could say there's a five word window around "), ('./static/5.mp4', 2407, u"each word instance that's often referred to as a word token, right? "), ('./static/5.mp4', 2411, u'So at NLP, we often want to distinguish between a particular kind of type like banana '), ('./static/5.mp4', 2417, u'or apple versus particular instances '), ('./static/5.mp4', 2420, u"often in the text and that's referred to as sort of a type token distinction. "), ('./static/5.mp4', 2423, u'So we could, um, '), ('./static/5.mp4', 2426, u'look at each um token with a word, '), ('./static/5.mp4', 2429, u'and the words five around that, '), ('./static/5.mp4', 2431, u'and then we could so start counting up which words occur, '), ('./static/5.mp4', 2434, u'occur with it and so we can then have a matrix of co-occurrence counts. '), ('./static/5.mp4', 2440, u'Um, okay. '), ('./static/5.mp4', 2442, u"So, we'll have again, "), ('./static/5.mp4', 2444, u"and I'm going to give me an example of this. "), ('./static/5.mp4', 2446, u'So, normally again you use the five to 10 but you know I can just '), ('./static/5.mp4', 2449, u'use a window of one to keep my counts very simple and small. '), ('./static/5.mp4', 2453, u'I ignore left or right just like Word2Vec did, '), ('./static/5.mp4', 2456, u'and so if I have a teeny baby corpus like this, '), ('./static/5.mp4', 2459, u'you know, what I could do, '), ('./static/5.mp4', 2460, u'is just say here is the matrix of word co-occurrence accounts. '), ('./static/5.mp4', 2465, u'So, within my window size of one, '), ('./static/5.mp4', 2468, u'I occurs next to like twice, '), ('./static/5.mp4', 2470, u"and that means that like occurs next I twice it's symmetric, "), ('./static/5.mp4', 2474, u'and all my other accounts here are singletons, um. '), ('./static/5.mp4', 2478, u'And so this gives me a big huge sparse matrix of word co-occurrence accounts. '), ('./static/5.mp4', 2485, u'And so one thing that you could do is just use this matrix directly, '), ('./static/5.mp4', 2490, u"because I haven't really got enough data here. "), ('./static/5.mp4', 2492, u'But, you know, if you sort of, '), ('./static/5.mp4', 2495, u'um, decided that, you know, '), ('./static/5.mp4', 2498, u'the word like is like the word learning, '), ('./static/5.mp4', 2500, u"what you'd do is you'd expect that "), ('./static/5.mp4', 2502, u'these two vectors would end up kind of similar to each other. '), ('./static/5.mp4', 2506, u'And [NOISE] they do. '), ('./static/5.mp4', 2508, u'So, you can just measure, um, '), ('./static/5.mp4', 2510, u'similarity of the vectors directly in terms of these co-occurrence counts. '), ('./static/5.mp4', 2515, u"But, you know, it's a little bit unappealing doing things this way, right? "), ('./static/5.mp4', 2519, u"If you have a quarter million word vocabulary that's where "), ('./static/5.mp4', 2523, u'you are in this space where my math is bad, '), ('./static/5.mp4', 2526, u"but it's in the trillions of the number of cells of this matrix, "), ('./static/5.mp4', 2529, u'might require a lot of storage. '), ('./static/5.mp4', 2531, u"Though if you're clever and notice that most of the cells were zero and could do "), ('./static/5.mp4', 2535, u'some clever sparse matrix representation might take a little bit less. '), ('./static/5.mp4', 2540, u'Um, your classification models might have sparsity issues cause, you know, '), ('./static/5.mp4', 2543, u"a lot of those cells aren't present and so it might not be very robust. "), ('./static/5.mp4', 2547, u'And so those are traditional answer to all of these things which is well, '), ('./static/5.mp4', 2551, u'maybe we could have that big co-occurrence counts matrix '), ('./static/5.mp4', 2556, u'and somehow reduce its dimensionality of just, um, '), ('./static/5.mp4', 2560, u'find a corresponding low dimensional matrix which preserves, '), ('./static/5.mp4', 2565, u'uh, most of the information, um, '), ('./static/5.mp4', 2567, u'in the original matrix and, you know, '), ('./static/5.mp4', 2570, u"maybe we'll reduce things to a dimensionality of somewhere around the size 25 to a 1,000, "), ('./static/5.mp4', 2576, u'um as is done with Word2Vec. '), ('./static/5.mp4', 2579, u"So, there's sort of a standard most common way of doing "), ('./static/5.mp4', 2582, u"this dimensionality reduction and you don't really have to understand all the math, "), ('./static/5.mp4', 2587, u'but you get to play with this and homework one which is, um, '), ('./static/5.mp4', 2590, u"for any matrix you can do what's called the singular value decomposition, um, "), ('./static/5.mp4', 2595, u'which is a way you can take an arbitrary matrix and decompose it into three matrices, um, '), ('./static/5.mp4', 2603, u'where the center one is diagonal and has what- in it what are '), ('./static/5.mp4', 2607, u'called singular vectors which are weightings of the different dimensions. '), ('./static/5.mp4', 2611, u'So, they decrease in size as you go downwards. '), ('./static/5.mp4', 2614, u'And then these two U and V are then '), ('./static/5.mp4', 2617, u'orthogonal bases corresponding to the rows and columns. '), ('./static/5.mp4', 2621, u'And so in particular, '), ('./static/5.mp4', 2623, u"it's even simpler than the case where we just have these word-word vectors, "), ('./static/5.mp4', 2627, u'because you have a square matrix and so they are effectively the same. '), ('./static/5.mp4', 2631, u'But, you know, for the general case, um, '), ('./static/5.mp4', 2633, u'although you get these sort of full orthogonal bases, '), ('./static/5.mp4', 2637, u"you then have these bits sort of don't really "), ('./static/5.mp4', 2640, u'matter cause they end up being used for nothing when you work out the product. '), ('./static/5.mp4', 2643, u'Um, and then if you want to reduce the dimensionality, what you say is, '), ('./static/5.mp4', 2649, u'throw away the smallest singular values which remember there are in decreasing '), ('./static/5.mp4', 2654, u"size and that means you're then effectively "), ('./static/5.mp4', 2657, u'throwing away rows and columns of these other matrices. '), ('./static/5.mp4', 2662, u'And then it says, '), ('./static/5.mp4', 2663, u"behold I've now reduced these things to "), ('./static/5.mp4', 2665, u'a two-dimensional representation from '), ('./static/5.mp4', 2668, u"the original three-dimensional representation and that's referred to as "), ('./static/5.mp4', 2672, u'the reduced SVD and the classic result is in terms of least squares error in '), ('./static/5.mp4', 2679, u'estimation that this- the product of these three things will give X k which is the best, '), ('./static/5.mp4', 2687, u'um, k- rank k approximation to the original X in terms of, '), ('./static/5.mp4', 2692, u'uh, X squared least squares criterion. '), ('./static/5.mp4', 2695, u'So, we could do this and we could build word vectors. '), ('./static/5.mp4', 2698, u'So, I can, um, '), ('./static/5.mp4', 2700, u'make use of, um, '), ('./static/5.mp4', 2702, u"NumPy's SVD function and I can throw into it, "), ('./static/5.mp4', 2707, u'um, matrices and, um, '), ('./static/5.mp4', 2711, u'I can make word vectors. '), ('./static/5.mp4', 2714, u'And these ones look really bad, but hey, '), ('./static/5.mp4', 2716, u"I give it a dataset of three centers [LAUGHTER] and it's not exactly a fair comparison. "), ('./static/5.mp4', 2720, u'But- so this technique was in, um, '), ('./static/5.mp4', 2723, u'popularized around, um, the turn- the turn of the millennium. '), ('./static/5.mp4', 2728, u'It generally, um, went for '), ('./static/5.mp4', 2730, u'some word applications under the name of latent semantic analysis or '), ('./static/5.mp4', 2734, u'latent semantic indexing and the idea was that you could have '), ('./static/5.mp4', 2738, u'these semantic directions that you are '), ('./static/5.mp4', 2741, u'finding in this low dimensional space that had meaning. '), ('./static/5.mp4', 2744, u'And people worked with it quite a bit for techniques '), ('./static/5.mp4', 2747, u'like t- trying to do information retrieval '), ('./static/5.mp4', 2750, u'using these LSA approximations and it sort of worked a bit. '), ('./static/5.mp4', 2756, u'It kind of never really worked very well I think, '), ('./static/5.mp4', 2762, u'um, and so it never sort of hugely caught on. '), ('./static/5.mp4', 2766, u"Um, but it's- the methods kind of continued to be explored actually mainly in the sort of "), ('./static/5.mp4', 2771, u'COG psych- COGS psych community where people were doing things with word meaning. '), ('./static/5.mp4', 2777, u"And there's this sort of kind of interesting, um, "), ('./static/5.mp4', 2780, u'the [NOISE] to the literature that there was this guy Doug Rohde, um, '), ('./static/5.mp4', 2784, u'who, um, did a PhD at CMU, um, in 2005. '), ('./static/5.mp4', 2791, u'And basically what he discovered was, '), ('./static/5.mp4', 2794, u'look if rather than just using raw counts, '), ('./static/5.mp4', 2797, u'I start doing quite a bit more in terms of, '), ('./static/5.mp4', 2802, u'you know, fiddling with the counts, '), ('./static/5.mp4', 2804, u'I can start to produce results that are much better. '), ('./static/5.mp4', 2807, u'So, rather than using low counts, '), ('./static/5.mp4', 2809, u'you have to do something to deal with those very high-frequency words. '), ('./static/5.mp4', 2813, u'So, one idea is you could log scale them which '), ('./static/5.mp4', 2815, u'is also commonly used in information retrieval. '), ('./static/5.mp4', 2818, u'Another idea is you could just use something like, '), ('./static/5.mp4', 2821, u'uh, a ceiling function, '), ('./static/5.mp4', 2823, u'so you take the minimum of X,t for t set and that some number like around 100. '), ('./static/5.mp4', 2829, u'Um, he had- he used the idea which was also another of the hacks that was put into '), ('./static/5.mp4', 2835, u'the Word2Vec was rather than just treating the whole window the same that you should, '), ('./static/5.mp4', 2841, u'um, count words that are closer more. '), ('./static/5.mp4', 2844, u'So, in Word2Vec, they sample closer words more commonly than further away words. '), ('./static/5.mp4', 2849, u"Um, in his system, you're sort of having to have "), ('./static/5.mp4', 2851, u'a differential count for closer words et cetera. '), ('./static/5.mp4', 2854, u'And then, um, compared to any of that rather than using counts at all, '), ('./static/5.mp4', 2860, u'he then started using Pearson correlations which '), ('./static/5.mp4', 2863, u"helped and set they're sometimes negative and he decided that it helped, "), ('./static/5.mp4', 2868, u'um, if you then got rid of the negative values. '), ('./static/5.mp4', 2872, u'So, in- in some sense, '), ('./static/5.mp4', 2874, u'this sounds like a bag of hacks, '), ('./static/5.mp4', 2876, u'um, but on the other hand, '), ('./static/5.mp4', 2878, u'he was able to show that, you know, '), ('./static/5.mp4', 2880, u'these transformed counts could actually then give '), ('./static/5.mp4', 2883, u"you very useful word vectors as I'm about to show. "), ('./static/5.mp4', 2887, u'And- well, we have to realize that actually in slightly different forms, '), ('./static/5.mp4', 2893, u'several of these exact same counts are actually being used in Word2Vec as well. '), ('./static/5.mp4', 2897, u'Do you hear that? '), ('./static/5.mp4', 2901, u'Yeah. Were they [inaudible]. '), ('./static/5.mp4', 2906, u"Yeah. So, so that's an- I'm about to show exactly that. "), ('./static/5.mp4', 2911, u"Um, that's actually a really interesting little, "), ('./static/5.mp4', 2914, u'um, bit of the data. '), ('./static/5.mp4', 2915, u'So, you know, what, um, yeah, '), ('./static/5.mp4', 2919, u'so the, the thing- if you do that, '), ('./static/5.mp4', 2922, u'you not only get word similarities pretty good. '), ('./static/5.mp4', 2924, u'Let me show you this example which is cleaner. '), ('./static/5.mp4', 2928, u'Um, so this- the precise idea of '), ('./static/5.mp4', 2932, u'evaluating with analogies was not something that had really been developed. '), ('./static/5.mp4', 2937, u'So, that was actually something that Marsh Mikolov, um, suggested. '), ('./static/5.mp4', 2941, u'But actually, um, Doug Rohde made this, um, '), ('./static/5.mp4', 2945, u'really interesting observation which was- he said, look, '), ('./static/5.mp4', 2951, u'"Once I do these kind of transformations to '), ('./static/5.mp4', 2954, u'improve the semantic representation of my word vectors, '), ('./static/5.mp4', 2957, u'look this really interesting property emerges. '), ('./static/5.mp4', 2961, u'Um, that what you find is that there is semantic vectors '), ('./static/5.mp4', 2967, u'are which basically linear components in my carefully-constructed space. '), ('./static/5.mp4', 2972, u'So, here we have the sort of, um, '), ('./static/5.mp4', 2975, u'verb to the doer of the verb direction, '), ('./static/5.mp4', 2978, u'drive, driver, um, clean, '), ('./static/5.mp4', 2980, u'janitor, swim, swimmer, learn, '), ('./static/5.mp4', 2983, u'teacher or teach, teacher, '), ('./static/5.mp4', 2985, u'doctor, treat, priest, pray. '), ('./static/5.mp4', 2988, u"I mean, you know, it's not exactly perfect, "), ('./static/5.mp4', 2991, u"you know, there's a little bit of wiggle there, right? "), ('./static/5.mp4', 2992, u"But, you know, roughly it's completely clear that there's sort of a direction "), ('./static/5.mp4', 2997, u'in the space that corresponds to- from a verb to the doers of a verb. '), ('./static/5.mp4', 3003, u'Um, and yeah, so he [inaudible] - he- '), ('./static/5.mp4', 3006, u'no one had thought of this idea of doing the analogies and tests. '), ('./static/5.mp4', 3010, u"But the thing in retrospect that's obvious is, "), ('./static/5.mp4', 3014, u'if you can construct a vector space that has this linearity property, '), ('./static/5.mp4', 3020, u"then you're definitely gonna do well in analogy. "), ('./static/5.mp4', 3023, u'So, effectively he had invented a vector space that do '), ('./static/5.mp4', 3026, u"well in analogies because this means that you've got "), ('./static/5.mp4', 3029, u'this direction which is the doer and then you can immediately '), ('./static/5.mp4', 3033, u"say that's the doer vector which you can get from subtracting clean from swimmer. "), ('./static/5.mp4', 3038, u"And the- Right. So, it's clean from janitor. "), ('./static/5.mp4', 3040, u"And then we can add it on to swim and we'll get somewhere close to swimmer. "), ('./static/5.mp4', 3045, u'Um, so his space actually did do that. '), ('./static/5.mp4', 3048, u'And so, um, this is- so the, '), ('./static/5.mp4', 3051, u'the moral in some sense is, '), ('./static/5.mp4', 3053, u'if you have- if you kind of do carefully control accounts and so on, '), ('./static/5.mp4', 3058, u'that conventional methods can also give you good word vector spaces and- I mean, '), ('./static/5.mp4', 3064, u'so that was actually the starting off point for our work on GloVe. '), ('./static/5.mp4', 3069, u'Um, so that essentially, '), ('./static/5.mp4', 3071, u'there had been these two schools of work. '), ('./static/5.mp4', 3073, u'Um, there had been the school of work that had '), ('./static/5.mp4', 3076, u'been explored more in COG psych than anywhere else, '), ('./static/5.mp4', 3080, u'which had been based on counting and transforming counts. '), ('./static/5.mp4', 3083, u'And, you know, it had some advantages or it seemed it had some advantages, right? '), ('./static/5.mp4', 3089, u"That, um, you're making sort of efficient use of statistics as you're using "), ('./static/5.mp4', 3093, u'the global statistics of the whole matrix directly to estimate things. '), ('./static/5.mp4', 3098, u'Um, and at that poi- up until then, '), ('./static/5.mp4', 3101, u'it had really only being used to capture word similarity, um, '), ('./static/5.mp4', 3105, u'and a lot of it had suffered from disproportionate im- importance given to large counts. '), ('./static/5.mp4', 3111, u'But Doug Rohde, he had sort of started to show how to solve both of these problems. '), ('./static/5.mp4', 3116, u'And so on the other hand, '), ('./static/5.mp4', 3117, u'there had been these neural network methods '), ('./static/5.mp4', 3119, u'which are kind of direct prediction methods that '), ('./static/5.mp4', 3122, u"we're defining that probability distribution and trying to predict the words that occur. "), ('./static/5.mp4', 3127, u'And they had some advantages, right? '), ('./static/5.mp4', 3129, u"The fact that your sampling means that you're not going to run out of memory hopefully. "), ('./static/5.mp4', 3135, u"I know we've had some memory problems with homework one, but in principle, "), ('./static/5.mp4', 3138, u"you're not as bad memory position and if you have to "), ('./static/5.mp4', 3141, u"construct a huge matrix because you're going linearly, "), ('./static/5.mp4', 3144, u"um, but, you know, since you're doing it sample by "), ('./static/5.mp4', 3147, u"sample it's inefficient use of statistics, um. "), ('./static/5.mp4', 3151, u"Okay. And so, but on the other hand Mikolov's work it performed perfectly. "), ('./static/5.mp4', 3157, u'Not perfectly, but really well. '), ('./static/5.mp4', 3159, u'Um, so this is sort of led into this work, '), ('./static/5.mp4', 3162, u'um, that Jeffrey Pennington, um, '), ('./static/5.mp4', 3165, u'Richard Socher [inaudible] can we sort of combine these ideas '), ('./static/5.mp4', 3170, u'and sort of have some of the goodness of the neural net methods, '), ('./static/5.mp4', 3175, u'um, while trying to do things with some kind of count matrix. '), ('./static/5.mp4', 3180, u'And so in particular, um, '), ('./static/5.mp4', 3182, u'we wanted to get the result in '), ('./static/5.mp4', 3184, u'a slightly less hacky way that you want to have components of meaning '), ('./static/5.mp4', 3190, u'being linear ope- linear operations in '), ('./static/5.mp4', 3193, u"the vector space that they're just some effective or adding or something like this. "), ('./static/5.mp4', 3198, u'And so the crucial observation of this model was that we could use '), ('./static/5.mp4', 3202, u'ratios of co-occurrence probabilities to encode meaning components. '), ('./static/5.mp4', 3207, u'And so the idea here is, '), ('./static/5.mp4', 3209, u'if you have a word like ice and '), ('./static/5.mp4', 3212, u"you say how often the thing's going to co-occur with that, "), ('./static/5.mp4', 3215, u"well solid should co-occur a lot and gas shouldn't. "), ('./static/5.mp4', 3218, u"But well water is also going to co-occur a lot and some random word won't occur much. "), ('./static/5.mp4', 3225, u'If you have, oops. '), ('./static/5.mp4', 3229, u'If you have steam, '), ('./static/5.mp4', 3231, u'you get the opposite pattern with solid and gas, right? '), ('./static/5.mp4', 3236, u'But so the thing to notice is, '), ('./static/5.mp4', 3238, u"it's not enough to just have large by itself because large "), ('./static/5.mp4', 3242, u'appears both here and here or small appears there and there, '), ('./static/5.mp4', 3246, u"the thing that's interesting and sort of the difference between "), ('./static/5.mp4', 3249, u'these components in there indicating a meaning component. '), ('./static/5.mp4', 3252, u'And so we can get it that if we look at the ratio of co-occurrence probabilities. '), ('./static/5.mp4', 3260, u'And so for the ratio of co-occurrence probabilities this is a dimension of '), ('./static/5.mp4', 3265, u'meaning and where for other words and this sort of ratio cancels out to about one. '), ('./static/5.mp4', 3273, u"And so in this slide I've moved so it's not how my "), ('./static/5.mp4', 3276, u'small and large that these are actually actual counts from a corpus. '), ('./static/5.mp4', 3280, u'So we roughly get dimension of meaning between '), ('./static/5.mp4', 3283, u'solid and gas are the ones coming out '), ('./static/5.mp4', 3286, u'as about one because they are not the dimension of meaning. '), ('./static/5.mp4', 3289, u'And so, it seems like what we want is we want to have ratio of '), ('./static/5.mp4', 3293, u'co-occurrence probabilities become linear and our space. '), ('./static/5.mp4', 3298, u"And then we're in a good business. "), ('./static/5.mp4', 3300, u"And so that's what we want to set about doing. "), ('./static/5.mp4', 3303, u'Well, how can you do that? '), ('./static/5.mp4', 3305, u'Well, the way you can do that, '), ('./static/5.mp4', 3307, u'is by if you can make the dot products equal to the log of the co-occurrence probability, '), ('./static/5.mp4', 3315, u'then immediately you get the fact that when you have '), ('./static/5.mp4', 3319, u'a vector difference it turns into a ratio of the co-occurrence probabilities. '), ('./static/5.mp4', 3326, u'And so, essentially the whole of the model is that we '), ('./static/5.mp4', 3330, u'want to have dot products or logs of co-occurrence probabilities. '), ('./static/5.mp4', 3334, u"And so, that's what we do. "), ('./static/5.mp4', 3336, u'So, here is our objective function here '), ('./static/5.mp4', 3339, u"and it's made to look a little bit more complicated. "), ('./static/5.mp4', 3343, u"But essentially we've got this squared loss here "), ('./static/5.mp4', 3347, u'and then we wanting to say the dot-product should be as similar '), ('./static/5.mp4', 3351, u'as possible to the log of '), ('./static/5.mp4', 3353, u"co-occurrence probability and so you'll they'll "), ('./static/5.mp4', 3357, u"be lost to the extent that they're not the same, "), ('./static/5.mp4', 3360, u'but we kind of complexify it a little by putting in bias terms for both of the two words. '), ('./static/5.mp4', 3367, u'Because maybe the word is just overall common and likes to '), ('./static/5.mp4', 3370, u'co-occur things or uncommon or does end. '), ('./static/5.mp4', 3373, u'And then we do one more little trick because every [inaudible] does tricks to make the performance '), ('./static/5.mp4', 3378, u'better is that we also use this f-function in front, '), ('./static/5.mp4', 3383, u"so that we're sort of capping the effect that "), ('./static/5.mp4', 3385, u'very common word pairs can have on the performance of the system. '), ('./static/5.mp4', 3390, u'Okay. And so that gave us the GloVe model of word vectors. '), ('./static/5.mp4', 3395, u'And theoretically, the interest of this was, '), ('./static/5.mp4', 3400, u'you know, a lot of the preceding literature had been there had '), ('./static/5.mp4', 3403, u'been these count methods and there had been these prediction methods. '), ('./static/5.mp4', 3406, u'And the hope was that this could sort of unify the '), ('./static/5.mp4', 3409, u'two by showing you how you could have a method that '), ('./static/5.mp4', 3413, u"is estimated simply of a count matrix but it's done in the same kind of "), ('./static/5.mp4', 3418, u"iterative loss based estimation method that's "), ('./static/5.mp4', 3421, u'used for the neural methods to get good word vectors. '), ('./static/5.mp4', 3424, u'And this also worked to give good word vectors. '), ('./static/5.mp4', 3427, u"So here's GloVe results for the word frog. "), ('./static/5.mp4', 3430, u'And frogs and toad are obvious. '), ('./static/5.mp4', 3433, u'But there are these different kinds of words, uh, '), ('./static/5.mp4', 3436, u'various kinds of pretty tree frogs and things like that. '), ('./static/5.mp4', 3441, u"Okay. Um, so I'll then go from here and say a little "), ('./static/5.mp4', 3446, u'bit more about some of the work on evaluating word vectors. '), ('./static/5.mp4', 3451, u'And this is maybe also a chance just talk a little bit about evaluation altogether. '), ('./static/5.mp4', 3456, u'So, normally in NLP when we do a valuation, '), ('./static/5.mp4', 3460, u'the first thing that comes up is intrinsic versus extrinsic evaluation. '), ('./static/5.mp4', 3465, u"So, normally if there's something we trying to do like model, um, "), ('./static/5.mp4', 3470, u"word similarity with word vectors or we're trying to, um, "), ('./static/5.mp4', 3474, u'put parts of speech on words or something, '), ('./static/5.mp4', 3477, u'we can just have an intrinsic evaluation of saying how good a job did you get. '), ('./static/5.mp4', 3482, u'Are you guessing the right part of speech? '), ('./static/5.mp4', 3485, u'Are you putting synonyms close together? '), ('./static/5.mp4', 3487, u"And that's sort of normally very easy to do and fast to compute. "), ('./static/5.mp4', 3492, u"And it's useful to do because it helps us understand the system. "), ('./static/5.mp4', 3496, u'On the other hand, a lot of the time those intrinsic evaluations, '), ('./static/5.mp4', 3500, u"it's not very clear where- where they're having done well on that task is really going to "), ('./static/5.mp4', 3506, u'help us build the amazing natural language understanding robots '), ('./static/5.mp4', 3510, u'that we so ardently desire. '), ('./static/5.mp4', 3512, u'Um, so, people are also very interested in extrinsic evaluations. '), ('./static/5.mp4', 3517, u'And so extrinsically is then saying well suppose you use '), ('./static/5.mp4', 3521, u"this new stuff in a real system doesn't make performance go up. "), ('./static/5.mp4', 3527, u"And it's then sort of definitional what counts "), ('./static/5.mp4', 3530, u"to you as a real system that normally that's "), ('./static/5.mp4', 3532, u"meaning it's some application that human beings actually care about and liked to use. "), ('./static/5.mp4', 3538, u"So that's something like web search, or question answering, "), ('./static/5.mp4', 3542, u'or phone dialog system or something like that, um, '), ('./static/5.mp4', 3547, u'hat you can put it into that system and the numbers get- go up. '), ('./static/5.mp4', 3551, u'So, that seems what you want to do. '), ('./static/5.mp4', 3553, u'You want to have stuff that works in real tasks. '), ('./static/5.mp4', 3555, u'Of course, there are sort of on the other hand a lot of things are a lot harder than. '), ('./static/5.mp4', 3560, u'So much more work to do such an evaluation and run different variance of a system. '), ('./static/5.mp4', 3567, u'And even when the results, uh, '), ('./static/5.mp4', 3570, u"poor or great sometimes it's hard to diagnose. "), ('./static/5.mp4', 3574, u"You know, if- if your great new word vectors don't work better in the system, you know, "), ('./static/5.mp4', 3579, u'it might be for sort of some extraneous reason about '), ('./static/5.mp4', 3581, u'how the system was built at sort of hiding all your magic. '), ('./static/5.mp4', 3584, u'And if you just change the rest of the system and suddenly show its good effects. '), ('./static/5.mp4', 3589, u"So, it's kind of hard to do, "), ('./static/5.mp4', 3591, u'um, sort of, um, '), ('./static/5.mp4', 3593, u'apportionment of goodness and badness Okay. '), ('./static/5.mp4', 3598, u"So, um, so, today I'm mainly going to say a little bit more about "), ('./static/5.mp4', 3601, u"these intrinsic word vector evaluations that we've talked about. "), ('./static/5.mp4', 3605, u"So we've talked quite a bit about these analogies. "), ('./static/5.mp4', 3609, u"So if we're actually working out the analogies, "), ('./static/5.mp4', 3612, u'it turns out that normally what people are doing is working out '), ('./static/5.mp4', 3616, u'a cosine distance and angle between, um, '), ('./static/5.mp4', 3620, u'different word candidates, um, '), ('./static/5.mp4', 3623, u'to work out which is the word that solves the analogy which '), ('./static/5.mp4', 3626, u'is an Norbert little tiny wrinkle of difference there. '), ('./static/5.mp4', 3630, u"And there's also one other trick that people commonly use. "), ('./static/5.mp4', 3634, u'They forbid the system from returning one of the three word she put '), ('./static/5.mp4', 3637, u'into the analogy Okay. '), ('./static/5.mp4', 3641, u'But nevertheless, so, this is something that you can evaluate. '), ('./static/5.mp4', 3645, u'Here now some GloVe visualizations. '), ('./static/5.mp4', 3648, u'And so these GloVe visualizations show exactly the same kind of '), ('./static/5.mp4', 3652, u"linearity property that Doug Rohde discovered which means that analogy's work. "), ('./static/5.mp4', 3657, u'Sort of by construction, '), ('./static/5.mp4', 3659, u'because our vector space wanted to make meaning components linear. '), ('./static/5.mp4', 3663, u'So, this is then, um, '), ('./static/5.mp4', 3665, u'showing a gender display. '), ('./static/5.mp4', 3668, u'This is showing one between companies and their CEOs, kind of cool. '), ('./static/5.mp4', 3673, u'And you can also do more syntactic facts. '), ('./static/5.mp4', 3676, u'So this is showing, um, '), ('./static/5.mp4', 3677, u'positive comparative and superlative of adjectives. '), ('./static/5.mp4', 3681, u'Yeah. So, Tomas Mikolov came up with this idea of doing these analogy tasks. '), ('./static/5.mp4', 3688, u'And so he built a data-set with a lot of analogies in it. '), ('./static/5.mp4', 3692, u"It's sort of- it's a bit of a weirdo data-set because it's sort of tests a few "), ('./static/5.mp4', 3697, u'random different things which may have been things that his system worked well on, um, '), ('./static/5.mp4', 3702, u'but you know, it tests countries and capitals, '), ('./static/5.mp4', 3706, u'country, cities and states, countries and currency. '), ('./static/5.mp4', 3712, u'So there are a bunch of semantic things that tests. '), ('./static/5.mp4', 3715, u'And then there are some, um, '), ('./static/5.mp4', 3718, u'syntactic things that tests so bad, worst, '), ('./static/5.mp4', 3721, u'fast fastest for superlatives. '), ('./static/5.mp4', 3724, u'But, you know, even some of the ones I was showing before, you know, '), ('./static/5.mp4', 3727, u"there's no- there's no Obama is to "), ('./static/5.mp4', 3730, u'Clinton kind of ones that are actually in this evaluation set. '), ('./static/5.mp4', 3735, u"Um, here's a big table of results, "), ('./static/5.mp4', 3738, u'um, that comes from our GloVe paper. '), ('./static/5.mp4', 3740, u'So not surprisingly the GloVe paper perform best in this evaluation. '), ('./static/5.mp4', 3745, u'Because that was our paper. Um, [LAUGHTER] '), ('./static/5.mp4', 3748, u'[LAUGHTER] But I mean perhaps- you know, '), ('./static/5.mp4', 3751, u'perhaps the things to start to notice is, '), ('./static/5.mp4', 3754, u'yeah, if you just do a plain SVD on counts. '), ('./static/5.mp4', 3757, u'You know that that works abominably badly for these, um, analogy tasks. '), ('./static/5.mp4', 3764, u'But, you know, kind of as Doug Rohde showed, '), ('./static/5.mp4', 3766, u'if you start then doing manipulations of the count matrix before you do an SVD, '), ('./static/5.mp4', 3773, u'you can actually start to produce '), ('./static/5.mp4', 3775, u'an SVD based system that actually performs quite well on these tasks. '), ('./static/5.mp4', 3781, u'Um, you know, not badly against other things. '), ('./static/5.mp4', 3785, u'Um, other things that you will discover, '), ('./static/5.mp4', 3787, u'right at the top there are a 100 dimensional ones, '), ('./static/5.mp4', 3790, u'and at the bottom there are some 1000 dimensional ones, '), ('./static/5.mp4', 3793, u'and other 300 dimensional ones. '), ('./static/5.mp4', 3795, u"At least when you're training on a big amount of text, "), ('./static/5.mp4', 3797, u'bigger dimensionality definitely works better. '), ('./static/5.mp4', 3800, u"And I'll come back to that in a minute. "), ('./static/5.mp4', 3802, u'Um, the amount of text makes a difference as well, right? '), ('./static/5.mp4', 3805, u"So we're going up from- so one to 1.5 billion words at the beginning, "), ('./static/5.mp4', 3810, u'to these ones down here are being trained over 42 billion words of text, '), ('./static/5.mp4', 3814, u'and perhaps unsurprisingly, the 42 billion words of texts ones work better. '), ('./static/5.mp4', 3820, u"Um, so it's big data. "), ('./static/5.mp4', 3822, u'Um, here are a couple more steps from this paper. '), ('./static/5.mp4', 3825, u'So this is a graph of dimensionality and what the performance is. '), ('./static/5.mp4', 3830, u"So for the three lines the green one's semantic, "), ('./static/5.mp4', 3832, u"the blue one's the syntactic analogies and so red's the overall score. "), ('./static/5.mp4', 3837, u'So sort of what you see is up to dimensionality '), ('./static/5.mp4', 3840, u'300 things that clearly increasing quite a bit, '), ('./static/5.mp4', 3844, u'and then it gets fairly flat, '), ('./static/5.mp4', 3846, u'which is precisely why you find a lot of word vectors, '), ('./static/5.mp4', 3849, u'um, that are of dimensionality 300. '), ('./static/5.mp4', 3851, u"Um, this one's showing what window size. "), ('./static/5.mp4', 3855, u'So this is sort of what we talked about symmetric on both sides window size, '), ('./static/5.mp4', 3860, u'and as it goes from 246810. '), ('./static/5.mp4', 3863, u'And sort of what you see is, '), ('./static/5.mp4', 3865, u'if you use a very small window like two, that actually works. '), ('./static/5.mp4', 3869, u'That the, the syntactic prediction is stronger because well, '), ('./static/5.mp4', 3873, u'syntactic effects are very local. '), ('./static/5.mp4', 3875, u'Whereas as you go out, '), ('./static/5.mp4', 3877, u'the semantic prediction gets better and better. '), ('./static/5.mp4', 3879, u'Actually this syntactic gets a bit better as well, '), ('./static/5.mp4', 3882, u"but it's especially the semantic that gains. "), ('./static/5.mp4', 3885, u'Um, the right graph shows that if you only use context on one side, '), ('./static/5.mp4', 3890, u"um, your numbers aren't as good. "), ('./static/5.mp4', 3892, u'Okay, um, so, I sort of just wanted to sort of sneak in a little cameos of a couple of, '), ('./static/5.mp4', 3900, u'um, recent bits of work, '), ('./static/5.mp4', 3903, u'as sort of a first of what things people are doing, '), ('./static/5.mp4', 3905, u'um, with word vectors. '), ('./static/5.mp4', 3907, u'Um, so this one, um, '), ('./static/5.mp4', 3909, u'was actually by two Stanford people. '), ('./static/5.mp4', 3912, u'Um, now the best- this would be the best story. '), ('./static/5.mp4', 3915, u'If I could say that this was a final project, '), ('./static/5.mp4', 3917, u'um, in this class last year, '), ('./static/5.mp4', 3919, u"but unfortunately that's not true. "), ('./static/5.mp4', 3921, u'This paper has nothing to do with this class [LAUGHTER]. '), ('./static/5.mp4', 3925, u'But it-- right. '), ('./static/5.mp4', 3926, u'Um, Zin Yin and Yuanyuan, '), ('./static/5.mp4', 3930, u'um, actually had, um, '), ('./static/5.mp4', 3933, u'some sort of clever and very mathy ideas, '), ('./static/5.mp4', 3937, u"where they're using matrix perturbation theory. "), ('./static/5.mp4', 3941, u'Um, and sort of just showing how, um, '), ('./static/5.mp4', 3944, u'dimensionality in word vectors actually sort of feeds into the bias-variance trade-off. '), ('./static/5.mp4', 3949, u"If you've seen that, "), ('./static/5.mp4', 3950, u'um, in other parts of machine learning. '), ('./static/5.mp4', 3952, u"And I'm not even going to attempt to explain their paper. "), ('./static/5.mp4', 3956, u'Um, but here it is, '), ('./static/5.mp4', 3957, u'that they did really well with this paper, '), ('./static/5.mp4', 3959, u"they gone all talk in Europe's from it. "), ('./static/5.mp4', 3961, u"Um, and so- but there's sort of "), ('./static/5.mp4', 3963, u'an interesting result of what you see with these word vectors, '), ('./static/5.mp4', 3967, u'which is in a way kind of surprising. '), ('./static/5.mp4', 3970, u'So this is showing doing word vector dimensions from zero up to 10,000. '), ('./static/5.mp4', 3976, u"So we're going way higher than we talked about before. "), ('./static/5.mp4', 3979, u'And so what you discover which people have known for ages is, '), ('./static/5.mp4', 3983, u"that there's sort of a little blip that somewhere around two or 300, "), ('./static/5.mp4', 3987, u'which seems to optimize performance. '), ('./static/5.mp4', 3990, u"So, I've used those sizes. "), ('./static/5.mp4', 3992, u'But the thing that they were sort of doing a lot of their theory about, '), ('./static/5.mp4', 3995, u"and it's kind of surprising is, well, "), ('./static/5.mp4', 3998, u'surely if you have a humongous humongous number, like, '), ('./static/5.mp4', 4002, u'if you are using 10,000, '), ('./static/5.mp4', 4004, u'um, dimensional vectors, you know, '), ('./static/5.mp4', 4006, u"you're trying to estimate another two orders of magnitude more numbers for every word, "), ('./static/5.mp4', 4013, u'surely things should just fall apart, um, '), ('./static/5.mp4', 4016, u"because you've got hopelessly many parameters relative to "), ('./static/5.mp4', 4019, u"the amount of training data that you're trying to estimate these numbers from. "), ('./static/5.mp4', 4024, u'And so the interesting result that they show is, '), ('./static/5.mp4', 4027, u"that things don't fall apart. "), ('./static/5.mp4', 4030, u'Um, and that you can essentially go out to these huge huge dimensionalities, '), ('./static/5.mp4', 4035, u'and the performance stays flat. '), ('./static/5.mp4', 4037, u"And that they've got a lot of theory, "), ('./static/5.mp4', 4039, u"sort of for predicting why that that's actually going to end up being the case. "), ('./static/5.mp4', 4044, u'Um, yeah. '), ('./static/5.mp4', 4046, u'So for training these models iteratively, '), ('./static/5.mp4', 4048, u'this is- orange is showing, um, GloVe training. '), ('./static/5.mp4', 4053, u'You know, they keep on getting better for a while. '), ('./static/5.mp4', 4056, u'So you know, just go out, '), ('./static/5.mp4', 4057, u"go sleep and see in the morning how it's doing, right? "), ('./static/5.mp4', 4061, u'So that if you were running it, um, '), ('./static/5.mp4', 4062, u'for 24 hours your numbers are better than if you only ran it for six hours. '), ('./static/5.mp4', 4067, u"Um, and that's true for a lot of deep learning models, sorry. "), ('./static/5.mp4', 4071, u"So this is the key reason why you don't want "), ('./static/5.mp4', 4074, u"to start your assignment the night before it's due. "), ('./static/5.mp4', 4077, u'Because even if you program it perfectly, '), ('./static/5.mp4', 4080, u'you might just not have enough time for it to run, '), ('./static/5.mp4', 4083, u'um, so that you produce good numbers at the end of it. '), ('./static/5.mp4', 4088, u'Um, okay. Uh, yeah so, '), ('./static/5.mp4', 4092, u'so couple of more, um, '), ('./static/5.mp4', 4094, u'things, on that, um. '), ('./static/5.mp4', 4097, u'Yes. So, um, what are we showing here? '), ('./static/5.mp4', 4101, u'So these are again semantics in tactic and overall numbers. '), ('./static/5.mp4', 4106, u'So there are sort of two things that are sort of being mixed together here. '), ('./static/5.mp4', 4109, u'One is, if we just look at the overall numbers, '), ('./static/5.mp4', 4113, u"they're highest over here, um, "), ('./static/5.mp4', 4115, u'which is this 42 billion Common Crawl web-pages corpus, '), ('./static/5.mp4', 4119, u'that gives us the highest overall number. '), ('./static/5.mp4', 4122, u"But there's sort of something else that's interesting in this graph, which is, "), ('./static/5.mp4', 4126, u'um, that using Wikipedia works frequently well. '), ('./static/5.mp4', 4131, u'So that you actually find that 1.6 billion tokens of Wikipedia works '), ('./static/5.mp4', 4137, u'better than 4.3 billion tokens of News-wire newspaper article data. '), ('./static/5.mp4', 4143, u"And so I, I think that's sort of actually make sense, "), ('./static/5.mp4', 4148, u'which is well, you know, '), ('./static/5.mp4', 4149, u'the job of encyclopedias is to just sort of '), ('./static/5.mp4', 4151, u'explain concepts and how they relate to each other, right? '), ('./static/5.mp4', 4154, u'So that encyclopedias are '), ('./static/5.mp4', 4156, u'just much more expository text that show all the connections between things, '), ('./static/5.mp4', 4162, u"whereas newspapers in general aren't trying to expose at how things fit together. "), ('./static/5.mp4', 4167, u"They're just telling you about, you know, "), ('./static/5.mp4', 4169, u'who got shot dead last night or something like that, right? '), ('./static/5.mp4', 4172, u'So, um, so this is sort of interesting fact, um, '), ('./static/5.mp4', 4177, u'that this Wikipedia data kind of really, '), ('./static/5.mp4', 4179, u'it sort of is differentially useful, um, '), ('./static/5.mp4', 4182, u'for, um, making word vectors. '), ('./static/5.mp4', 4186, u'And you know, in fact, you know, '), ('./static/5.mp4', 4188, u'when we did very well without GloVe word vectors and lots of people use those. '), ('./static/5.mp4', 4193, u'You know, I think actually one of the reasons why they work so well is that '), ('./static/5.mp4', 4198, u'the original word2vec vectors that Google distributes are built only on Google News data, '), ('./static/5.mp4', 4203, u'where else sort of have this, '), ('./static/5.mp4', 4205, u'um, Wikipedia data inside them. '), ('./static/5.mp4', 4207, u'Okay, um, rushing ahead. '), ('./static/5.mp4', 4211, u"Um, yes, so the- there's all the work on analogy, "), ('./static/5.mp4', 4214, u'but the other more basic evaluation is this one of capturing similarity judgments. '), ('./static/5.mp4', 4220, u"And I haven't said much about this, but you know, "), ('./static/5.mp4', 4223, u'there is this sort of large sub-literature in the psychology community, '), ('./static/5.mp4', 4228, u'where people have wanted to model humans judgments of similarity. '), ('./static/5.mp4', 4232, u'So like a good psych person, what you do, '), ('./static/5.mp4', 4236, u'is you find your classroom of Psych one undergrads, '), ('./static/5.mp4', 4239, u'and you show them pairs of words and say rate '), ('./static/5.mp4', 4242, u'these things for similarity on a scale of one to 10. '), ('./static/5.mp4', 4245, u'And lots of that data has been collected, '), ('./static/5.mp4', 4248, u'and you work out the mean over human beings, '), ('./static/5.mp4', 4250, u'and they give numbers like this of tiger and cat, 7.35. '), ('./static/5.mp4', 4255, u"Tiger's similar to Tiger 10, book and paper, "), ('./static/5.mp4', 4258, u'plane and car, stock and phone, '), ('./static/5.mp4', 4261, u'stock and CD, and you get numbers. '), ('./static/5.mp4', 4263, u"So then, what we're doing is wanting to say, "), ('./static/5.mp4', 4267, u"well let's use distance in the space to map directly onto these similarity judgments, "), ('./static/5.mp4', 4273, u'and how well does it map? '), ('./static/5.mp4', 4275, u"And so that's sort of similarity judging has "), ('./static/5.mp4', 4278, u'also then being used for evaluating these systems. '), ('./static/5.mp4', 4282, u'So again, here are a lot of models. '), ('./static/5.mp4', 4284, u'This is again from our GloVe paper. '), ('./static/5.mp4', 4286, u'But so there are these various similarity data-sets. '), ('./static/5.mp4', 4289, u'So one of the best-known ones that I had on the slide before is this, um, Wordsim 353. '), ('./static/5.mp4', 4296, u'It has 353, um, '), ('./static/5.mp4', 4298, u'different ones in it, '), ('./static/5.mp4', 4300, u'and so you are sort of then modeling a correlation '), ('./static/5.mp4', 4304, u'between your judgments of similarity and the ones that came from the human beings. '), ('./static/5.mp4', 4309, u'Okay. Two more things I want to say. Um, yes. '), ('./static/5.mp4', 4312, u'So, we had that problem right at the beginning '), ('./static/5.mp4', 4316, u'of Clinton and how that could be various people. '), ('./static/5.mp4', 4321, u"And that's perhaps in some sense the simplest case of words being ambiguous, "), ('./static/5.mp4', 4327, u'when you have names which have reference to different people. '), ('./static/5.mp4', 4330, u"Um, but it's not only true of names. "), ('./static/5.mp4', 4333, u'So by and large, '), ('./static/5.mp4', 4335, u'words in human languages are ambiguous and have lots of meanings. '), ('./static/5.mp4', 4341, u"Um, that's especially true of common words. "), ('./static/5.mp4', 4344, u'They always have lots of meaning. '), ('./static/5.mp4', 4346, u"It's especially true of words that have existed for a long time. "), ('./static/5.mp4', 4350, u"It's not true of new very technical words, you know, carcinoma. "), ('./static/5.mp4', 4354, u'I think that only has one meaning. '), ('./static/5.mp4', 4355, u'Um, but, you know, '), ('./static/5.mp4', 4357, u'if you think of any relatively, um, '), ('./static/5.mp4', 4360, u'common word and starts, um, '), ('./static/5.mp4', 4363, u'scratching your head for a moment, '), ('./static/5.mp4', 4365, u"you'll find it has lots of meanings. "), ('./static/5.mp4', 4368, u"I- maybe this isn't even such a common word, "), ('./static/5.mp4', 4370, u"but my random word I've got here is Pike. "), ('./static/5.mp4', 4373, u'Um, pike has lots of meanings, '), ('./static/5.mp4', 4375, u'it has meanings like? '), ('./static/5.mp4', 4377, u'Fish. '), ('./static/5.mp4', 4379, u"Fish, it's a kind of fish, yeah. "), ('./static/5.mp4', 4380, u"So there's a fish that's a pike. "), ('./static/5.mp4', 4382, u'What else is a pike? '), ('./static/5.mp4', 4383, u'A large spear. '), ('./static/5.mp4', 4384, u'A large spear. '), ('./static/5.mp4', 4385, u'Yes, so a large spear is a pike. '), ('./static/5.mp4', 4388, u"Other kinds of pike's? "), ('./static/5.mp4', 4389, u'Gymnastics move. '), ('./static/5.mp4', 4390, u"It's a road. "), ('./static/5.mp4', 4390, u'Gymnastics move or in diving move. '), ('./static/5.mp4', 4393, u"It's a road. "), ('./static/5.mp4', 4394, u'Um, yeah. Um, so there are lots of meanings. '), ('./static/5.mp4', 4398, u'Um, there are other meanings. '), ('./static/5.mp4', 4399, u'Um, in Australian English, '), ('./static/5.mp4', 4401, u'pike is also used as a verb to mean, '), ('./static/5.mp4', 4403, u'um, to pull out from doing something. '), ('./static/5.mp4', 4405, u'Like, "We were all going to go out to a nightclub later, but Joe piked." '), ('./static/5.mp4', 4411, u"[LAUGHTER] Um, I don't think that usage is common in this country, "), ('./static/5.mp4', 4415, u'but, um, you can try that, um. [LAUGHTER] '), ('./static/5.mp4', 4418, u'Right. But lots of meanings and, you know, '), ('./static/5.mp4', 4421, u"this isn't only true of the word pike, right? "), ('./static/5.mp4', 4424, u'Pick any other simple word, right? '), ('./static/5.mp4', 4427, u'You can pick a word like shell or field or house or make, '), ('./static/5.mp4', 4432, u'you know, they have lots of meanings when it comes down to it. '), ('./static/5.mp4', 4435, u'So, you know, but, uh, '), ('./static/5.mp4', 4436, u'how can this work if we just have one meaningful words? '), ('./static/5.mp4', 4441, u"And that's the interesting question and it was something that "), ('./static/5.mp4', 4444, u'[NOISE] we were actually interested in early on. '), ('./static/5.mp4', 4448, u"So, I'm even before the Word2Vec paper came out back in 2012, "), ('./static/5.mp4', 4453, u'um, we were playing around, um, '), ('./static/5.mp4', 4455, u'with neural word vectors and, um, we thought, '), ('./static/5.mp4', 4460, u'boy this is so broken having only one, '), ('./static/5.mp4', 4463, u'um, cents, for a word. '), ('./static/5.mp4', 4466, u"Why don't we come up with a model that has multiple sensors for a word? "), ('./static/5.mp4', 4470, u'And so we did that and we did it in a pretty crude way, '), ('./static/5.mp4', 4473, u'I guess, [NOISE] um, '), ('./static/5.mp4', 4474, u'the way we did it is say, '), ('./static/5.mp4', 4476, u"well, let's for each common word, "), ('./static/5.mp4', 4480, u"let's cluster all the contexts in which it occurs. "), ('./static/5.mp4', 4484, u"And then we'll see if there seem to be "), ('./static/5.mp4', 4487, u'multiple clear clusters by some criterion for that word. '), ('./static/5.mp4', 4491, u"And if so, we'll just sort of split the word into pseudo words. "), ('./static/5.mp4', 4496, u'So, if it seems like that there are five clusters, '), ('./static/5.mp4', 4499, u'um, for the word, '), ('./static/5.mp4', 4500, u'the example I meant to use here is jaguar. '), ('./static/5.mp4', 4503, u'Five clusters for the word jaguar, '), ('./static/5.mp4', 4505, u'I will just call them jaguar_1, jaguar_2, jaguar_3, four, '), ('./static/5.mp4', 4508, u"five, so it's just literally changed "), ('./static/5.mp4', 4510, u'the word in our corpus according to its cluster number. '), ('./static/5.mp4', 4513, u'And then we run our word vectoring algorithm and so we get '), ('./static/5.mp4', 4516, u'a representation that each of those sensors of the word. '), ('./static/5.mp4', 4520, u'And basically, that works, '), ('./static/5.mp4', 4522, u'right up the top is jaguar_1 next, '), ('./static/5.mp4', 4524, u'uh, luxury and convertible. '), ('./static/5.mp4', 4526, u"Um, here is, I guess there's a very old version of MacOS called Jaguar, "), ('./static/5.mp4', 4533, u'any remem- remember that one? '), ('./static/5.mp4', 4534, u"Um. Right. So, jaguars right next to software and Microsoft up there, so that's hopeful. "), ('./static/5.mp4', 4540, u"Um, here's the jaguar that's right next to the Hunter, um, "), ('./static/5.mp4', 4544, u"and I'm being confused on this one, "), ('./static/5.mp4', 4547, u'is jaguar as near solo musical keyboard and string. '), ('./static/5.mp4', 4550, u'Is there a band, [NOISE] a brand of keyboard called jaguar? '), ('./static/5.mp4', 4553, u"I'm not quite sure about that one, "), ('./static/5.mp4', 4555, u"but anyway, it's sort of basically works. "), ('./static/5.mp4', 4557, u"Um, but that was sort of crude and it's also perhaps problematic, "), ('./static/5.mp4', 4562, u"so a lot of time, the divisions between sensors aren't very clear, right? "), ('./static/5.mp4', 4566, u'A lot of sensors are actually related to each other and overlapping because '), ('./static/5.mp4', 4570, u'when how sensors normally arrive is that people stretch the meanings of words. '), ('./static/5.mp4', 4574, u"It's not that they just sort of randomly "), ('./static/5.mp4', 4576, u'wake up the next morning and say, "I know carpet. '), ('./static/5.mp4', 4579, u'I could also refer to that as stone," um, '), ('./static/5.mp4', 4583, u'and given a new sense to the word stone, right? '), ('./static/5.mp4', 4585, u'You so take something that you know about like '), ('./static/5.mp4', 4588, u'a web and you extend it metaphorically to other uses of webbing. '), ('./static/5.mp4', 4593, u"Um, so here's a perhaps more interesting things, "), ('./static/5.mp4', 4597, u'so this is the other Sanjeev Arora, '), ('./static/5.mp4', 4599, u'um, paper that I was going to mention. '), ('./static/5.mp4', 4601, u"So, that what happens if you don't, "), ('./static/5.mp4', 4604, u"um, if you don't have more than one cents for each word? "), ('./static/5.mp4', 4610, u'Well, effectively what you get is that '), ('./static/5.mp4', 4613, u"the word vector that you learn is what's referred to by "), ('./static/5.mp4', 4617, u'physicists and fancy people as a superposition '), ('./static/5.mp4', 4621, u'of the word vectors of the different sentence, different sensors. '), ('./static/5.mp4', 4625, u'By supersitio- superposition just means a weighted average. '), ('./static/5.mp4', 4629, u'Um, um, [LAUGHTER] so that effectively '), ('./static/5.mp4', 4634, u'my meaning of pike is sort of '), ('./static/5.mp4', 4637, u'a weighted average of the vectors for the different sensors of pike, '), ('./static/5.mp4', 4640, u'and the components are just weighted by their frequency. '), ('./static/5.mp4', 4644, u'Um, so that part maybe is perhaps not too surprising, '), ('./static/5.mp4', 4648, u"but the part that's really surprising is well, "), ('./static/5.mp4', 4651, u'if we just averaging these word vectors, '), ('./static/5.mp4', 4654, u"you'd think you couldn't get anything out of the average, right|? "), ('./static/5.mp4', 4658, u"Like if I tell you I'm thinking of two numbers and they're here, "), ('./static/5.mp4', 4663, u'weighted sum is 54, '), ('./static/5.mp4', 4666, u'what are my two numbers, right? '), ('./static/5.mp4', 4667, u'You are sort of really short of information to be able to answer my question. '), ('./static/5.mp4', 4671, u'But, well, you know, '), ('./static/5.mp4', 4673, u'for these word vectors, um, '), ('./static/5.mp4', 4676, u'we have these high dimensional spaces and even though there '), ('./static/5.mp4', 4682, u'are a lot of words that the space is so vast for thoughts dimensions, '), ('./static/5.mp4', 4687, u'that actual words or sensors are very sparse in that space. '), ('./static/5.mp4', 4693, u"And so it turns out that there's this whole literature on, um, "), ('./static/5.mp4', 4697, u'sparse coding, compressed sensing, '), ('./static/5.mp4', 4700, u'um, some of which has actually done by people in the stats department here, '), ('./static/5.mp4', 4703, u'um, which shows that in these cases where you have these sort of sparse, '), ('./static/5.mp4', 4709, u'um, codes in these high dimensional spaces, '), ('./static/5.mp4', 4712, u'you can actually commonly reconstruct out the components of a superposition, '), ('./static/5.mp4', 4716, u"even though all you've done is sort of done this weighted average, "), ('./static/5.mp4', 4720, u'and so, um, this paper looks at how you can do this and so they have, '), ('./static/5.mp4', 4725, u'um, these underlying meaning components, '), ('./static/5.mp4', 4728, u'and they sort of separated out. '), ('./static/5.mp4', 4729, u'So, tie has one meaning component, '), ('./static/5.mp4', 4732, u"there's in this space of trousers, blouse, waistcoat, "), ('./static/5.mp4', 4735, u'that makes sense, and other one in this meaning component of seasoned teams, '), ('./static/5.mp4', 4740, u'winning league, makes sense. '), ('./static/5.mp4', 4742, u'Um, scoreline goal with equalizer clinching scorers, '), ('./static/5.mp4', 4746, u'this one seems to overlap with this one a bit. '), ('./static/5.mp4', 4748, u'Um, but here tie, '), ('./static/5.mp4', 4750, u'this is sort of cable ties and wire ties and things like that. '), ('./static/5.mp4', 4753, u'So, they are actually able to pull out the different sense meanings, '), ('./static/5.mp4', 4757, u'um, from outside, out of the meaning of the word. '), ('./static/5.mp4', 4761, u'Um, so that is a kind of a cool thing. '), ('./static/5.mp4', 4764, u'I just wanna, um, '), ('./static/5.mp4', 4766, u'say one more thing. '), ('./static/5.mp4', 4768, u'Okay. [NOISE] All the evaluations so far was intrinsic, '), ('./static/5.mp4', 4772, u'um, you also might wanna do extrinsic evaluation. '), ('./static/5.mp4', 4775, u'Why, why word vectors excited people on NLP so much? '), ('./static/5.mp4', 4780, u'Is it turned out that having this meaning, '), ('./static/5.mp4', 4782, u'having this representation meaning just turned out to be '), ('./static/5.mp4', 4785, u'very useful and sort of improve all of your tasks after that. '), ('./static/5.mp4', 4789, u'Um, and so, um, '), ('./static/5.mp4', 4792, u'this is doing named entity recognition which is labeling '), ('./static/5.mp4', 4795, u'persons and locations and organizations, but, you know, '), ('./static/5.mp4', 4798, u"it's typical of many tasks of what people found, "), ('./static/5.mp4', 4801, u'was if you started with a model without sort of word representations '), ('./static/5.mp4', 4806, u'and you throw in your word vectors regardless of whether they were to vehicle GloVe ones, '), ('./static/5.mp4', 4811, u'just kind of your numbers go up a couple of percent or more? '), ('./static/5.mp4', 4815, u'And so word vectors were just sort of this useful source that you could '), ('./static/5.mp4', 4819, u'throw into any NLP system that you build and your numbers went up. '), ('./static/5.mp4', 4824, u'So, that there was just a very effective technology, um, '), ('./static/5.mp4', 4827, u'which actually did work in '), ('./static/5.mp4', 4828, u'basically any extrinsic tasks you type tried it on. Okay. Thanks a lot. ')]