[('./static/2.mp4', 4, u'Okay. Hello everyone. '), ('./static/2.mp4', 7, u'[LAUGHTER] Okay we should get started. '), ('./static/2.mp4', 11, u"Um, they're actually are still quite a few seats left. "), ('./static/2.mp4', 14, u'If you wanna be really bold, '), ('./static/2.mp4', 15, u'there are a couple of seats right in front of me in the front row. '), ('./static/2.mp4', 18, u"If you're less bolder a few over there. "), ('./static/2.mp4', 20, u"Um, but they're also on some of the rows are quite a few middle seat. "), ('./static/2.mp4', 23, u'So if people wanted to be really civic minded some people could sort of '), ('./static/2.mp4', 28, u'squeeze towards the edges and make  more accessible um, '), ('./static/2.mp4', 32, u'some of the seats that still exist in the classroom. '), ('./static/2.mp4', 35, u'Okay. Um, so, um, '), ('./static/2.mp4', 39, u"it's really exciting and great to see so many people here. "), ('./static/2.mp4', 42, u"So I'm a hearty welcome to CS224N and occasionally also "), ('./static/2.mp4', 47, u'known as Ling 284 which is Natural Language Processing with Deep Learning. '), ('./static/2.mp4', 52, u'Um, as just a sort of a  personal anecdote, '), ('./static/2.mp4', 55, u'is still sort of blows my mind that so many people turn up to this class these days. '), ('./static/2.mp4', 59, u'So, for about the first decade that I taught NLP here, '), ('./static/2.mp4', 63, u'you know the number of people I got each year was approximately 45. '), ('./static/2.mp4', 68, u"[LAUGHTER] So it's an order of [LAUGHTER] magnitude smaller than "), ('./static/2.mp4', 71, u'it is now but guess it says quite a lot '), ('./static/2.mp4', 74, u'on about what a revolutionary   impact '), ('./static/2.mp4', 77, u'that artificial intelligence in general and machine learning, '), ('./static/2.mp4', 80, u'deep learning, NLP are starting to have in modern society. '), ('./static/2.mp4', 85, u'Okay. So this is our plan for today. '), ('./static/2.mp4', 88, u"So, um, um, we're really gonna get straight down to business today. "), ('./static/2.mp4', 92, u"So they'll be a brief, very brief introduction some of the sort of course logistics, "), ('./static/2.mp4', 97, u'very brief discussion and talk about human language and '), ('./static/2.mp4', 102, u'word meaning and then we wanna get right into talking about um, '), ('./static/2.mp4', 106, u"the first thing that we're doing which is coming up with word vectors and looking "), ('./static/2.mp4', 110, u'at the word2vec algorithm and that will then sort of fill up the rest of the class. '), ('./static/2.mp4', 115, u'There are still two seats right in '), ('./static/2.mp4', 116, u'the front row for someone who wants to sit right in front of me, '), ('./static/2.mp4', 119, u'just letting you know [LAUGHTER]. '), ('./static/2.mp4', 122, u'Okay. Okay. So here are the course logistics in brief. '), ('./static/2.mp4', 126, u"So I'm Christopher Manning, "), ('./static/2.mp4', 128, u'the person who bravely became the head TA is Abigail See is right there. '), ('./static/2.mp4', 135, u"And then we have quite a lot of wonderful TA's. "), ('./static/2.mp4', 138, u"To the people who are wonderful TA's just sort of stand up for one moment. "), ('./static/2.mp4', 142, u'So, um, [LAUGHTER] we have some sense for wonderful TAs. '), ('./static/2.mp4', 146, u'[LAUGHTER] Okay great. '), ('./static/2.mp4', 148, u'Um, okay. '), ('./static/2.mp4', 151, u'So you know when the lecture is because you made it '), ('./static/2.mp4', 153, u'here and so welcome also to SCPD people. '), ('./static/2.mp4', 157, u'This is also an SCPD class and you can watch it on video. '), ('./static/2.mp4', 161, u'But we love for Stanford students to turn '), ('./static/2.mp4', 164, u'up and show their beautiful faces in the classroom. '), ('./static/2.mp4', 167, u'Okay. So, um, the web-page has all the info about syllabus et cetera et cetera. '), ('./static/2.mp4', 172, u'Okay. So this class what do we hope to teach? '), ('./static/2.mp4', 176, u'So, one thing that we wanna teach is, uh, you know, '), ('./static/2.mp4', 179, u'an understanding of effective modern methods for deep learning. '), ('./static/2.mp4', 182, u'Starting off by reviewing some of the basics and then '), ('./static/2.mp4', 185, u'particularly talking about the kinds of techniques including um, '), ('./static/2.mp4', 188, u'recurrent networks and attention that are widely '), ('./static/2.mp4', 191, u'used for natural language processing models. '), ('./static/2.mp4', 194, u'A second thing we wanna teach is a big picture understanding of '), ('./static/2.mp4', 198, u'human languages and some of the difficulties in understanding and producing them. '), ('./static/2.mp4', 203, u'Of course if you wanna know a lot about human languages, '), ('./static/2.mp4', 205, u"there's a whole linguistics department and you can do a lot of courses of that. "), ('./static/2.mp4', 209, u'Um, but so I wanna give at least some appreciation so you have some clue of what are '), ('./static/2.mp4', 213, u'the challenges and difficulties and varieties of human languages. '), ('./static/2.mp4', 218, u'And then this is also kind of a practical class. '), ('./static/2.mp4', 221, u'Like we actually wanna teach you how you can '), ('./static/2.mp4', 224, u'build practical systems that work for some of the major parts of NLP. '), ('./static/2.mp4', 229, u'So if you go and get a job at one of those tech firms and they say "Hey, '), ('./static/2.mp4', 233, u'could you build us a named entity recognizer?" '), ('./static/2.mp4', 235, u'You can say "Sure, I can do that." '), ('./static/2.mp4', 238, u'And so for a bunch of problems, '), ('./static/2.mp4', 240, u"obviously we can't do everything, "), ('./static/2.mp4', 242, u"we're gonna do word meaning, "), ('./static/2.mp4', 243, u'dependency parsing, machine translation and you have an option to do question answering, '), ('./static/2.mp4', 247, u"I'm actually building systems for those. "), ('./static/2.mp4', 250, u"If you'd been talking to friends who did the class in the last couple of years, "), ('./static/2.mp4', 255, u'um, here are the differences for this year just to get things straight. '), ('./static/2.mp4', 258, u"Um, so we've updated some of the content of the course. "), ('./static/2.mp4', 261, u"So, uh, between me and guest lectures there's new content. "), ('./static/2.mp4', 266, u'Well that look bad. '), ('./static/2.mp4', 269, u"Wonder if that will keep happening, we'll find out. "), ('./static/2.mp4', 272, u"There's new content and on various topics that are sort of developing areas. "), ('./static/2.mp4', 278, u'One of the problems with this course is really big area of deep learning at '), ('./static/2.mp4', 281, u'the moment is still just developing really really quickly. '), ('./static/2.mp4', 284, u"So, it's sort of seems like one-year-old content is already "), ('./static/2.mp4', 287, u"things kind of data and we're trying to update things. "), ('./static/2.mp4', 291, u"A big change that we're making this year is we're "), ('./static/2.mp4', 294, u'having five-one week assignments instead of '), ('./static/2.mp4', 296, u'three-two week assignments at the beginning of '), ('./static/2.mp4', 299, u"the course and I'll say a bit more about that in a minute. "), ('./static/2.mp4', 302, u"Um, this year we're gonna use PyTorch instead of TensorFlow, "), ('./static/2.mp4', 306, u"and we'll talk about that more later too. "), ('./static/2.mp4', 308, u"Um, we're having the assignments due before class on either Tuesday or Thursday. "), ('./static/2.mp4', 313, u"So you're not distracted and can come to class. "), ('./static/2.mp4', 316, u'So starting off, um, yeah. '), ('./static/2.mp4', 320, u"So we're trying to give an easier, "), ('./static/2.mp4', 322, u'gentler ramp-up but on the other hand a fast ramp-up. '), ('./static/2.mp4', 326, u"So we've got this first assignment which is sort of easy, uh, "), ('./static/2.mp4', 329, u"but it's available right now and is due next Tuesday. "), ('./static/2.mp4', 334, u"And the final thing is we're not having a midterm this year. "), ('./static/2.mp4', 337, u'Um, okay. '), ('./static/2.mp4', 339, u"So this is what we're doing. "), ('./static/2.mp4', 340, u'So there are five of these assignments that I just mentioned. '), ('./static/2.mp4', 344, u'Um, So six percent for the first one, '), ('./static/2.mp4', 346, u'12 percent for each of the other ones, '), ('./static/2.mp4', 349, u'um, and, I already said that. '), ('./static/2.mp4', 352, u"We're gonna use gradescope for grading. "), ('./static/2.mp4', 354, u"It'll be really help out the TAs if you could use "), ('./static/2.mp4', 356, u'your SUnet ID as your gradescope account ID. '), ('./static/2.mp4', 361, u'Um, so then for the second part of the course, '), ('./static/2.mp4', 364, u'people do a final project and there are two choices for the final project. '), ('./static/2.mp4', 368, u'You can either do our default final project, '), ('./static/2.mp4', 372, u'which is a good option for many people, '), ('./static/2.mp4', 374, u"or you can do a custom final project and I'll "), ('./static/2.mp4', 375, u'talk about that in the more in the beginning. '), ('./static/2.mp4', 379, u'This is not working right. '), ('./static/2.mp4', 381, u'Um, and so then at the end we have '), ('./static/2.mp4', 385, u'a final poster presentation session at which your attendance is expected, '), ('./static/2.mp4', 390, u"and we're gonna be having that Wednesday in the evening. "), ('./static/2.mp4', 394, u"Probably not quite five hours but it'll be within that window, "), ('./static/2.mp4', 397, u"we'll work out the details in a bit. "), ('./static/2.mp4', 399, u'Three percent for participation, '), ('./static/2.mp4', 401, u'see the website for details. '), ('./static/2.mp4', 403, u'Six late days, um, '), ('./static/2.mp4', 405, u'collaboration, like always in computer science classes, '), ('./static/2.mp4', 410, u"we want you to do your own work and not borrow stuff from other people's Githubs and "), ('./static/2.mp4', 415, u'so we really do emphasize that you should '), ('./static/2.mp4', 417, u'read and pay attention to collaboration policies. '), ('./static/2.mp4', 421, u"Okay. So here's the high level plan for the problem sets. "), ('./static/2.mp4', 424, u'So, homework one available right now, '), ('./static/2.mp4', 427, u'is a hopefully easy on ramp. '), ('./static/2.mp4', 430, u"That's on iPython notebook, "), ('./static/2.mp4', 431, u'just help get everyone up to speed. '), ('./static/2.mp4', 433, u'Homework two is pure Python plus numpy but that '), ('./static/2.mp4', 437, u'will start to kind of teach you more about the sort of underlying, '), ('./static/2.mp4', 442, u'how do we do deep learning. '), ('./static/2.mp4', 444, u"If you're not so good or a bit rusty or never seen um, "), ('./static/2.mp4', 449, u'Python or numpy, um, '), ('./static/2.mp4', 451, u"we're gonna have an extra section on Friday. "), ('./static/2.mp4', 454, u'So Friday from 1:30 to 2:50 um, '), ('./static/2.mp4', 458, u"in Skilling Auditorium, we'll have a section that's a Python review. "), ('./static/2.mp4', 462, u"That's our only plan section at the moment, "), ('./static/2.mp4', 464, u"we're not gonna have a regular section. "), ('./static/2.mp4', 466, u'Um, so encourage to go to that and that will also be '), ('./static/2.mp4', 469, u'recorded for SCPD and available for video as well. '), ('./static/2.mp4', 473, u'Um, then Homework three um, '), ('./static/2.mp4', 476, u'will start us on using PyTorch. '), ('./static/2.mp4', 480, u"And then homeworks four and five we're then gonna be using "), ('./static/2.mp4', 484, u"py- PyTorch on GPU and we're actually gonna be using "), ('./static/2.mp4', 488, u'Microsoft Azure with big thank yous to the kind Microsoft Azure people who have '), ('./static/2.mp4', 493, u'sponsored our GPU computing for the last um, three years. '), ('./static/2.mp4', 499, u'Um, yes. So basically I mean all of modern deep learning has moved to the use '), ('./static/2.mp4', 504, u'of one or other of the large deep learning libraries like PyTorch TensorFlow, '), ('./static/2.mp4', 510, u'Chainer or MXNet um, '), ('./static/2.mp4', 512, u'et cetera and then doing the computing on GPU. '), ('./static/2.mp4', 516, u"So of course since we're in the one building, "), ('./static/2.mp4', 518, u'we should of course be using, um, '), ('./static/2.mp4', 520, u'GPUs [LAUGHTER] but I mean in general '), ('./static/2.mp4', 522, u"the so parallelisms scalability of GPUs is what's powered most of modern deep learning. "), ('./static/2.mp4', 528, u'Okay. The final project. '), ('./static/2.mp4', 530, u'So for the final project there are two things that you can do. '), ('./static/2.mp4', 535, u'So we have a default final project which is essentially our final project in a box. '), ('./static/2.mp4', 540, u'And so this is building a question answering system and we do it over the squad dataset. '), ('./static/2.mp4', 546, u'So what you build and how you can improve your performance is completely up to you. '), ('./static/2.mp4', 551, u'It is open-ended but it has an easier start, '), ('./static/2.mp4', 554, u'a clearly defined objective and we can '), ('./static/2.mp4', 556, u'have a leaderboard for how well things are working. '), ('./static/2.mp4', 559, u"Um, so if you don't have a clear research objective that can be a good choice for you "), ('./static/2.mp4', 564, u"or you can propose the custom Final Project and  assuming it's sensible, "), ('./static/2.mp4', 569, u'we will approve your custom final project, '), ('./static/2.mp4', 572, u'we will give you feedback, um, '), ('./static/2.mp4', 574, u'form someone as a mentor, um, '), ('./static/2.mp4', 576, u'and either way for only the final project we allow teams of one, two or three. '), ('./static/2.mp4', 582, u'For the homework should expect it to do them yourself. '), ('./static/2.mp4', 585, u'Of course you can chat to people in a general way about the problems. '), ('./static/2.mp4', 590, u'Okay. So that is the course. '), ('./static/2.mp4', 593, u'All good, and not even behind schedule yet. '), ('./static/2.mp4', 595, u'Okay. So the next section is human language and word meaning.Um. '), ('./static/2.mp4', 601, u'You know, if I was um, '), ('./static/2.mp4', 604, u'really going to tell you a lot about human language that would take a lot of time um, '), ('./static/2.mp4', 610, u"which I don't really have here. "), ('./static/2.mp4', 612, u"So I'm just going to tell you um, "), ('./static/2.mp4', 614, u'two anecdotes about human language. '), ('./static/2.mp4', 616, u'And the first is this XKCD cartoon. '), ('./static/2.mp4', 619, u"Um, and I mean this isn't, "), ('./static/2.mp4', 622, u"and I don't know why that's happening. "), ('./static/2.mp4', 626, u"I'm not sure what to make of that. "), ('./static/2.mp4', 628, u'Um, so, I actually really liked this XKCD cartoon. '), ('./static/2.mp4', 634, u"It's not one of the classic ones that you see most often around the place, "), ('./static/2.mp4', 637, u'but I actually think it says a lot about language and is worth thinking about. '), ('./static/2.mp4', 642, u'Like I think a lot of the time for the kind of people who come '), ('./static/2.mp4', 645, u'to this class who are mainly people like CS people, '), ('./static/2.mp4', 649, u'and EE people and random others. '), ('./static/2.mp4', 651, u"There's some other people I know since these people linguists and so on around. "), ('./static/2.mp4', 655, u'But for a lot of those people like, '), ('./static/2.mp4', 657, u"you've sort of spent your life looking at formal languages and the impression "), ('./static/2.mp4', 661, u'is that sort of human language as a sort of somehow a little bit broken formal languages, '), ('./static/2.mp4', 666, u"but there's really a lot more to it than that, right? "), ('./static/2.mp4', 668, u'That language is this amazing um, '), ('./static/2.mp4', 671, u'human created system that is used for '), ('./static/2.mp4', 675, u'all sorts of purposes and is adaptable to all sorts of purposes. '), ('./static/2.mp4', 679, u'So you can do everything from describing mathematics and human language '), ('./static/2.mp4', 683, u'um to sort of nuzzling up to your best friend and getting them to understand you better. '), ('./static/2.mp4', 688, u"So there's actually an amazing thing of human language. Anyway, I'll just read it. "), ('./static/2.mp4', 691, u"Um, so it's the first person, "), ('./static/2.mp4', 694, u'the dark haired person says, '), ('./static/2.mp4', 696, u'"Anyway, I could care less." '), ('./static/2.mp4', 698, u'And her friend says, '), ('./static/2.mp4', 700, u'"I think you mean you couldn\'t care less." '), ('./static/2.mp4', 702, u'Saying you could care less implies you care at least some amount. '), ('./static/2.mp4', 706, u'And the dark haired person says, "I don\'t know, '), ('./static/2.mp4', 709, u"we're these unbelievably complicated brains drifting through a void trying "), ('./static/2.mp4', 714, u'in vain to connect with one another by blindly flinging words out into the darkness." '), ('./static/2.mp4', 719, u'Every choice of phrasing and spelling, and tone, '), ('./static/2.mp4', 722, u'and timing carries countless signals and contexts and subtexts and more. '), ('./static/2.mp4', 727, u'And every listener interprets those signals in their own way. '), ('./static/2.mp4', 731, u"Language isn't a formal system, "), ('./static/2.mp4', 733, u'language is glorious chaos. '), ('./static/2.mp4', 736, u'You can never know for sure what any words will mean to anyone. '), ('./static/2.mp4', 740, u'All you can do is try to get better at guessing how your words affect people so '), ('./static/2.mp4', 746, u'you can have a chance of finding the ones that will make '), ('./static/2.mp4', 748, u'them feel something like what you want them to feel. '), ('./static/2.mp4', 751, u'Everything else is pointless. '), ('./static/2.mp4', 754, u"I assume you're giving me tips on how you interpret "), ('./static/2.mp4', 757, u'words because you want me to feel less alone. '), ('./static/2.mp4', 761, u'If so, thank you. '), ('./static/2.mp4', 763, u'That means a lot. '), ('./static/2.mp4', 765, u"But if you're just running my sentences past "), ('./static/2.mp4', 768, u'some mental checklist so you can show off how well you know it, '), ('./static/2.mp4', 771, u'then I could care less. '), ('./static/2.mp4', 773, u'[NOISE] Um, and so I think um, '), ('./static/2.mp4', 782, u'I think actually this has some nice messages about how language is this uncertain '), ('./static/2.mp4', 787, u'evolved system of communication but somehow we have enough agreed meaning that you know, '), ('./static/2.mp4', 793, u'we can kind of pretty much communicate. '), ('./static/2.mp4', 795, u"But we're doing some kind of you know "), ('./static/2.mp4', 796, u"probabilistic inference of guessing what people mean and we're "), ('./static/2.mp4', 800, u'using language not just for '), ('./static/2.mp4', 802, u'the information functions but for the social functions etc etc. '), ('./static/2.mp4', 806, u"Okay. And then here's my one other thought I had review about language. "), ('./static/2.mp4', 813, u"So, essentially if we want to have artificial intelligence that's intelligent, "), ('./static/2.mp4', 820, u'what we need to somehow get to the point of having '), ('./static/2.mp4', 823, u'compu- computers that have the knowledge of human beings, right? '), ('./static/2.mp4', 828, u'Because human beings have knowledge that gives them intelligence. '), ('./static/2.mp4', 832, u'And if you think about how we sort of '), ('./static/2.mp4', 835, u'convey knowledge around the place in our human world, '), ('./static/2.mp4', 839, u'mainly the way we do it is through human language. '), ('./static/2.mp4', 844, u'You know, some kinds of knowledge you can sort of '), ('./static/2.mp4', 846, u'work out for yourself by doing physical stuff right, '), ('./static/2.mp4', 849, u"I can hold this and drop that and I've learnt something. "), ('./static/2.mp4', 851, u'So I have to learn a bit of knowledge there. '), ('./static/2.mp4', 853, u"But sort of most of the knowledge in your heads and why you're sitting in "), ('./static/2.mp4', 857, u'this classroom has come from people communicating in human language to you. '), ('./static/2.mp4', 861, u'Um, so one of the famous, '), ('./static/2.mp4', 864, u'most famous steep learning people Yann Le Cun, '), ('./static/2.mp4', 866, u'he likes to say this line about, '), ('./static/2.mp4', 869, u"oh, you know really I think that you know there's not much difference "), ('./static/2.mp4', 873, u'between the intelligence of human being and orangutan. '), ('./static/2.mp4', 877, u"And I actually think he's really wrong on that. "), ('./static/2.mp4', 880, u'Like the sense in which he means that is, '), ('./static/2.mp4', 882, u'an orangutan has a really good vision system. '), ('./static/2.mp4', 885, u'Orangutans have very good you know control of '), ('./static/2.mp4', 888, u'their arms just like human beings for picking things up. '), ('./static/2.mp4', 892, u'Orangutans um can use tools um and orangutans can make plans so '), ('./static/2.mp4', 898, u'that if you sort of put the food somewhere where they have to sort of move '), ('./static/2.mp4', 902, u'the plank to get to the island with the food they can do a plan like that. '), ('./static/2.mp4', 905, u"So yeah, in a sense they've got a fair bit of intelligence but you know, "), ('./static/2.mp4', 909, u"sort of orangutans just aren't like human beings. "), ('./static/2.mp4', 913, u"And why aren't they like human beings? "), ('./static/2.mp4', 916, u"And I'd like to suggest to you the reason for that is what human beings have achieved is, "), ('./static/2.mp4', 921, u"we don't just have sort of one computer like "), ('./static/2.mp4', 925, u"a you know dusty old IBM PC in your mother's garage. "), ('./static/2.mp4', 929, u'What we have is a human computer network. '), ('./static/2.mp4', 933, u"And the way that we've achieved that human computer network is that, "), ('./static/2.mp4', 937, u'we use human languages as our networking language. '), ('./static/2.mp4', 941, u'Um, and so, when you think about it um, '), ('./static/2.mp4', 944, u'so on any kind of evolutionary scale language is super super super super recent, right? '), ('./static/2.mp4', 951, u"That um, creatures have had vision for people don't quite know but you know, "), ('./static/2.mp4', 957, u"maybe it's 75 million years or maybe it's longer, right? "), ('./static/2.mp4', 960, u'A huge length of time. '), ('./static/2.mp4', 963, u'How long have human beings have had language? '), ('./static/2.mp4', 967, u"You know people don't know that either because it turns out you know, "), ('./static/2.mp4', 969, u'when you have fossils, '), ('./static/2.mp4', 971, u"you can't knock the skull on the side and say, "), ('./static/2.mp4', 973, u'do you not have language. '), ('./static/2.mp4', 975, u'Um, but you know, most people estimate that sort of language is '), ('./static/2.mp4', 979, u'a very recent invention before current human beings moved out of um, out of Africa. '), ('./static/2.mp4', 985, u"So that many people think that we've only had language for "), ('./static/2.mp4', 988, u'something like a 100,000 years or something like that. '), ('./static/2.mp4', 991, u"So that's sort of you know blink of an eye on the evolutionary timescale. "), ('./static/2.mp4', 995, u'But you know, it was the development of language [inaudible] '), ('./static/2.mp4', 999, u'that sort of made human beings invisible- [NOISE] in invincible, right? '), ('./static/2.mp4', 1003, u"It wasn't that, human beings um, "), ('./static/2.mp4', 1006, u'developed poison fangs or developed ability to run '), ('./static/2.mp4', 1011, u'faster than any other creature or '), ('./static/2.mp4', 1013, u'put a big horn on their heads or something like that, right? '), ('./static/2.mp4', 1016, u'You know, humans are basically pretty puny um, '), ('./static/2.mp4', 1019, u'but they had this um, '), ('./static/2.mp4', 1021, u'unbeatable advantage that they could communicate with '), ('./static/2.mp4', 1024, u'each other and therefore work much more effectively in teams. '), ('./static/2.mp4', 1027, u'And that sort of basically made human beings invincible. '), ('./static/2.mp4', 1031, u'But you know, even then humans were kind of limited, right? '), ('./static/2.mp4', 1035, u'That kind of got you to about the Stone Age right, '), ('./static/2.mp4', 1038, u'where you could bang on your stones and with '), ('./static/2.mp4', 1040, u'the right kind of stone make something sharp to cut with. '), ('./static/2.mp4', 1043, u'Um, what got humans beyond that, '), ('./static/2.mp4', 1045, u'was that they invented writing. '), ('./static/2.mp4', 1048, u'So writing was then an ability where you could take knowledge '), ('./static/2.mp4', 1052, u'not only communicated um mouth to mouth to people that you saw. '), ('./static/2.mp4', 1057, u'You could put it down on your piece of papyrus so your clay tablet or whatever '), ('./static/2.mp4', 1061, u'it was at first and that knowledge could then be sent places. '), ('./static/2.mp4', 1065, u'It could be sent spatially around the world and it could then '), ('./static/2.mp4', 1070, u'be sent temporally through time. '), ('./static/2.mp4', 1075, u'And well, how old is writing? '), ('./static/2.mp4', 1077, u'I mean, we sort of basically know about how old writing is, right? '), ('./static/2.mp4', 1080, u'That writing is about 5,000 years old. '), ('./static/2.mp4', 1084, u"It's incredibly incredibly recent on this scale of evolution but you know, "), ('./static/2.mp4', 1089, u'essentially writing was so powerful as a way of having knowledge that then in those 5,000 '), ('./static/2.mp4', 1096, u'years that enabled human beings to go from stone age sharp piece or flint to you know, '), ('./static/2.mp4', 1104, u'having iPhones and all of these things, '), ('./static/2.mp4', 1106, u'all these incredibly sophisticated devices. '), ('./static/2.mp4', 1108, u"So, language is pretty special thing I'd like to suggest. "), ('./static/2.mp4', 1112, u"Um, but you know, if I go back to my analogy that sort of it's allowed humans to "), ('./static/2.mp4', 1117, u'construct a networked computer that is way way more powerful than um, '), ('./static/2.mp4', 1123, u'just having individual creatures as sort of intelligent like an orangutan. '), ('./static/2.mp4', 1127, u'Um, and you compare it to our computer networks, '), ('./static/2.mp4', 1130, u"it's a really funny kind of network, right? "), ('./static/2.mp4', 1133, u'You know that these days um, '), ('./static/2.mp4', 1135, u'we have networks that run around where we have sort of large network bandwidth, right? '), ('./static/2.mp4', 1141, u'You know, we might be frustrated sometimes with '), ('./static/2.mp4', 1143, u'our Netflix downloads but by and large you know, '), ('./static/2.mp4', 1146, u'we can download hundreds of megabytes really easily and quickly. '), ('./static/2.mp4', 1149, u"And we don't think that's fast enough, "), ('./static/2.mp4', 1151, u"so we're going to be rolling out 5G networks. "), ('./static/2.mp4', 1153, u"So it's an order of magnitude faster again. "), ('./static/2.mp4', 1156, u'I mean, by comparison to that, I mean, '), ('./static/2.mp4', 1158, u'human language is a pathetically slow network, right? '), ('./static/2.mp4', 1163, u'That the amount of information you can convey by human language is very slow. '), ('./static/2.mp4', 1169, u'I mean you know, whatever it is I sort of speak at about 15 words a second right, '), ('./static/2.mp4', 1173, u'you can start doing um, '), ('./static/2.mp4', 1175, u'your information theory if you know some right? '), ('./static/2.mp4', 1177, u"But um, you don't actually get much bandwidth at all. "), ('./static/2.mp4', 1181, u'And that then leads- so you can think of, '), ('./static/2.mp4', 1184, u'how does it work then? '), ('./static/2.mp4', 1185, u'So, humans have come up with '), ('./static/2.mp4', 1187, u'this incredibly impressive system which is essentially form of compression. '), ('./static/2.mp4', 1193, u'Sort of a very adaptive form of compression, '), ('./static/2.mp4', 1196, u"so that when we're talking to people, "), ('./static/2.mp4', 1198, u'we assume that they have an enormous amount of knowledge in their heads which '), ('./static/2.mp4', 1202, u"isn't the same as but it's broadly similar to mine when I'm talking to you right? "), ('./static/2.mp4', 1207, u'That you know what English words mean, '), ('./static/2.mp4', 1210, u'and you know a lot about how the wor- world works. '), ('./static/2.mp4', 1213, u'And therefore, I can say a short message and communicate '), ('./static/2.mp4', 1217, u'only a relatively short bit string and you can actually understand a lot. All right? '), ('./static/2.mp4', 1222, u'So, I can say sort of whatever you know, '), ('./static/2.mp4', 1226, u'imagine a busy shopping mall and that '), ('./static/2.mp4', 1228, u'there are two guys standing in front of a makeup counter, '), ('./static/2.mp4', 1231, u"and you know I've only said whatever that was sort of about 200 bits of "), ('./static/2.mp4', 1236, u"information but that's enabled you to construct "), ('./static/2.mp4', 1238, u"a whole visual scene that we're taking megabytes to um, "), ('./static/2.mp4', 1242, u'represent as an image. '), ('./static/2.mp4', 1244, u"So, that's why language is good. "), ('./static/2.mp4', 1246, u'Um, so from that more authorial level, '), ('./static/2.mp4', 1249, u"I'll now move back to the concrete stuff. "), ('./static/2.mp4', 1251, u'What we wanna do in this class is not solve the whole of language, '), ('./static/2.mp4', 1255, u'but we want to represent, um, '), ('./static/2.mp4', 1257, u'the meaning of words, right? '), ('./static/2.mp4', 1260, u'So, a lot of language is bound up in words and their meanings '), ('./static/2.mp4', 1263, u'and words can have really rich meanings, right? '), ('./static/2.mp4', 1266, u'As soon as you say a word teacher, '), ('./static/2.mp4', 1267, u"that's kinda quite a lot of rich meaning or you can have actions that have rich meaning. "), ('./static/2.mp4', 1272, u'So, if I say a word like prognosticate or, '), ('./static/2.mp4', 1277, u'um, total or something you know, '), ('./static/2.mp4', 1279, u'these words that have rich meanings and a lot of nuance on them. '), ('./static/2.mp4', 1282, u'And so we wanna represent meaning. '), ('./static/2.mp4', 1284, u'And so, the question is what is meaning? '), ('./static/2.mp4', 1286, u'So, you can of course you can- dictionaries are meant to tell you about meanings. '), ('./static/2.mp4', 1289, u'So, you can look up dictionaries um, '), ('./static/2.mp4', 1291, u'and Webster says sort of tries to relate meaning to idea. '), ('./static/2.mp4', 1295, u'The idea that is represented by a word or a phrase. '), ('./static/2.mp4', 1299, u'The idea that a person wants to express by word signs et cetera. '), ('./static/2.mp4', 1304, u'I mean, you know, '), ('./static/2.mp4', 1306, u'you could think that these definitions are kind of a cop-out because it seems '), ('./static/2.mp4', 1309, u"like they're rewriting meaning in terms of the word idea, "), ('./static/2.mp4', 1313, u'and is that really gotten you anywhere. '), ('./static/2.mp4', 1315, u'Um, how do linguists think about meaning? '), ('./static/2.mp4', 1318, u'I mean, the most common way that linguists have thought about '), ('./static/2.mp4', 1323, u"meaning is an idea that's called denotational "), ('./static/2.mp4', 1325, u'semantics which is also used in programming languages. '), ('./static/2.mp4', 1328, u'So, the idea of that is we think of meaning as what things represent. '), ('./static/2.mp4', 1334, u'So, if I say the word chair, '), ('./static/2.mp4', 1336, u'the denotation of the word chair includes this one here and that one, '), ('./static/2.mp4', 1341, u'that one, that one, that one. '), ('./static/2.mp4', 1342, u'And so, the word chair is sort of representing '), ('./static/2.mp4', 1344, u'all the things that are chairs and you can sort of, um, '), ('./static/2.mp4', 1348, u"you can then think of something like running as well that you know there's sort of sets "), ('./static/2.mp4', 1353, u"of actions that people can partake that- that's their denotation. "), ('./static/2.mp4', 1357, u"And that's sort of what you most commonly see in philosophy or linguistics as denotation. "), ('./static/2.mp4', 1362, u"It's kind of a hard thing to get your hands on, um, computationally. "), ('./static/2.mp4', 1367, u'So, um, what type of people most commonly '), ('./static/2.mp4', 1370, u'do or use the most commonly do I guess I should say now '), ('./static/2.mp4', 1374, u'for working out the meaning of words on the computer that '), ('./static/2.mp4', 1377, u'commonly that turn to something that was a bit like a dictionary. '), ('./static/2.mp4', 1381, u'In particular favorite online thing was this online thesaurus called WordNet which '), ('./static/2.mp4', 1386, u'sort of tells you about word meanings and relationships between word meanings. '), ('./static/2.mp4', 1391, u'Um, so this is just giving you the very slices sense of, '), ('./static/2.mp4', 1396, u"um, of what's in WordNet. "), ('./static/2.mp4', 1399, u'Um, so this is an actual bit of Python code up there which you can, '), ('./static/2.mp4', 1404, u'um, type into your computer and run and do this for yourself. '), ('./static/2.mp4', 1408, u'Um, so this uses a thing called NLTK. '), ('./static/2.mp4', 1411, u'Um, so NLTK is sort of like '), ('./static/2.mp4', 1413, u'the "Swiss Army Knife of NLP" meaning that it\'s not terribly good for anything, '), ('./static/2.mp4', 1419, u'but it has a lot of basic tools. '), ('./static/2.mp4', 1421, u'So, if you wanted to do something like just get some stuff out of WordNet and show it, '), ('./static/2.mp4', 1426, u"it's the perfect thing to use. Um, okay. "), ('./static/2.mp4', 1429, u"So, um, from NLTK I'm importing WordNet and so then I can say, "), ('./static/2.mp4', 1434, u'"Okay, um, for the word good tell me about the synonym sets with good participates in." '), ('./static/2.mp4', 1441, u"And there's good goodness as a noun. "), ('./static/2.mp4', 1443, u'There is an adjective good. '), ('./static/2.mp4', 1444, u"There's one estimable good, honorable, respectable. "), ('./static/2.mp4', 1448, u'Um, this looks really complex and hard to understand. '), ('./static/2.mp4', 1451, u'But the idea of word- WordNet makes '), ('./static/2.mp4', 1453, u'these very fine grain distinctions between senses of a word. '), ('./static/2.mp4', 1458, u'So, what sort of saying for good, um, '), ('./static/2.mp4', 1460, u"there's what some sensors where it's a noun, right? "), ('./static/2.mp4', 1463, u"That's where you sort of, "), ('./static/2.mp4', 1464, u'I bought some goods for my trip, right? '), ('./static/2.mp4', 1467, u"So, that's sort of, um, "), ('./static/2.mp4', 1468, u'one of these noun sensors like this one I guess. '), ('./static/2.mp4', 1472, u"Um, then there are adjective sensors and it's trying to "), ('./static/2.mp4', 1475, u"distinguish- there's a basic adjective sense of good being good, "), ('./static/2.mp4', 1478, u'and then in certain, um, sensors, '), ('./static/2.mp4', 1481, u'there are these extended sensors of good in different directions. '), ('./static/2.mp4', 1484, u'So, I guess this is good in the sense of beneficial, um, '), ('./static/2.mp4', 1488, u'and this one is sort of person who is respectable or something. '), ('./static/2.mp4', 1492, u"He's a good man or something like that, right? "), ('./static/2.mp4', 1495, u'So, um, but you know, '), ('./static/2.mp4', 1496, u"part of what's kind of makes us "), ('./static/2.mp4', 1499, u'think very problematic and practice to use is it tries to make '), ('./static/2.mp4', 1502, u'all these very fine-grain differences between sensors that are a human being can '), ('./static/2.mp4', 1506, u'barely understand the difference between them um, and relate to. '), ('./static/2.mp4', 1511, u'Um, so you can then do other things with WordNet. '), ('./static/2.mp4', 1513, u'So, this bit of code you can sort of well walk up and is a kind of hierarchy. '), ('./static/2.mp4', 1518, u"So, it's kinda like a traditional, um, database. "), ('./static/2.mp4', 1521, u'So, if I start with a panda and say- [NOISE] if I start with a panda. '), ('./static/2.mp4', 1529, u'Um, and walk up, um, '), ('./static/2.mp4', 1532, u'the pandas are [inaudible]. '), ('./static/2.mp4', 1535, u"Maybe you'd guys to bio which are carnivores, "), ('./static/2.mp4', 1537, u'placentals, mammals, blah, blah, blah. '), ('./static/2.mp4', 1539, u"Okay, so, um, that's the kind of stuff you can get out to- out of WordNet. "), ('./static/2.mp4', 1544, u'Um, you know, in practice WordNet has been. '), ('./static/2.mp4', 1547, u'Everyone sort of used to use it because it gave '), ('./static/2.mp4', 1549, u'you some sort of sense of the meaning of the word. '), ('./static/2.mp4', 1551, u"But you know it's also sort of well-known. "), ('./static/2.mp4', 1554, u'It never worked that well. '), ('./static/2.mp4', 1556, u'Um, so you know that sort of the synonym sets miss a lot of nuance. '), ('./static/2.mp4', 1562, u'So, you know one of the synonym sets for good has '), ('./static/2.mp4', 1565, u'proficient in it and good sort of like proficient '), ('./static/2.mp4', 1568, u"but doesn't proficient have some more connotations and nuance? "), ('./static/2.mp4', 1571, u'I think it does. '), ('./static/2.mp4', 1573, u'Um, WordNet like most hand built resources is sort of very incomplete. '), ('./static/2.mp4', 1578, u"So, as soon as you're coming to new meanings of words, "), ('./static/2.mp4', 1581, u'or new words and slang words, '), ('./static/2.mp4', 1583, u'well then, that gives you nothing. '), ('./static/2.mp4', 1585, u"Um, it's sort of built with human labor, "), ('./static/2.mp4', 1588, u"um, in ways that you know it's hard to sort of create and adapt. "), ('./static/2.mp4', 1595, u'And in particular, what we want to focus on is, '), ('./static/2.mp4', 1597, u"seems like a basic thing you'd like to do with words and it's actually at least "), ('./static/2.mp4', 1601, u'understand similarities and relations between the meaning of words. '), ('./static/2.mp4', 1605, u"And it turns out that you know WordNet doesn't actually do that that well "), ('./static/2.mp4', 1609, u'because it just has these sort of fixed discrete synonym sets. '), ('./static/2.mp4', 1613, u"So, if you have a words in a synonym said that there's "), ('./static/2.mp4', 1616, u'sort of a synonym and maybe not exactly the same meaning, '), ('./static/2.mp4', 1619, u"they're not in the same synonyms set, "), ('./static/2.mp4', 1620, u"you kind of can't really measure the partial resemblance as a meaning for them. "), ('./static/2.mp4', 1624, u"So, if something like good and marvelous aren't in the same synonym set, "), ('./static/2.mp4', 1628, u"but there's something that they share in common that you'd like to represent. "), ('./static/2.mp4', 1631, u"Okay. So, um, that's kinda turn to lead into "), ('./static/2.mp4', 1636, u'us wanting to do something different and better for word meaning. '), ('./static/2.mp4', 1641, u'And, um, before getting there I just sort of wanna again sort '), ('./static/2.mp4', 1645, u'of build a little from traditional NLP. '), ('./static/2.mp4', 1649, u'So, traditional NLP in the context of this course sort of means '), ('./static/2.mp4', 1653, u'Natural Language Processing up until approximately 2012. '), ('./static/2.mp4', 1659, u'There were some earlier antecedents but as basically, um, '), ('./static/2.mp4', 1663, u'in 2013 that things really began to change with '), ('./static/2.mp4', 1667, u'people starting to use neural net style representations for natural language processing. '), ('./static/2.mp4', 1673, u'So, up until 2012, '), ('./static/2.mp4', 1675, u'um, standardly you know we had words. '), ('./static/2.mp4', 1678, u'They are just words. So, we had hotel conference motel. '), ('./static/2.mp4', 1682, u"They were words, and we'd have you know lexicons and put words into our model. "), ('./static/2.mp4', 1686, u'Um, and in neural networks land this is referred to as a localist representation. '), ('./static/2.mp4', 1692, u"I'll come back to those terms again next time. "), ('./static/2.mp4', 1694, u"But that's sort of meaning that for any concept there's sort of one particular, "), ('./static/2.mp4', 1700, u'um, place which is the word hotel or the word motel. '), ('./static/2.mp4', 1704, u'A way of thinking about that is to think '), ('./static/2.mp4', 1706, u'about what happens when you build a machine learning model. '), ('./static/2.mp4', 1709, u'So, if you have a categorical variable like you have words with the choice of word '), ('./static/2.mp4', 1714, u'and you want to stick that into some kind of classifier in a Machine Learning Model, '), ('./static/2.mp4', 1720, u'somehow you have to code that categorical variable, '), ('./static/2.mp4', 1722, u'and the standard way of doing it is that you code it by having '), ('./static/2.mp4', 1726, u'different levels of the variable which means that you have a vector, '), ('./static/2.mp4', 1731, u'and you have, this is the word house. '), ('./static/2.mp4', 1733, u'This is the word cat. This is the word dog. '), ('./static/2.mp4', 1735, u'This is the word some chairs. '), ('./static/2.mp4', 1737, u'This is the word agreeable. '), ('./static/2.mp4', 1738, u'This is the word something else. '), ('./static/2.mp4', 1739, u'This is the word, um, '), ('./static/2.mp4', 1741, u'hotel, um, and this is another word for something different, right? '), ('./static/2.mp4', 1745, u'So that you have put a one at the position '), ('./static/2.mp4', 1748, u'and neural net land we call these one-hot vectors, '), ('./static/2.mp4', 1751, u'and so these might be, ah, '), ('./static/2.mp4', 1752, u'one-hot vectors for hotel and motel. '), ('./static/2.mp4', 1756, u'So, there are a couple of things that are bad here. '), ('./static/2.mp4', 1759, u"Um, the one that's sort of, ah, "), ('./static/2.mp4', 1761, u'practical nuisance is you know languages have a lot of words. '), ('./static/2.mp4', 1767, u"Ah, so, it's sort of one of those dictionaries that you might have still had in "), ('./static/2.mp4', 1770, u'school that you probably have about 250,000 words in them. '), ('./static/2.mp4', 1775, u'But you know, if you start getting into '), ('./static/2.mp4', 1777, u"more technical and scientific English it's easy to get to a million words. "), ('./static/2.mp4', 1781, u'I mean, actually the number of words that you have in a language, um, '), ('./static/2.mp4', 1785, u'like English is actually infinite because we have '), ('./static/2.mp4', 1788, u'these processes which are called derivational morphology, '), ('./static/2.mp4', 1792, u'um, where you can make more words by adding endings onto existing words. '), ('./static/2.mp4', 1796, u'So, you know you can start with something like paternalist, '), ('./static/2.mp4', 1799, u'fatherly, and then you can sort of say from maternal, '), ('./static/2.mp4', 1803, u'you can say paternalist, or paternalistic, '), ('./static/2.mp4', 1806, u'paternalism and pa- I did it paternalistically. '), ('./static/2.mp4', 1810, u'Right? Now all of these ways that you can bake bigger words by adding more stuff into it. '), ('./static/2.mp4', 1814, u'Um, and so really you end up with an infinite space of words. '), ('./static/2.mp4', 1818, u"Um, yeah. So that's a minor problem, right? "), ('./static/2.mp4', 1822, u'We have very big vectors if we want to represent a sensible size vocabulary. '), ('./static/2.mp4', 1828, u"Um, but there's a much bigger problem than that, which is, well, "), ('./static/2.mp4', 1831, u'precisely what we want to do all the time, is we want to, '), ('./static/2.mp4', 1835, u'sort of, understand relationships and the meaning of words. '), ('./static/2.mp4', 1838, u'So, you know, an obvious example of this is web search. '), ('./static/2.mp4', 1842, u'So, if I do a search for Seattle motel, '), ('./static/2.mp4', 1845, u"it'd be useful if it also showed me results that had "), ('./static/2.mp4', 1848, u'Seattle hotel on the page and vice versa because, '), ('./static/2.mp4', 1852, u'you know, hotels and motels pretty much the same thing. '), ('./static/2.mp4', 1855, u'Um, but, you know, if we have these one-hot vectors like we had before they have '), ('./static/2.mp4', 1859, u'no s- similarity relationship between them, right? '), ('./static/2.mp4', 1864, u'So, in math terms, '), ('./static/2.mp4', 1865, u'these two vectors are orthogonal. '), ('./static/2.mp4', 1867, u'No similarity relationship between them. '), ('./static/2.mp4', 1870, u'Um, and so you, '), ('./static/2.mp4', 1872, u'kind of, get nowhere. '), ('./static/2.mp4', 1874, u'Now, you know, there are things that you could do, '), ('./static/2.mp4', 1876, u"I- I just showed you WordNet's. "), ('./static/2.mp4', 1878, u"WordNet's shows you some synonyms and stuff. "), ('./static/2.mp4', 1880, u'So that might help a bit. '), ('./static/2.mp4', 1882, u'There are other things you could do. '), ('./static/2.mp4', 1884, u'You could sort of say, well wait, '), ('./static/2.mp4', 1885, u"why don't we just build up a big table where we have a big table of, "), ('./static/2.mp4', 1889, u'um, word similarities, and we could work with that. '), ('./static/2.mp4', 1892, u'And, you know, people used to try and do that, right? '), ('./static/2.mp4', 1894, u"You know, that's sort of what Google did in 2005 or something. "), ('./static/2.mp4', 1899, u'You know, it had word similarity tables. '), ('./static/2.mp4', 1902, u'The problem with doing that is you know, '), ('./static/2.mp4', 1904, u'we were talking about how maybe we want 500,000 words. '), ('./static/2.mp4', 1908, u'And if you want to build up then a word similarity table out '), ('./static/2.mp4', 1912, u'of our pairs of words from one-hot representations, '), ('./static/2.mp4', 1916, u'um, you- that means that the size of that table, '), ('./static/2.mp4', 1918, u'as my math is pretty bad, '), ('./static/2.mp4', 1920, u'is it 2.5 trillion? '), ('./static/2.mp4', 1922, u"It's some very big number of cells in your similarity, um, matrix. "), ('./static/2.mp4', 1927, u"So that's almost impossible to do. "), ('./static/2.mp4', 1929, u"So, what we're gonna instead do is explore a method in which, "), ('./static/2.mp4', 1933, u'um, we are going to represent words as vectors, '), ('./static/2.mp4', 1936, u"in a way I'll show you just, um, "), ('./static/2.mp4', 1938, u'a minute in such a way that just the representation of '), ('./static/2.mp4', 1941, u'a word gives you their similarity with no further work. '), ('./static/2.mp4', 1946, u"Okay. And so that's gonna lead into these different ideas. "), ('./static/2.mp4', 1950, u'So, I mentioned before denotational semantics. '), ('./static/2.mp4', 1954, u"Here's another idea for representing the meaning of words, "), ('./static/2.mp4', 1959, u'um, which is called distributional semantics. '), ('./static/2.mp4', 1961, u'And so the idea of distributional semantics is, well, '), ('./static/2.mp4', 1965, u'how are we going to represent the meaning of a word is by looking at the contexts, '), ('./static/2.mp4', 1970, u'um, in which it appears. '), ('./static/2.mp4', 1972, u'So, this is a picture of JR Firth who was a British linguist. '), ('./static/2.mp4', 1976, u"Um, he's famous for this saying, "), ('./static/2.mp4', 1978, u'"You shall know a word by the company it keeps." '), ('./static/2.mp4', 1981, u"Um, but another person who's very famous for developing this notion of meaning is, um, "), ('./static/2.mp4', 1986, u'the philosopher Ludwig- Ludwig Wittgenstein in his later writings, '), ('./static/2.mp4', 1990, u'which he referred to as a use theory of meeting- meaning. '), ('./static/2.mp4', 1993, u"Well, actually he's- he used some big German word that I don't know, "), ('./static/2.mp4', 1996, u"but, um, we'll call it a use theory of meaning. "), ('./static/2.mp4', 1998, u'And, you know, essentially the point was, well, you know, '), ('./static/2.mp4', 2002, u'if you can explain every- if- if you can '), ('./static/2.mp4', 2006, u"explain what contexts it's correct to use a certain word, "), ('./static/2.mp4', 2011, u'versus in what contexts would be the wrong word to use, '), ('./static/2.mp4', 2014, u'this maybe gives you bad memories of doing English in high school, '), ('./static/2.mp4', 2018, u"when people said, ah, that's the wrong word to use there, "), ('./static/2.mp4', 2020, u'um, well, then you understand the meaning of the word, right? '), ('./static/2.mp4', 2023, u"Um, and so that's the idea of distributional semantics. "), ('./static/2.mp4', 2027, u"And it's been- so one of the most successful ideas in "), ('./static/2.mp4', 2029, u'modern statistical NLP because it gives you a great way to learn about word meaning. '), ('./static/2.mp4', 2034, u"And so what we're gonna do is we're going to say, "), ('./static/2.mp4', 2036, u'haha, I want to know what the word banking means. '), ('./static/2.mp4', 2038, u"So, I'm gonna grab a lot of texts, "), ('./static/2.mp4', 2041, u'which is easy to do now when we have the World Wide Web, '), ('./static/2.mp4', 2044, u"I'll find lots of sentences where the word banking is used, "), ('./static/2.mp4', 2047, u'Government debt problems turning into banking crises as happened in 2009. '), ('./static/2.mp4', 2052, u"And both these- I'm just going to say all of "), ('./static/2.mp4', 2055, u'this stuff is the meaning of the word banking. '), ('./static/2.mp4', 2059, u'Um, that those are the contexts in which the word banking is used. '), ('./static/2.mp4', 2063, u'And that seems like very simple and perhaps even not quite right idea, '), ('./static/2.mp4', 2069, u'but it turns out to be a very usable idea that does a great job at capturing meaning. '), ('./static/2.mp4', 2074, u"And so what we're gonna do is say rather than "), ('./static/2.mp4', 2078, u"our old localist representation we're now gonna "), ('./static/2.mp4', 2082, u'represent words in what we call a distributed representation. '), ('./static/2.mp4', 2088, u"And so, for the distributed representation we're still going "), ('./static/2.mp4', 2091, u'to [NOISE] represent the meaning of a word as a numeric vector. '), ('./static/2.mp4', 2095, u"But now we're going to say that the meaning of each word is, "), ('./static/2.mp4', 2099, u'ah, smallish vector, um, '), ('./static/2.mp4', 2101, u"but it's going to be a dense vector where by all of the numbers are non-zero. "), ('./static/2.mp4', 2107, u'So the meaning of banking is going to be '), ('./static/2.mp4', 2110, u'distributed over the dim- dimensions of this vector. '), ('./static/2.mp4', 2113, u'Um, now, my vector here is of dimension nine because I want to keep the slide, um, nice. '), ('./static/2.mp4', 2119, u"Um, life isn't quite that good in practice. "), ('./static/2.mp4', 2123, u'When we do this we use a larger dimensionality, '), ('./static/2.mp4', 2125, u'kinda, solid the minimum that people use is 50. '), ('./static/2.mp4', 2129, u'Um, a typical number that you might use on your laptop is '), ('./static/2.mp4', 2132, u'300 if you want to really max out performance, '), ('./static/2.mp4', 2135, u'um, maybe 1,000, 2,000, 4,000. '), ('./static/2.mp4', 2138, u'But, you know, nevertheless [NOISE] orders of magnitude is '), ('./static/2.mp4', 2142, u'smaller compared to a length 500,000 vector. '), ('./static/2.mp4', 2146, u'Okay. So we have words with their vector representations. '), ('./static/2.mp4', 2151, u'And so since each word is going to have a vector, um, '), ('./static/2.mp4', 2155, u'representation we then have a vector space in which we can place all of the words. '), ('./static/2.mp4', 2161, u"Um, and that's completely unreadable, um, "), ('./static/2.mp4', 2163, u"but if you zoom into the vector space it's still completely unreadable. "), ('./static/2.mp4', 2168, u'But if you zoom in a bit further, '), ('./static/2.mp4', 2170, u'um, you can find different parts of this space. '), ('./static/2.mp4', 2173, u"So here's the part that where countries attending to, "), ('./static/2.mp4', 2176, u'um, exist Japanese, German, '), ('./static/2.mp4', 2178, u'French, Russian, British Australian American, '), ('./static/2.mp4', 2181, u'um, France, Britain, Germany et cetera. '), ('./static/2.mp4', 2185, u'And you can shift over to a different part of the space. '), ('./static/2.mp4', 2187, u"So here's a part of the space where various verbs are, "), ('./static/2.mp4', 2191, u'so has have, had, been, be. '), ('./static/2.mp4', 2193, u'Oops. Um, um, [inaudible] be always was where. '), ('./static/2.mp4', 2200, u'You can even see that some morphological forms are grouping together, '), ('./static/2.mp4', 2203, u'and things that sort of go together like say, '), ('./static/2.mp4', 2206, u'think expect to things that take those, kind of, compliment. '), ('./static/2.mp4', 2208, u'He said or thought something. '), ('./static/2.mp4', 2210, u'Um, they group together. '), ('./static/2.mp4', 2212, u'Now, what am I actually showing you here? '), ('./static/2.mp4', 2215, u'Um, you know, really this was built from, '), ('./static/2.mp4', 2217, u'ah, 100 dimensional word vectors. '), ('./static/2.mp4', 2220, u'And there is this problem is really hard to visualize 100 dimensional word vectors. '), ('./static/2.mp4', 2225, u'So, what is actually happening here is these, um, '), ('./static/2.mp4', 2229, u'100 dimensional word vectors are being projected down into two-dimensions, '), ('./static/2.mp4', 2235, u"and you're so- seeing the two-dimensional view, "), ('./static/2.mp4', 2237, u"which I'll get back to later. "), ('./static/2.mp4', 2239, u'Um, so, on the one hand, um, '), ('./static/2.mp4', 2242, u'whenever you see these pictures you should hold on to '), ('./static/2.mp4', 2244, u"the your wallet because there's a huge amount of "), ('./static/2.mp4', 2246, u'detail on the original vector space that got completely killed and went away, um, '), ('./static/2.mp4', 2251, u'in the 2D projection, '), ('./static/2.mp4', 2252, u'and indeed some of what push things together in the 2D, '), ('./static/2.mp4', 2257, u'um, projection may really, really, '), ('./static/2.mp4', 2259, u"really misrepresent what's in the original space. "), ('./static/2.mp4', 2262, u'Um, but even looking at these 2D representations, '), ('./static/2.mp4', 2265, u'the overall feeling is, '), ('./static/2.mp4', 2266, u"my gosh this actually sort of works, doesn't it? "), ('./static/2.mp4', 2268, u'Um, we can sort of see similarities, um, between words. '), ('./static/2.mp4', 2274, u'Okay. So, um, ha- so that was the idea of what we want to do. '), ('./static/2.mp4', 2282, u'Um, the next part, um, '), ('./static/2.mp4', 2284, u'is then how do we actually go about doing it? '), ('./static/2.mp4', 2287, u"I'll pause for breath for half a minute. "), ('./static/2.mp4', 2290, u"Has anyone got a question they're dying to ask? "), ('./static/2.mp4', 2292, u'[NOISE] Yeah. '), ('./static/2.mp4', 2300, u'Where were the- the vectors is each, um, '), ('./static/2.mp4', 2306, u'had a different order in each contact, '), ('./static/2.mp4', 2308, u'like, say the first decimal vector, '), ('./static/2.mp4', 2310, u'second decimal vector, are those standard '), ('./static/2.mp4', 2312, u'across all theory or people choose them themselves? '), ('./static/2.mp4', 2315, u"Um, they're not standards across NLP um and they're not chosen at all. "), ('./static/2.mp4', 2322, u"So what we're gonna present is a learning algorithm. "), ('./static/2.mp4', 2325, u'So where we just sort of shuffle in lots of text '), ('./static/2.mp4', 2328, u'and miraculously these word vectors come out. '), ('./static/2.mp4', 2331, u'And so the l- learning algorithm itself decides the dimensions. '), ('./static/2.mp4', 2337, u'But um that actually reminds me of something I sort of meant to say which was yeah, '), ('./static/2.mp4', 2343, u'I mean, since this is a vector space, '), ('./static/2.mp4', 2345, u'in some sense the dimensions over the arbitrary right, '), ('./static/2.mp4', 2349, u'because you can you know just have your basis vectors in '), ('./static/2.mp4', 2352, u'any different direction and you could sort of re-represent, '), ('./static/2.mp4', 2355, u'um the words in the vector space with a different set of basics, '), ('./static/2.mp4', 2359, u"basis vectors and it'd be exactly the same vector space "), ('./static/2.mp4', 2362, u'just sort of rotate around to your new um, vectors. '), ('./static/2.mp4', 2366, u"So, you know, you shouldn't read too much into the sort of elements. "), ('./static/2.mp4', 2370, u'So, it actually turns out that because of the way a lot of '), ('./static/2.mp4', 2372, u'deep learning um operations work, '), ('./static/2.mp4', 2376, u'some things they do, do element-wise. '), ('./static/2.mp4', 2378, u'So that the dimensions do actually tend to get some meaning to them it turns out. '), ('./static/2.mp4', 2382, u'But um, though I think I really wanted to say was, '), ('./static/2.mp4', 2386, u'that you know one thing we can just think of is how close things '), ('./static/2.mp4', 2392, u"are in the vector space and that's "), ('./static/2.mp4', 2394, u'a notion of meaning similarity that we are going to exploit. '), ('./static/2.mp4', 2397, u'But you might hope that you get more than that, '), ('./static/2.mp4', 2400, u"and you might actually think that there's meaning in "), ('./static/2.mp4', 2403, u'different dimensions and directions in the word vector space. '), ('./static/2.mp4', 2406, u"And the answer to that is there is and I'll come back to that a bit later. "), ('./static/2.mp4', 2411, u'Okay. Um, so in some sense this thing that had '), ('./static/2.mp4', 2417, u'the biggest impact um in sort of turning the world of '), ('./static/2.mp4', 2422, u'NLP in a neural networks direction was that picture. '), ('./static/2.mp4', 2427, u'Um, was this um algorithm that um '), ('./static/2.mp4', 2432, u'Thomas Mikolov came up with in 2013 called the word2vec algorithm. '), ('./static/2.mp4', 2437, u"So it wasn't the first work and having distributed representations of words. "), ('./static/2.mp4', 2443, u'So there was older work from Yoshua Bengio that went '), ('./static/2.mp4', 2445, u'back to about the sort of turn on the millennium, '), ('./static/2.mp4', 2448, u"that somehow it's sort of hadn't really sort of hit the world over their head and had "), ('./static/2.mp4', 2452, u'a huge impact and has really sort of Thomas Mikolov showed this very simple, '), ('./static/2.mp4', 2457, u'very scalable way of learning '), ('./static/2.mp4', 2460, u'vector representations of um words and that sort of really opened the flood gates. '), ('./static/2.mp4', 2465, u"And so that's the algorithm that I'm going to um show now. "), ('./static/2.mp4', 2468, u'Okay. So the idea of this algorithm is you start with a big pile of text. '), ('./static/2.mp4', 2475, u'Um, so wherever you find you know web pages on newspaper articles or something, '), ('./static/2.mp4', 2480, u'a lot of continuous text, right? '), ('./static/2.mp4', 2482, u'Actual sentences because we want to learn wo- word meaning context. '), ('./static/2.mp4', 2486, u'Um, NLP people call a large pile of text a corpus. '), ('./static/2.mp4', 2492, u"And I mean that's just the Latin word for body, right? "), ('./static/2.mp4', 2495, u"It's a body of text. "), ('./static/2.mp4', 2497, u'Important things to note if you want to seem really educated is in Latin, '), ('./static/2.mp4', 2503, u'this is a fourth declensions noun. '), ('./static/2.mp4', 2506, u'So the plural of corpus is corpora. '), ('./static/2.mp4', 2509, u'And whereas if you say '), ('./static/2.mp4', 2511, u"core Pi everyone will know that you didn't study Latin in high school. "), ('./static/2.mp4', 2515, u'[LAUGHTER] Um, okay. '), ('./static/2.mp4', 2520, u'Um, so right- so we then want to say that every word um '), ('./static/2.mp4', 2526, u'in a- in a fixed vocabulary which would just be '), ('./static/2.mp4', 2528, u'the vocabulary the corpus is um represented by a vector. '), ('./static/2.mp4', 2532, u'And we just start those vectors off as random vectors. '), ('./static/2.mp4', 2536, u"And so then what we're going to do is do "), ('./static/2.mp4', 2538, u'this big iterative algorithm where we go through each position in the text. '), ('./static/2.mp4', 2542, u"We say, here's a word in the text. "), ('./static/2.mp4', 2544, u"Let's look at the words around it and what we're going to want to do is say well, "), ('./static/2.mp4', 2550, u'the meaning of a word is its contexts of use. '), ('./static/2.mp4', 2552, u'So we want the representation of the word '), ('./static/2.mp4', 2555, u'in the middle to be able to predict the words that are '), ('./static/2.mp4', 2557, u"around it and so we're gonna achieve that by moving the position of the word vector. "), ('./static/2.mp4', 2563, u'And we just repeat that a billion times and '), ('./static/2.mp4', 2567, u'somehow a miracle occurs and outcomes at the end we have '), ('./static/2.mp4', 2571, u'a word vector space that looks like a picture I showed where it has '), ('./static/2.mp4', 2574, u'a good meaning of word meet good representation of word meaning. '), ('./static/2.mp4', 2579, u'So slightly more, um, '), ('./static/2.mp4', 2583, u'um, slightly more um graphically right. '), ('./static/2.mp4', 2587, u"So here's the situation. "), ('./static/2.mp4', 2588, u"So we've got part of our corpus problems turning into banking crisis, "), ('./static/2.mp4', 2592, u'and so what we want to say is well, '), ('./static/2.mp4', 2594, u"we want to know the meaning of the word into and so we're going to hope that "), ('./static/2.mp4', 2597, u"its representation can be used in a way that'll "), ('./static/2.mp4', 2601, u'make precise to predict what words appear in '), ('./static/2.mp4', 2604, u"the context of into because that's the meaning of into. "), ('./static/2.mp4', 2608, u"And so we're going to try and make those predictions, "), ('./static/2.mp4', 2611, u'see how well we can predict and then change '), ('./static/2.mp4', 2614, u'the vector representations of words in a way that we can do that prediction better. '), ('./static/2.mp4', 2619, u"And then once we've dealt with into, "), ('./static/2.mp4', 2621, u'we just go onto the next word and we say, '), ('./static/2.mp4', 2623, u"okay, let's take banking as the word. "), ('./static/2.mp4', 2626, u'The meaning of banking is predicting the contexts in which banking occurs. '), ('./static/2.mp4', 2629, u"Here's one context. "), ('./static/2.mp4', 2631, u"Let's try and predict these words that occur around banking and "), ('./static/2.mp4', 2634, u"see how we do and then we'll move on again from there. "), ('./static/2.mp4', 2638, u'Okay. Um, sounds easy so far. '), ('./static/2.mp4', 2642, u'Um, [NOISE] now we go on and sort of do a bit more stuff. '), ('./static/2.mp4', 2646, u'Okay. So overall, we have a big long corpus of capital T words. '), ('./static/2.mp4', 2652, u'So if we have a whole lot of documents we just concatenate them all together and we say, '), ('./static/2.mp4', 2657, u"okay, here's a billion words, "), ('./static/2.mp4', 2659, u'and so big long list of words. '), ('./static/2.mp4', 2661, u"And so what we're gonna do, "), ('./static/2.mp4', 2663, u"is for the first um product we're going to sort of "), ('./static/2.mp4', 2666, u'go through all the words and then for the second product, '), ('./static/2.mp4', 2670, u"we're gonna say- we're gonna choose some fixed size window, you know, "), ('./static/2.mp4', 2674, u"it might be five words on each side or something and we're going to try and "), ('./static/2.mp4', 2677, u'predict the 10 words that are around that center word. '), ('./static/2.mp4', 2682, u"And we're going to predict in the sense of trying to "), ('./static/2.mp4', 2684, u'predict that word given the center word. '), ('./static/2.mp4', 2686, u"That's our probability model. "), ('./static/2.mp4', 2688, u'And so if we multiply all those things together, '), ('./static/2.mp4', 2691, u"that's our model likelihood is how good a job it "), ('./static/2.mp4', 2694, u'does at predicting the words around every word. '), ('./static/2.mp4', 2698, u'And that model likelihood is going to depend '), ('./static/2.mp4', 2701, u'on the parameters of our model which we write as theta. '), ('./static/2.mp4', 2705, u'And in this particular model, '), ('./static/2.mp4', 2707, u'the only parameters in it is actually '), ('./static/2.mp4', 2710, u'going to be the vector representations we give the words. '), ('./static/2.mp4', 2713, u'The model has absolutely no other parameters to it. '), ('./static/2.mp4', 2716, u"So, we're just going to say we're representing "), ('./static/2.mp4', 2720, u'a word with a vector in a vector space and that '), ('./static/2.mp4', 2723, u"representation of it is its meaning and we're then going to be able to "), ('./static/2.mp4', 2727, u"use that to predict what other words occur in a way I'm about to show you. "), ('./static/2.mp4', 2732, u"Okay. So, um, that's our likelihood and so what we do in all of "), ('./static/2.mp4', 2737, u"these models is we sort of define an objective function and then we're going to be, "), ('./static/2.mp4', 2742, u'I want to come up with vector representations of words in '), ('./static/2.mp4', 2745, u'such a way as to minimize our objective function. '), ('./static/2.mp4', 2750, u"Um, so objective function is basically the same as what's on the top half of the slide, "), ('./static/2.mp4', 2756, u'but we change a couple of things. '), ('./static/2.mp4', 2758, u'We stick a minus sign in front of it so we can do minimization rather than maximization. '), ('./static/2.mp4', 2763, u'Completely arbitrary makes no difference. '), ('./static/2.mp4', 2765, u'Um, we stick a one and T in front of it, '), ('./static/2.mp4', 2768, u"so that we're working out the sort of average "), ('./static/2.mp4', 2771, u'as of a goodness of predicting for each choice of center word. '), ('./static/2.mp4', 2776, u'Again, that sort of makes no difference but it kinda keeps the scale of '), ('./static/2.mp4', 2779, u'things ah not dependent on the size of the corpus. '), ('./static/2.mp4', 2783, u"Um, the bit that's actually important is we stick a log in front of "), ('./static/2.mp4', 2787, u'the function that was up there um because it turns out that everything always gets nice. '), ('./static/2.mp4', 2791, u'So when you stick logs and find the products '), ('./static/2.mp4', 2793, u"um when you're doing things like optimization. "), ('./static/2.mp4', 2796, u'So, when we do that we then got a log of '), ('./static/2.mp4', 2798, u'all these products which will allow us to turn things you know, '), ('./static/2.mp4', 2802, u'into a sums of the log of this probability '), ('./static/2.mp4', 2806, u"and we'll go through that again um in just a minute. "), ('./static/2.mp4', 2810, u'Okay. Um, and so if we can mi- if we can change '), ('./static/2.mp4', 2815, u'our vector representations of these words so as to minimize this J of theta, '), ('./static/2.mp4', 2820, u"that means we'll be good at predicting words in the context of another word. "), ('./static/2.mp4', 2826, u'So then, that all sounded good but it was all '), ('./static/2.mp4', 2830, u'dependent on having this probability function where you wanna '), ('./static/2.mp4', 2833, u'predict the probability of a word in '), ('./static/2.mp4', 2837, u'the context given the center word and the question is, '), ('./static/2.mp4', 2840, u'how can you possibly do that? '), ('./static/2.mp4', 2843, u'Um, well um, remember what I said is actually our model is just gonna '), ('./static/2.mp4', 2848, u'have vector representations of words and that was the only parameters of the model. '), ('./static/2.mp4', 2853, u"Now, that's, that's almost true. "), ('./static/2.mp4', 2855, u"It's not quite true. "), ('./static/2.mp4', 2857, u'Um, we actually cheat slightly. '), ('./static/2.mp4', 2859, u'Since we actually propose two vector representations for '), ('./static/2.mp4', 2862, u'each word and this makes it simpler to do this. '), ('./static/2.mp4', 2866, u'Um, you cannot do this, '), ('./static/2.mp4', 2868, u'there are ways to get around it but this is the simplest way to do it. '), ('./static/2.mp4', 2870, u"So we have one vector for word when it's the center word that's predicting "), ('./static/2.mp4', 2874, u"other words but we have a second vector for each word when it's a context word, "), ('./static/2.mp4', 2879, u"so that's one of the words in context. "), ('./static/2.mp4', 2881, u'So for each word type, '), ('./static/2.mp4', 2882, u'we have these two vectors as center word, as context word. '), ('./static/2.mp4', 2886, u"Um, so then we're gonna work out this probability of a word in the context, "), ('./static/2.mp4', 2892, u'given the center word, '), ('./static/2.mp4', 2894, u'purely in terms of these vectors and the way we do it is with this equation right here, '), ('./static/2.mp4', 2902, u"which I'll explain more in just a moment. "), ('./static/2.mp4', 2905, u"So we're still on exactly the same situation, right? "), ('./static/2.mp4', 2909, u"That we're wanting to work out probabilities of "), ('./static/2.mp4', 2912, u'words occurring in the context of our center word. '), ('./static/2.mp4', 2915, u'So the center word is C and the context words represented with '), ('./static/2.mp4', 2918, u'O and these [inaudible] slide notation but sort of, '), ('./static/2.mp4', 2922, u"we're basically saying there's one kind of "), ('./static/2.mp4', 2924, u'vector for center words is a different kind of vector '), ('./static/2.mp4', 2927, u"for context words and we're gonna work out this probabilistic prediction um, "), ('./static/2.mp4', 2933, u'in terms of these word vectors. '), ('./static/2.mp4', 2936, u'Okay. So how can we do that? '), ('./static/2.mp4', 2939, u'Well, the way we do it is with this um, '), ('./static/2.mp4', 2942, u'formula here which is the sort of shape that you see over and over again um, '), ('./static/2.mp4', 2947, u'in deep learning with categorical staff. '), ('./static/2.mp4', 2950, u'So for the very center bit of it, '), ('./static/2.mp4', 2952, u'the bit in orange are more the same thing occurs in the um, denominator. '), ('./static/2.mp4', 2957, u"What we're doing there is calculating a dot product. "), ('./static/2.mp4', 2961, u"So, we're gonna go through the components of our vector and we're gonna "), ('./static/2.mp4', 2964, u'multiply them together and that means if um, '), ('./static/2.mp4', 2968, u'different words have B components of the same sign, '), ('./static/2.mp4', 2972, u'plus or minus, in the same positions, '), ('./static/2.mp4', 2975, u'the dot product will be big and if '), ('./static/2.mp4', 2978, u'they have different signs or one is big and one is small, '), ('./static/2.mp4', 2982, u'the dot product will be a lot smaller. '), ('./static/2.mp4', 2984, u'So that orange part directly calculates uh, '), ('./static/2.mp4', 2988, u'sort of a similarity between words where '), ('./static/2.mp4', 2991, u'the similarity is the sort of vectors looking the same, right? '), ('./static/2.mp4', 2995, u"Um, and so that's the heart of it, right? "), ('./static/2.mp4', 2997, u"So we're gonna have words that have similar vectors, "), ('./static/2.mp4', 3000, u'IS close together in the vector space have similar meaning. '), ('./static/2.mp4', 3004, u'Um, so for the rest of it- um, '), ('./static/2.mp4', 3006, u'so the next thing we do is take that number and put an X around it. '), ('./static/2.mp4', 3010, u'So, um, the exponential has '), ('./static/2.mp4', 3012, u'this nice property that no matter what number you stick into it, '), ('./static/2.mp4', 3015, u'because the dot product might be positive or negative, '), ('./static/2.mp4', 3017, u"it's gonna come out as a positive number and if "), ('./static/2.mp4', 3020, u"we eventually wanna get a probability, um, that's really good. "), ('./static/2.mp4', 3024, u"If we have positive numbers and not negative numbers, um, so that's good. "), ('./static/2.mp4', 3028, u'Um, then the third part of which is the bid in blue is we wanted to have '), ('./static/2.mp4', 3033, u'probabilities and probabilities are meant to add up to '), ('./static/2.mp4', 3036, u'one and so we do that in the standard, dumbest possible way. '), ('./static/2.mp4', 3039, u'We sum up what this quantity is, '), ('./static/2.mp4', 3042, u'that every different word in our vocabulary and we divide through by '), ('./static/2.mp4', 3047, u'it and so that normalizes things and turns them into a probability distribution. '), ('./static/2.mp4', 3052, u"Yeah, so there's sort of in practice, "), ('./static/2.mp4', 3054, u'there are two parts. '), ('./static/2.mp4', 3055, u"There's the orange part which is this idea of using "), ('./static/2.mp4', 3059, u'dot product and a vector space as our similarity measure between words '), ('./static/2.mp4', 3063, u'and then the second part is all the rest of it where we feed it '), ('./static/2.mp4', 3067, u'through what we refer to a news all the time as a softmax distribution. '), ('./static/2.mp4', 3071, u'So the two parts of the expen normalizing gives you a softmax distribution. '), ('./static/2.mp4', 3077, u'Um, and softmax functions will sort of map any numbers into '), ('./static/2.mp4', 3082, u'a probability distribution always for the two reasons that I gave and so, '), ('./static/2.mp4', 3086, u"it's referred to as a softmax um, "), ('./static/2.mp4', 3090, u'because it works like a softmax, right? '), ('./static/2.mp4', 3093, u'So if you have numbers, '), ('./static/2.mp4', 3095, u"you could just say what's the max of these numbers, um, "), ('./static/2.mp4', 3099, u"and you know that's sort of a hot- if you sort of map your original numbers into, "), ('./static/2.mp4', 3106, u"if it's the max of the max and everything else is zero, "), ('./static/2.mp4', 3109, u"that's sort of a hard max. "), ('./static/2.mp4', 3111, u'Um, soft- this is a softmax because the exponenti- you know, '), ('./static/2.mp4', 3116, u'if you sort of imagine this but- if we just ignore the problem '), ('./static/2.mp4', 3120, u'negative numbers for a moment and you got rid of the exp, um, '), ('./static/2.mp4', 3124, u"then you'd sort of coming out with "), ('./static/2.mp4', 3126, u"a probability distribution but by and large it's so be fairly "), ('./static/2.mp4', 3129, u"flat and wouldn't particularly pick out the max of "), ('./static/2.mp4', 3132, u'the different XI numbers whereas when you exponentiate them, '), ('./static/2.mp4', 3135, u'that sort of makes big numbers way bigger and so, this, '), ('./static/2.mp4', 3138, u"this softmax sort of mainly puts mass where the max's or the couple of max's are. "), ('./static/2.mp4', 3145, u"Um, so that's the max part and a soft part is that this isn't "), ('./static/2.mp4', 3149, u'a hard decisions still spreads a little bit of probability mass everywhere else. '), ('./static/2.mp4', 3154, u'Okay, so now we have uh, loss function. '), ('./static/2.mp4', 3160, u'We have a loss function with a probability model on the inside that we can '), ('./static/2.mp4', 3165, u'build and so what we want to be able to do is then um, '), ('./static/2.mp4', 3170, u'move our vector representations of words around '), ('./static/2.mp4', 3175, u'so that they are good at predicting what words occur in the context of other words. '), ('./static/2.mp4', 3181, u"Um, and so, at this point what we're gonna do is optimization. "), ('./static/2.mp4', 3186, u'So, we have vector components of different words. '), ('./static/2.mp4', 3190, u'We have a very high-dimensional space again but here, '), ('./static/2.mp4', 3193, u"I've just got two for the picture and we're gonna wanna "), ('./static/2.mp4', 3196, u"say how- how can we minimize this function and we're going to "), ('./static/2.mp4', 3199, u'want to jiggle the numbers that are used in the word representations in '), ('./static/2.mp4', 3203, u"such a way that we're walking down the slope of this space. "), ('./static/2.mp4', 3208, u'I walking down the gradient and um, '), ('./static/2.mp4', 3212, u"then we're gonna minimize the function we found good representations for words. "), ('./static/2.mp4', 3217, u'So doing this for this case, '), ('./static/2.mp4', 3219, u'we want to make a very big vector in '), ('./static/2.mp4', 3222, u'a very high-dimensional vector space of all the parameters of '), ('./static/2.mp4', 3225, u'our model and the only parameters that this model '), ('./static/2.mp4', 3228, u'has is literally the vector space representations of words. '), ('./static/2.mp4', 3233, u'So if there are a 100 dimensional word representations, '), ('./static/2.mp4', 3236, u"they're sort of a 100 parameters for aardvark and context, "), ('./static/2.mp4', 3239, u'100 parameters for the word a- in context et cetera going through, '), ('./static/2.mp4', 3243, u'100 parameters for the word aardvark [NOISE] as a center word et cetera, '), ('./static/2.mp4', 3248, u'et cetera through that gives us a high big vector of parameters to '), ('./static/2.mp4', 3252, u"optimize and we're gonna run this optimization and then um, move them down. "), ('./static/2.mp4', 3258, u"Um, [NOISE] yeah so that's essentially what you do. "), ('./static/2.mp4', 3263, u'Um, I sort of wanted to go through um, '), ('./static/2.mp4', 3266, u'the details of this um, '), ('./static/2.mp4', 3268, u"just so we've kind of gone through things concretely to "), ('./static/2.mp4', 3272, u'make sure everyone is on the same page. '), ('./static/2.mp4', 3276, u'Um, so I suspect that, you know, '), ('./static/2.mp4', 3279, u'if I try and do this concretely, '), ('./static/2.mp4', 3283, u'um, there are a lot of people um, '), ('./static/2.mp4', 3285, u'that this will bore and some people that are- will bore very badly, '), ('./static/2.mp4', 3290, u'um, so I apologize for you, '), ('./static/2.mp4', 3294, u'um, but you know, '), ('./static/2.mp4', 3295, u"I'm hoping and thinking that there's probably "), ('./static/2.mp4', 3299, u"some people who haven't done as much of this stuff recently "), ('./static/2.mp4', 3302, u'and it might just actually be good to do it concretely '), ('./static/2.mp4', 3305, u'and get everyone up to speed right at the beginning. Yeah? '), ('./static/2.mp4', 3309, u'[inaudible] how do we calculate [inaudible] specifically? '), ('./static/2.mp4', 3314, u'Well, so, we- so the way we calculate the, '), ('./static/2.mp4', 3320, u"the U and V vectors is we're literally going to start with a random vector for "), ('./static/2.mp4', 3326, u'each word and then we iteratively going to change those vectors a little bit as we learn. '), ('./static/2.mp4', 3333, u"And the way we're going to work out how to change them is we're gonna say, "), ('./static/2.mp4', 3337, u'"I want to do optimization," and that is going to be implemented as okay. '), ('./static/2.mp4', 3342, u'We have the current vectors for each word. '), ('./static/2.mp4', 3344, u'Let me do some calculus to work out how I could change the word vectors, um, to mean, '), ('./static/2.mp4', 3351, u'that the word vectors would calculate a higher probability for '), ('./static/2.mp4', 3355, u'the words that actually occur in contexts of this center word. '), ('./static/2.mp4', 3360, u'And we will do that, '), ('./static/2.mp4', 3361, u"and we'll do it again and again and again, "), ('./static/2.mp4', 3363, u'and then will eventually end up with good word vectors. '), ('./static/2.mp4', 3366, u'Thank you for that question, '), ('./static/2.mp4', 3368, u"cause that's a concept that you're meant to have understood. "), ('./static/2.mp4', 3370, u"Is that how this works and maybe I didn't "), ('./static/2.mp4', 3373, u'explain that high-level recipe well enough, yeah. '), ('./static/2.mp4', 3376, u"Okay, so yeah, so let's just go through it. So, we've seen it, right? "), ('./static/2.mp4', 3380, u'So, we had this formula that we wanted to maximize, you know, '), ('./static/2.mp4', 3384, u'our original function which was the product of T equals one to T, '), ('./static/2.mp4', 3392, u'and then the product of the words, uh, '), ('./static/2.mp4', 3395, u'position minus M less than or equal to J, '), ('./static/2.mp4', 3400, u'less than or equal to M, '), ('./static/2.mp4', 3402, u'J not equal to zero of, um, '), ('./static/2.mp4', 3406, u'the probability of W. At prime at T '), ('./static/2.mp4', 3411, u'plus J given WT according to the parameters of our model. '), ('./static/2.mp4', 3417, u"Okay, and then we'd already seen that we were gonna convert that "), ('./static/2.mp4', 3421, u"into the function that we're going to use where we have J of Theta, "), ('./static/2.mp4', 3425, u'where we had the minus one on T. Of the sum of T equals one to T of the sum of minus M, '), ('./static/2.mp4', 3435, u'less than or equal to J less than or equal to M, '), ('./static/2.mp4', 3437, u'J not equal to zero of the log of the probability of W times T, plus J, W, '), ('./static/2.mp4', 3447, u"T. Okay, so we had that and then we'd had "), ('./static/2.mp4', 3451, u'this formula that the probability of the outside word given '), ('./static/2.mp4', 3456, u'the context word is this formula we just went through of xu ot vc over '), ('./static/2.mp4', 3466, u'the sum of W equals one to the vocabulary size of xu wt vc. '), ('./static/2.mp4', 3476, u"Okay, so that's sort of our model. "), ('./static/2.mp4', 3479, u'We want to min- minimize this. '), ('./static/2.mp4', 3483, u'So, we wanna minimize this and we want to minimize that by changing these parameters. '), ('./static/2.mp4', 3491, u'And these parameters are the contents of these vectors. '), ('./static/2.mp4', 3495, u'And so, what we want to do now, '), ('./static/2.mp4', 3497, u"is do calculus and we wanna say let's work out in terms of these parameters which are, "), ('./static/2.mp4', 3503, u'u and v vectors, um, '), ('./static/2.mp4', 3505, u'for the current values of the parameters which we initialized randomly. '), ('./static/2.mp4', 3510, u"Like what's the slope of the space? "), ('./static/2.mp4', 3512, u'Where is downhill? '), ('./static/2.mp4', 3513, u'Because if we can work out downhill is, '), ('./static/2.mp4', 3515, u'we got just gotta walk downhill and our model gets better. '), ('./static/2.mp4', 3519, u"So, we're gonna take derivatives and work out what "), ('./static/2.mp4', 3522, u'direction downhill is and then we wanna walk that way, yeah. '), ('./static/2.mp4', 3525, u'So, why do we wanna maximize that probable edge and like, '), ('./static/2.mp4', 3530, u'like going through every word, '), ('./static/2.mp4', 3531, u"it's like [inaudible] given the [inaudible] "), ('./static/2.mp4', 3537, u'So, well, so, so, '), ('./static/2.mp4', 3539, u"I'm wanting to achieve this, um, "), ('./static/2.mp4', 3542, u'what I want to achieve for my distributional notion of meaning is, '), ('./static/2.mp4', 3548, u'I have a meaningful word, a vector. '), ('./static/2.mp4', 3551, u'And that vector knows what words occur in the context of, '), ('./static/2.mp4', 3556, u'um, a word- of itself. '), ('./static/2.mp4', 3559, u'And knowing what words occur in its context means, '), ('./static/2.mp4', 3563, u'it can accurately give '), ('./static/2.mp4', 3564, u'a high probability estimate to those words that occur in the context, '), ('./static/2.mp4', 3568, u'and it will give low probability estimates '), ('./static/2.mp4', 3572, u"to words that don't typically occur in the context. "), ('./static/2.mp4', 3575, u'So, you know, if the word is bank, '), ('./static/2.mp4', 3577, u"I'm hoping that words like branch, "), ('./static/2.mp4', 3579, u'and open, and withdrawal, '), ('./static/2.mp4', 3581, u'will be given high probability, '), ('./static/2.mp4', 3583, u'cause they tend to occur with the word bank. '), ('./static/2.mp4', 3585, u"And I'm hoping that some other words, um, "), ('./static/2.mp4', 3589, u'like neural network or something have '), ('./static/2.mp4', 3592, u"a lower probability because they don't tend to occur with the word bank. "), ('./static/2.mp4', 3598, u'Okay, um, does that make sense? '), ('./static/2.mp4', 3601, u'Yeah. '), ('./static/2.mp4', 3601, u'Yeah. And the other thing I was, '), ('./static/2.mp4', 3603, u"I'd forgotten meant to comment was, you know, obviously, "), ('./static/2.mp4', 3606, u"we're not gonna be able to do this super well or it's just not gonna be able, "), ('./static/2.mp4', 3610, u'that we can say all the words in the context is going to '), ('./static/2.mp4', 3613, u'be this word with probability 0.97, right? '), ('./static/2.mp4', 3615, u"Because we're using this one simple probability distribution "), ('./static/2.mp4', 3619, u'to predict all words in our context. '), ('./static/2.mp4', 3623, u"So, in particular, we're using it to predict 10 different words generally, right? "), ('./static/2.mp4', 3627, u'So, at best, we can kind of be giving sort of five percent chance to one of them, right? '), ('./static/2.mp4', 3632, u"We can't possibly be, "), ('./static/2.mp4', 3633, u'so guessing right every time. '), ('./static/2.mp4', 3635, u'Um, and well, you know, '), ('./static/2.mp4', 3637, u"they're gonna be different contexts with different words in them. "), ('./static/2.mp4', 3640, u"So, you know, it's gonna be a very loose model, "), ('./static/2.mp4', 3644, u'but nevertheless, we wanna capture the fact that, you know, '), ('./static/2.mp4', 3648, u'withdrawal is much more likely, um, '), ('./static/2.mp4', 3651, u'to occur near the word bank than something like football. '), ('./static/2.mp4', 3657, u"That's, you know, basically what our goal is. "), ('./static/2.mp4', 3661, u'Okay, um, yes, so we want to maximize this, '), ('./static/2.mp4', 3667, u'by minimizing this, which means we then want to do some calculus to work this out. '), ('./static/2.mp4', 3672, u"So, what we're then gonna do is, "), ('./static/2.mp4', 3674, u"that we're going to say, well, "), ('./static/2.mp4', 3676, u'these parameters are our word vectors '), ('./static/2.mp4', 3679, u"and we're gonna sort of want to move these word vectors, "), ('./static/2.mp4', 3682, u'um, to, um, work things out as to how to, um, walk downhill. '), ('./static/2.mp4', 3688, u"So, the case that I'm going to do now is gonna look at the parameters of "), ('./static/2.mp4', 3692, u'this center word vc and work out how to do things with respect to it. '), ('./static/2.mp4', 3698, u"Um, now, that's not the only thing that you wanna do, "), ('./static/2.mp4', 3700, u'you also want to work out the slope with respect to the uo vector. '), ('./static/2.mp4', 3704, u"Um, but I'm not gonna do that because time in class is going to run out. "), ('./static/2.mp4', 3707, u"So, it'd be really good if you did that one at "), ('./static/2.mp4', 3709, u"home and then you'd feel much more competent. "), ('./static/2.mp4', 3711, u"Right, so then, um, so what I'm wanting you to do is work out the partial derivative with "), ('./static/2.mp4', 3717, u'respect to my vc vector representation of this quantity, '), ('./static/2.mp4', 3723, u'that we were just looking at. '), ('./static/2.mp4', 3724, u'Which is, um, the quantity in here, '), ('./static/2.mp4', 3728, u"um, where we're taking the log of that quantity. "), ('./static/2.mp4', 3731, u'Right, the log of the x of u, '), ('./static/2.mp4', 3737, u'o, T, v, c, '), ('./static/2.mp4', 3740, u'over the sum of W equals one to V of the x of u, '), ('./static/2.mp4', 3746, u'o, T, v, c. Okay, '), ('./static/2.mp4', 3750, u'so this, um, so now we have a log of the division, '), ('./static/2.mp4', 3753, u"so that's easy to rewrite, um, "), ('./static/2.mp4', 3755, u'that we have a partial derivative of the log of '), ('./static/2.mp4', 3759, u'the numerator minus and '), ('./static/2.mp4', 3767, u'I can distribute the partial derivative. '), ('./static/2.mp4', 3769, u'So, I can have minus the partial derivative, '), ('./static/2.mp4', 3773, u'um, of the denominator, '), ('./static/2.mp4', 3776, u'um, which is log of this thing. '), ('./static/2.mp4', 3779, u'[NOISE] '), ('./static/2.mp4', 3788, u'Okay. Um, so this is sort of what was the numerator and this is what was the denominator. '), ('./static/2.mp4', 3799, u'Okay. So, um, the part that was the numerator is really easy. '), ('./static/2.mp4', 3807, u'In fact maybe I can fit it in here. '), ('./static/2.mp4', 3809, u'Um, so log on exp are just inverses of each other, '), ('./static/2.mp4', 3813, u'so they cancel out. '), ('./static/2.mp4', 3814, u"So, we've got the partial derivative of U_o T V_c. "), ('./static/2.mp4', 3823, u'Okay, so this point I should, um, just, um, '), ('./static/2.mp4', 3827, u"remind people right that this V_c here's a vector of- um, "), ('./static/2.mp4', 3831, u"it's still a vector right because we had a 100 dimensional representation of a word. "), ('./static/2.mp4', 3836, u'Um, so this is doing multivariate calculus. '), ('./static/2.mp4', 3840, u"Um, so you know, if you're, "), ('./static/2.mp4', 3842, u'if you at all, um, '), ('./static/2.mp4', 3844, u'remember any of this stuff, '), ('./static/2.mp4', 3846, u'you can say, "Ha this is trivial". '), ('./static/2.mp4', 3848, u"The answer to that is you are done, um and that's great. "), ('./static/2.mp4', 3852, u"But you know, if you're, um, feeling, um, "), ('./static/2.mp4', 3854, u'not so good on all of this stuff, um, '), ('./static/2.mp4', 3857, u'and you wanna sort of, um, '), ('./static/2.mp4', 3859, u'cheat a little on the side and try and work out what it is, '), ('./static/2.mp4', 3862, u'um, you can sort of say, '), ('./static/2.mp4', 3864, u'"Well, let me um,, '), ('./static/2.mp4', 3865, u'work out the partial derivative, '), ('./static/2.mp4', 3868, u'um with respect to one element of this vector like the first element of this vector". '), ('./static/2.mp4', 3874, u'Well, what I actually got here for this dot product is I have U_o one times V_c one, '), ('./static/2.mp4', 3882, u'plus U_o two times V_c two plus dot, dot, '), ('./static/2.mp4', 3889, u'dot plus U_o 100 times V_c 100, right, '), ('./static/2.mp4', 3896, u"and I'm finding the partial derivative of this with respect to V_c one, "), ('./static/2.mp4', 3902, u'and hopefully remember that much calculus from high school '), ('./static/2.mp4', 3905, u'of none of these terms involve V_c one. '), ('./static/2.mp4', 3909, u"So, the only thing that's left is this U_o one, "), ('./static/2.mp4', 3912, u"and that's what I've got there for this dimension. "), ('./static/2.mp4', 3915, u'So, this particular parameter. '), ('./static/2.mp4', 3917, u"But I don't only want to do the first component of the V_c vector, "), ('./static/2.mp4', 3923, u'I also want to do the second component of the V_c vector et cetera, '), ('./static/2.mp4', 3926, u"which means I'm going to end up with all of them "), ('./static/2.mp4', 3930, u'turning up in precisely one of these things. '), ('./static/2.mp4', 3935, u'Um, and so the end result is I get the vector U_o. '), ('./static/2.mp4', 3941, u'Okay. Um, but you know, '), ('./static/2.mp4', 3943, u"if you're sort of getting confused and your brain is falling apart, "), ('./static/2.mp4', 3947, u'I think it can be sort of kind of useful to re- reduce things to sort of um, '), ('./static/2.mp4', 3952, u"single dimensional calculus and actually sort of play out what's actually happening. "), ('./static/2.mp4', 3958, u'Um, anyway, this part was easy. '), ('./static/2.mp4', 3960, u'The numerator, we get um, U_o. '), ('./static/2.mp4', 3963, u"Um, so things aren't quite so nice when we do the denominator. "), ('./static/2.mp4', 3968, u'So we now want to have this, um, B_d, '), ('./static/2.mp4', 3971, u'V_c of the log of the sum of W equals '), ('./static/2.mp4', 3977, u'one to the P_x of U_o T V_c. '), ('./static/2.mp4', 3982, u'Okay. So, now at this point, '), ('./static/2.mp4', 3985, u"I'm not quite so pretty. "), ('./static/2.mp4', 3987, u"We've got this log sum X combination that you see a lot, "), ('./static/2.mp4', 3991, u'and so at this point you have to remember that there was E, the chain rule. '), ('./static/2.mp4', 3995, u"Okay. So, what we can say is here's you know, "), ('./static/2.mp4', 3998, u'our function F and here is the body of the function, '), ('./static/2.mp4', 4002, u'and so what we want to do is um, '), ('./static/2.mp4', 4006, u'do it in two stages. '), ('./static/2.mp4', 4008, u'Um, so that at the end of the day, '), ('./static/2.mp4', 4011, u"we've got this V_c at the end. "), ('./static/2.mp4', 4013, u'So, we have sort of some function here. '), ('./static/2.mp4', 4017, u"There's ultimately a function of V_c, "), ('./static/2.mp4', 4019, u'and so we gonna do with a chain rule. '), ('./static/2.mp4', 4022, u"We'll say the chain rule is we first take "), ('./static/2.mp4', 4025, u'the derivative of this outside thing putting in this body, '), ('./static/2.mp4', 4029, u'and then we remember that the derivative of log is one on X. '), ('./static/2.mp4', 4033, u'So, we have one over the sum of W equals one to V of the exp of U_o T V_c '), ('./static/2.mp4', 4042, u'and then we need to multiply that by then taking '), ('./static/2.mp4', 4046, u'the derivative of the inside part which is um, '), ('./static/2.mp4', 4052, u'what we have here. '), ('./static/2.mp4', 4060, u'Okay. Times the derivative of the inside part with '), ('./static/2.mp4', 4064, u'the important reminder that you need to do a change of variables, '), ('./static/2.mp4', 4068, u"and for the inside part use a different variable that you're summing over. "), ('./static/2.mp4', 4073, u"Okay. So, now we're trying to find the derivative of a sum of X. "), ('./static/2.mp4', 4080, u'The first thing that we can do is v-very easy. '), ('./static/2.mp4', 4085, u'We can move the derivative inside a sum. '), ('./static/2.mp4', 4088, u'So, we can rewrite that and have at the sum first of the X equals one to '), ('./static/2.mp4', 4094, u'V of the partial derivatives with respect to V_c of the [inaudible]. '), ('./static/2.mp4', 4100, u"Um, so that's a little bit of progress. "), ('./static/2.mp4', 4102, u'Um and that point we have to sort of do the chain rule again, right. '), ('./static/2.mp4', 4106, u"So, here is our function and here's the thing in it again which is some function of V_c. "), ('./static/2.mp4', 4113, u'So, we again want to do um, the chain rule. '), ('./static/2.mp4', 4117, u'So, [NOISE] we then have well, '), ('./static/2.mp4', 4121, u'the derivative of X um, is exp. '), ('./static/2.mp4', 4125, u'So, we gonna have the sum of X equals one to V of exp of U_x T V_c, '), ('./static/2.mp4', 4134, u"and then we're going to multiply that by the partial derivative with "), ('./static/2.mp4', 4140, u'respect to T V_c of the inside U_x T V_c. '), ('./static/2.mp4', 4145, u'Well, we saw that one before, so, '), ('./static/2.mp4', 4148, u'the derivative of that is U- well, '), ('./static/2.mp4', 4153, u"yeah, U_x because we're doing it through a different X, right. "), ('./static/2.mp4', 4156, u'This then becomes out as U_x, '), ('./static/2.mp4', 4158, u'and so we have the sum of the X equals one to '), ('./static/2.mp4', 4163, u'V of this exp U X T B C times the U_of X. '), ('./static/2.mp4', 4170, u"Okay. So, by doing the chain rule twice, we've got that. "), ('./static/2.mp4', 4174, u'So, now if we put it together, you know, '), ('./static/2.mp4', 4178, u'the derivative of V_c with respect of the whole thing, '), ('./static/2.mp4', 4183, u'this log of the probability of O given C, right. '), ('./static/2.mp4', 4186, u'That for the numerator it was just U_o, '), ('./static/2.mp4', 4191, u"and then we're subtracting, "), ('./static/2.mp4', 4194, u'we had this term here, um, '), ('./static/2.mp4', 4197, u'which is sort of a denominator, '), ('./static/2.mp4', 4199, u'and then we have this term here which is the numerator. '), ('./static/2.mp4', 4203, u"So, we're subtracting in the numerator, "), ('./static/2.mp4', 4207, u'we have the sum of X equals one to V of '), ('./static/2.mp4', 4212, u'the exp of U_x T V_c times U_x, '), ('./static/2.mp4', 4218, u'and then in the denominator, we have um, '), ('./static/2.mp4', 4225, u'the sum of W equals one to V of exp of U_w T V_c. '), ('./static/2.mp4', 4236, u'Um, okay, so we kind of get that. '), ('./static/2.mp4', 4240, u"Um, oh wait. Yeah. Yeah, I've gotten. "), ('./static/2.mp4', 4244, u"Yeah, that's right. Um, okay. "), ('./static/2.mp4', 4245, u'We kind of get that and then we can sort of just re-arrange this a little. '), ('./static/2.mp4', 4252, u'So, we can have this sum right out front, '), ('./static/2.mp4', 4256, u'and we can say that this is sort of a big sum of X equals one to V, '), ('./static/2.mp4', 4263, u'and we can sort of take that U_x out the end and say, okay. '), ('./static/2.mp4', 4269, u"Let's call that put over here a U_x, "), ('./static/2.mp4', 4273, u'and if we do that, '), ('./static/2.mp4', 4275, u'sort of an interesting thing has happened because look right here, '), ('./static/2.mp4', 4280, u"we've rediscovered exactly the same form "), ('./static/2.mp4', 4285, u'that we use as our probability distribution for predicting the probability of words. '), ('./static/2.mp4', 4291, u'So, this is now simply the probability of X given C according to our model. '), ('./static/2.mp4', 4297, u"Um, so we can rewrite this and say that what we're getting is U_o minus the sum of "), ('./static/2.mp4', 4306, u'X equals one to V of the probability of X given C times U_x. '), ('./static/2.mp4', 4314, u'This has a kind of an interesting meaning if you think about it. '), ('./static/2.mp4', 4318, u'So, this is actually giving us, you know, '), ('./static/2.mp4', 4321, u'our slope in this multi-dimensional space '), ('./static/2.mp4', 4324, u"and how we're getting that slope is we're taking "), ('./static/2.mp4', 4327, u'the observed representation of '), ('./static/2.mp4', 4331, u"the context word and we're subtracting from that what our model thinks um, "), ('./static/2.mp4', 4338, u'the context should look like. '), ('./static/2.mp4', 4340, u'What does the model think that the context should look like? '), ('./static/2.mp4', 4344, u'This part here is formal in expectation. '), ('./static/2.mp4', 4347, u"So, what you're doing is you're finding the weighted average "), ('./static/2.mp4', 4351, u'of the models of the representations of each word, '), ('./static/2.mp4', 4356, u'multiplied by the probability of it in the current model. '), ('./static/2.mp4', 4359, u'So, this is sort of the expected context word according to our current model, '), ('./static/2.mp4', 4365, u"and so we're taking the difference between "), ('./static/2.mp4', 4367, u'the expected context word and the actual context word that showed up, '), ('./static/2.mp4', 4372, u'and that difference then turns out to exactly give '), ('./static/2.mp4', 4375, u'us the slope as to which direction we should be '), ('./static/2.mp4', 4378, u'walking changing the words '), ('./static/2.mp4', 4381, u"representation in order to improve our model's ability to predict. "), ('./static/2.mp4', 4386, u"Okay. Um, so we'll, "), ('./static/2.mp4', 4391, u'um, assignment two, um, yeah. '), ('./static/2.mp4', 4394, u"So, um, it'll be a great exercise for you guys, "), ('./static/2.mp4', 4398, u'um, to in- um, '), ('./static/2.mp4', 4400, u'to try and do that for the cen-, wait, '), ('./static/2.mp4', 4402, u'um I did the center words trying to look context words as well '), ('./static/2.mp4', 4406, u'and show you that you can do the same kind of piece of math and have it work out. '), ('./static/2.mp4', 4411, u"Um, if I've just got a few minutes left at the end. "), ('./static/2.mp4', 4415, u'Um, what I just wanted to show you if I can get all of this to work right. '), ('./static/2.mp4', 4423, u"Um, let's go [inaudible] this way. "), ('./static/2.mp4', 4429, u'Okay, find my. '), ('./static/2.mp4', 4434, u'Okay. Um, so I just wanted to just show you a quick example. '), ('./static/2.mp4', 4440, u'So, for the first assignment, '), ('./static/2.mp4', 4441, u"um, again it's an iPython Notebook. "), ('./static/2.mp4', 4444, u"So, if you're all set up you sort of can do Jupyter Notebook. "), ('./static/2.mp4', 4449, u'Um, and you have some notebook. '), ('./static/2.mp4', 4452, u"Um, here's my little notebook I'm gonna show you, "), ('./static/2.mp4', 4457, u'um, and the trick will be to make this big enough that people can see it. '), ('./static/2.mp4', 4470, u'That readable? [LAUGHTER] Okay, um, '), ('./static/2.mp4', 4475, u'so right so, so Numpy is the sort of, '), ('./static/2.mp4', 4479, u'um, do math package in Python. '), ('./static/2.mp4', 4481, u"You'll want to know about that. "), ('./static/2.mp4', 4483, u"If you don't know about it. "), ('./static/2.mp4', 4484, u'Um, Matplotlib is sort of the, '), ('./static/2.mp4', 4486, u'one of the most basic graphing package '), ('./static/2.mp4', 4489, u"if you don't know about that you're going to want to know about it. "), ('./static/2.mp4', 4491, u'This is sort of an IPython or Jupyter special that '), ('./static/2.mp4', 4495, u'lets you have an interactive matplotlib um, inside. '), ('./static/2.mp4', 4499, u'And if you want to get fancy you can play it- play with your graphic styles. '), ('./static/2.mp4', 4503, u"Um, there's that. "), ('./static/2.mp4', 4506, u'Scikit-learn is kind of a general machine learning package. '), ('./static/2.mp4', 4510, u"Um, Gensim isn't a deep learning package. "), ('./static/2.mp4', 4513, u'Gensim is kind of a word similarity package which started off um, '), ('./static/2.mp4', 4517, u'with um, methods like Latent Dirichlet analysis. '), ('./static/2.mp4', 4520, u'If you know about that from modelling words '), ('./static/2.mp4', 4522, u'similarities that sort of grown as a good package um, '), ('./static/2.mp4', 4525, u'for doing um, word vectors as well. '), ('./static/2.mp4', 4528, u"So, it's quite often used for word vectors and "), ('./static/2.mp4', 4531, u'word similarities that sort of efficient for doing things at large-scale. '), ('./static/2.mp4', 4536, u'Um, yeah. '), ('./static/2.mp4', 4537, u"So, I haven't yet told you about will next time we have "), ('./static/2.mp4', 4541, u'our own homegrown form of word vectors which are the GloVe word vectors. '), ('./static/2.mp4', 4546, u"I'm using them not because it really matters for what I'm showing but you know, "), ('./static/2.mp4', 4551, u'these vectors are conveniently small. '), ('./static/2.mp4', 4555, u'It turns out that the vectors that Facebook and Google '), ('./static/2.mp4', 4560, u'distribute are extremely large vocabulary and extremely high dimensional. '), ('./static/2.mp4', 4565, u'So take me just too long to load them in '), ('./static/2.mp4', 4568, u'the last five minutes of this class where conveniently uh, '), ('./static/2.mp4', 4572, u'in our Stanford vectors we have a 100 dimensional vectors, um, '), ('./static/2.mp4', 4576, u'and 50 dimensional vectors which are kinda '), ('./static/2.mp4', 4579, u'good for doing small things on a laptop frankly. '), ('./static/2.mp4', 4581, u"Um, so, what I'm doing here is Gensim doesn't natively support "), ('./static/2.mp4', 4587, u'GloVe vectors but they actually provide a utility that '), ('./static/2.mp4', 4590, u'converts the GloVe file format to the word2vec file format. '), ('./static/2.mp4', 4593, u"So I've done that. And then I've loaded a pre-trained model of word vectors. "), ('./static/2.mp4', 4600, u'Um, and, so this is what they call a keyed vector. '), ('./static/2.mp4', 4604, u'And so, the keyed vector is nothing fancy. '), ('./static/2.mp4', 4606, u"It's just you have words like potato and there's a vector that hangs off each one. "), ('./static/2.mp4', 4611, u"So it's really just sort of a big dictionary with a vector for each thing. "), ('./static/2.mp4', 4615, u'But, so this model has been a trained model where '), ('./static/2.mp4', 4618, u'we just use the kind of algorithm we looked at and, '), ('./static/2.mp4', 4622, u'you know, trained at billions of times fiddling our word vectors. '), ('./static/2.mp4', 4626, u'Um, and once we have one we can then, um, '), ('./static/2.mp4', 4631, u'ask questions like, we can say, '), ('./static/2.mp4', 4634, u'what is the most similar word to some other words? '), ('./static/2.mp4', 4637, u'So we could take something like, um, '), ('./static/2.mp4', 4639, u"what are the most similar words to Obama let's say? "), ('./static/2.mp4', 4643, u'And we get back Barrack, Bush, Clinton, '), ('./static/2.mp4', 4645, u'McCain, Gore, Hillary Dole, Martin, Henry. '), ('./static/2.mp4', 4649, u'That seems actually kind of interesting. '), ('./static/2.mp4', 4651, u'These factors from a few years ago. '), ('./static/2.mp4', 4654, u"So we don't have a post- post-Obama staff. "), ('./static/2.mp4', 4657, u'I mean if you put in another word, um, you know, '), ('./static/2.mp4', 4660, u'we can put in something like banana and we get coconut, '), ('./static/2.mp4', 4664, u'mango, bananas, potato, pineapple. '), ('./static/2.mp4', 4666, u'We get kind of tropical food. '), ('./static/2.mp4', 4669, u'So, you can actually- you can actually ask uh, '), ('./static/2.mp4', 4674, u'for being dissimilar to words. '), ('./static/2.mp4', 4676, u"By itself dissimilar isn't very useful. "), ('./static/2.mp4', 4679, u'So if I ask most similar and I say um, '), ('./static/2.mp4', 4684, u'negative equals, um, banana, '), ('./static/2.mp4', 4689, u"um, I'm not sure what your concept of what's most dissimilar to, "), ('./static/2.mp4', 4694, u'um, banana is, but you know, '), ('./static/2.mp4', 4696, u"actually by itself you don't get anything useful out of this, um, "), ('./static/2.mp4', 4702, u'because, um, you just so get these weird really rare words um, '), ('./static/2.mp4', 4708, u"which, um, [LAUGHTER] definitely weren't the ones who are thinking of. "), ('./static/2.mp4', 4711, u'Um, but it turns out you can do something really useful with this negative idea '), ('./static/2.mp4', 4717, u'which was one of '), ('./static/2.mp4', 4719, u'the highly celebrated results of word vectors when they first started off. '), ('./static/2.mp4', 4724, u'And that was this idea that there is actually dimensions of meaning in this space. '), ('./static/2.mp4', 4730, u'And so this was the most celebrated example um, which was look, '), ('./static/2.mp4', 4734, u'what we could do is we could start with the word king and subtract '), ('./static/2.mp4', 4739, u'from it the meaning of man and then we could add to it the meaning of woman. '), ('./static/2.mp4', 4745, u'And then we could say which word in our vector space as '), ('./static/2.mp4', 4749, u'most similar in meaning to that word. '), ('./static/2.mp4', 4753, u'And that would be a way of sort of doing analogies. '), ('./static/2.mp4', 4755, u'Would be able to do the, um, analogy, '), ('./static/2.mp4', 4758, u'man is the king as woman is to what? '), ('./static/2.mp4', 4762, u"And so, the way we're gonna do that is to say we want to be similar to king "), ('./static/2.mp4', 4766, u"and woman because they're both positive ones and far away from man. "), ('./static/2.mp4', 4771, u'And so, we could do that manually, '), ('./static/2.mp4', 4775, u'here is said manually, '), ('./static/2.mp4', 4776, u'most similar positive woman king, negative man. '), ('./static/2.mp4', 4781, u'And we can run this and lo and behold it produces queen. '), ('./static/2.mp4', 4785, u'To make that a little bit easier I defined this analogy, '), ('./static/2.mp4', 4788, u'um, analogy predicates so I can run other ones. '), ('./static/2.mp4', 4793, u'And so I can run another one like analogy Japan Japanese, '), ('./static/2.mp4', 4799, u'Austria is to Austrian. '), ('./static/2.mp4', 4801, u'Um, and you know, '), ('./static/2.mp4', 4803, u"I think it's fair to say that when people first "), ('./static/2.mp4', 4807, u'saw that you could have this simple piece of math and run it, '), ('./static/2.mp4', 4810, u'and learn meanings of words. '), ('./static/2.mp4', 4812, u"I mean it actually just sort of blew people's minds how effective this was. "), ('./static/2.mp4', 4818, u"You know. Like there- there's is no mirrors and strings here, right? "), ('./static/2.mp4', 4822, u"You know it's not that I have a separate- "), ('./static/2.mp4', 4824, u"a special sort of list in my Python where there's a difficult I'm looking up, "), ('./static/2.mp4', 4828, u'er, for Austria Austrian, '), ('./static/2.mp4', 4830, u'uh, and things like that. '), ('./static/2.mp4', 4831, u'But somehow these vector representations are '), ('./static/2.mp4', 4835, u'such that it is actually encoding these semantic relationships, '), ('./static/2.mp4', 4838, u'you know, so you can try different ones, '), ('./static/2.mp4', 4840, u"you know, like it's not that only this one works. "), ('./static/2.mp4', 4843, u'I can put in France, it says French. '), ('./static/2.mp4', 4846, u'I can put in Germany, it says German, '), ('./static/2.mp4', 4849, u'I can put in Australia not Austria and it says Australian, '), ('./static/2.mp4', 4854, u'you know that somehow if you want this vector representations of words that '), ('./static/2.mp4', 4859, u'for sort of these ideas like understanding the relationships between words, '), ('./static/2.mp4', 4864, u"you're just doing this vector space manipulation on these 100 dimensional numbers, "), ('./static/2.mp4', 4870, u'that it actually knows about them.This not only the similarities of word meanings but '), ('./static/2.mp4', 4875, u'actually different semantic relationships '), ('./static/2.mp4', 4878, u'between words like country names and their peoples. '), ('./static/2.mp4', 4881, u"And yeah that's actually pretty amazing. "), ('./static/2.mp4', 4883, u"It really-you know, it's sort of surprising that running such a dumb algorithm on um, "), ('./static/2.mp4', 4891, u'vectors of numbers could capture so well the meaning of words. '), ('./static/2.mp4', 4895, u"And so that's sort of became the foundation of a lot of sort "), ('./static/2.mp4', 4898, u'of modern distributed neural representations of words. '), ('./static/2.mp4', 4901, u"Okay I'll stop there. "), ('./static/2.mp4', 4902, u'Thanks a lot guys and see you on Thursday. [NOISE] ')]